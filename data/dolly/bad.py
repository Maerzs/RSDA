import json
import pandas as pd
import asyncio
from openai import AsyncOpenAI
from tqdm.asyncio import tqdm_asyncio
import os

# ================= é…ç½®åŒºåŸŸ =================
# API é…ç½®
API_KEY = os.getenv("OPENAI_API_KEY", "sk-dwyhno2Lan773h3YPtOArk86AV8nPiMU443qWr97qXFN3afk")
BASE_URL = "https://api.n1n.ai/v1"
MODEL_NAME = "gpt-4o-mini"

# æ–‡ä»¶è·¯å¾„é…ç½®
INPUT_FILE = "dolly.jsonl"  # åŸå§‹dollyæ•°æ®
ALPACA_FULL_FILE = "dolly_alpaca.json"  # è½¬æ¢åçš„å®Œæ•´alpacaæ ¼å¼
SCORED_FILE = "dolly_score_low.csv"  # æ‰“åˆ†åçš„ä¸­é—´æ–‡ä»¶
OUTPUT_FILE = "dolly_bad.json"  # æœ€ç»ˆç­›é€‰åçš„ä½è´¨é‡æ•°æ®

# ç­›é€‰å‚æ•°
TOP_K = 1000  # ç­›é€‰å‰1000æ¡ä½è´¨é‡æ•°æ®

# å¹¶å‘æ§åˆ¶
MAX_CONCURRENT = 10
RETRY_TIMES = 3


# ============================================


def load_dolly_data(filepath):
    """
    åŠ è½½dollyçš„jsonlæ ¼å¼æ•°æ®å¹¶è½¬æ¢ä¸ºalpacaæ ¼å¼

    Dollyæ ¼å¼:
    {
        "instruction": "...",
        "context": "...",  # å¯é€‰
        "response": "...",
        "category": "..."
    }

    Alpacaæ ¼å¼:
    {
        "instruction": "...",
        "input": "...",  # å¯¹åº”dollyçš„context
        "output": "..."  # å¯¹åº”dollyçš„response
    }
    """
    print(f"ğŸ“‚ åŠ è½½Dollyæ•°æ®: {filepath}")

    data = []
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                item = json.loads(line)
                # è½¬æ¢ä¸ºalpacaæ ¼å¼
                alpaca_item = {
                    "instruction": item.get("instruction", ""),
                    "input": item.get("context", ""),  # dollyçš„contextæ˜ å°„åˆ°alpacaçš„input
                    "output": item.get("response", ""),  # dollyçš„responseæ˜ å°„åˆ°alpacaçš„output
                }
                data.append(alpaca_item)

    print(f"âœ… åŠ è½½äº† {len(data)} æ¡æ•°æ®\n")
    return data


def build_prompt(item):
    """æ„å»ºè¯„åˆ† Prompt"""
    instruction = item.get('instruction', '')
    inp = item.get('input', '')
    output = item.get('output', '')

    prompt = f"""You are an expert judge evaluating the quality of an AI assistant's response.

Instruction: {instruction}
Input: {inp}
Response: {output}

Rate the Response on a scale of 1 to 5 (5 being best). 
Criteria: 
- Accuracy: Is the information correct?
- Helpfulness: Does it address the instruction?
- Clarity: Is it well-structured and easy to understand?

Output ONLY a single number between 1 and 5 (e.g., 5).
Do not include any explanation or additional text.

Score:"""

    return prompt


async def score_single_sample(client, item, semaphore, index):
    """
    å¯¹å•ä¸ªæ ·æœ¬è¿›è¡Œè¯„åˆ†ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    """
    prompt = build_prompt(item)

    async with semaphore:
        for attempt in range(RETRY_TIMES):
            try:
                response = await client.chat.completions.create(
                    model=MODEL_NAME,
                    messages=[
                        {"role": "system",
                         "content": "You are a strict data quality evaluator. Output only decimal numbers with 2 decimal places (e.g., 3.47)."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.3,
                    max_tokens=10,
                    timeout=30.0
                )

                generated_text = response.choices[0].message.content.strip()

                try:
                    score = float(generated_text)
                    score = max(0.00, min(4.99, score))
                    score = round(score, 2)
                except ValueError:
                    import re
                    numbers = re.findall(r'\d+\.?\d*', generated_text)
                    if numbers:
                        score = float(numbers[0])
                        score = max(0.00, min(4.99, score))
                        score = round(score, 2)
                    else:
                        score = 0.00

                return index, score

            except Exception as e:
                if attempt == RETRY_TIMES - 1:
                    print(f"\næ ·æœ¬ {index} è¯„åˆ†å¤±è´¥ (å·²é‡è¯• {RETRY_TIMES} æ¬¡): {e}")
                    return index, 1.0
                await asyncio.sleep(1)


async def score_all_samples(data):
    """
    æ‰¹é‡è¯„åˆ†æ‰€æœ‰æ ·æœ¬
    """
    client = AsyncOpenAI(
        api_key=API_KEY,
        base_url=BASE_URL,
        timeout=60.0,
        max_retries=2
    )

    semaphore = asyncio.Semaphore(MAX_CONCURRENT)

    print(f"å¼€å§‹è¯„åˆ† {len(data)} ä¸ªæ ·æœ¬...")
    print(f"å¹¶å‘æ•°: {MAX_CONCURRENT}, æ¨¡å‹: {MODEL_NAME}\n")

    tasks = [
        score_single_sample(client, item, semaphore, i)
        for i, item in enumerate(data)
    ]

    results = await tqdm_asyncio.gather(*tasks, desc="è¯„åˆ†è¿›åº¦")

    results.sort(key=lambda x: x[0])
    scores = [score for _, score in results]

    return scores


def filter_and_save_low_quality(scored_file, output_file, top_k):
    """
    ç­›é€‰ä½è´¨é‡æ•°æ®å¹¶ä¿å­˜
    """
    df = pd.read_csv(scored_file)

    print(f"\n{'=' * 50}")
    print(f"æ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ ·æœ¬æ•°: {len(df)}")
    print(f"  å¹³å‡åˆ†æ•°: {df['quality_score'].mean():.2f}")
    print(f"  åˆ†æ•°èŒƒå›´: {df['quality_score'].min():.2f} - {df['quality_score'].max():.2f}")

    # ç»Ÿè®¡å„åˆ†æ•°æ®µåˆ†å¸ƒ
    print(f"\n  åˆ†æ•°åˆ†å¸ƒ:")
    bins = [0, 1, 2, 3, 4, 5]
    labels = ['0-1', '1-2', '2-3', '3-4', '4-5']
    df['score_range'] = pd.cut(df['quality_score'], bins=bins, labels=labels, include_lowest=True)
    for label in labels:
        count = len(df[df['score_range'] == label])
        print(f"    {label} åˆ†: {count} ä¸ª ({count / len(df) * 100:.1f}%)")
    print(f"{'=' * 50}\n")

    # æŒ‰åˆ†æ•°å‡åºæ’åº(æœ€ä½åˆ†åœ¨å‰)
    df_sorted = df.sort_values(by="quality_score", ascending=True)

    # å–å‰Kä¸ªæœ€å·®æ ·æœ¬
    top_k_df = df_sorted.head(top_k)

    print(f"ç­›é€‰å {top_k} ä¸ªä½è´¨é‡æ ·æœ¬:")
    print(f"  åˆ†æ•°èŒƒå›´: {top_k_df['quality_score'].min():.2f} - {top_k_df['quality_score'].max():.2f}")
    print(f"  å¹³å‡åˆ†æ•°: {top_k_df['quality_score'].mean():.2f}\n")

    # è½¬æ¢ä¸ºæ ‡å‡†alpacaæ ¼å¼
    result_data = []
    for _, row in top_k_df.iterrows():
        result_data.append({
            "instruction": row['instruction'],
            "input": str(row['input']) if pd.notna(row['input']) else "",
            "output": row['output']
        })

    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, indent=2, ensure_ascii=False)

    print(f"âœ… å·²ä¿å­˜ {len(result_data)} æ¡ä½è´¨é‡æ•°æ®åˆ°: {output_file}")


async def main():
    """
    ä¸»æµç¨‹ï¼šåŠ è½½ â†’ è½¬æ¢ â†’ è¯„åˆ† â†’ ç­›é€‰ â†’ ä¿å­˜
    """
    print("=" * 60)
    print("Dolly to Alpaca æ•°æ®å¤„ç† - API ç‰ˆæœ¬")
    print("=" * 60)

    # Step 1: åŠ è½½dollyæ•°æ®å¹¶è½¬æ¢ä¸ºalpacaæ ¼å¼
    alpaca_data = load_dolly_data(INPUT_FILE)

    # Step 1.5: ä¿å­˜å®Œæ•´çš„alpacaæ ¼å¼æ•°æ®ï¼ˆæ¯é›†ï¼‰
    print(f"ğŸ’¾ ä¿å­˜å®Œæ•´alpacaæ ¼å¼æ•°æ®: {ALPACA_FULL_FILE}")
    with open(ALPACA_FULL_FILE, 'w', encoding='utf-8') as f:
        json.dump(alpaca_data, f, indent=2, ensure_ascii=False)
    print(f"âœ… å·²ä¿å­˜å®Œæ•´æ¯é›† {len(alpaca_data)} æ¡æ•°æ®\n")

    # Step 2: æ‰¹é‡è¯„åˆ†
    print("ğŸ“Š å¼€å§‹è´¨é‡è¯„åˆ†...")
    scores = await score_all_samples(alpaca_data)

    # Step 3: ä¿å­˜è¯„åˆ†ç»“æœ
    print(f"\nğŸ’¾ ä¿å­˜è¯„åˆ†ç»“æœ: {SCORED_FILE}")
    df = pd.DataFrame(alpaca_data)
    df['quality_score'] = scores

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(SCORED_FILE), exist_ok=True)
    df.to_csv(SCORED_FILE, index=False, encoding='utf-8')
    print(f"âœ… å·²ä¿å­˜è¯„åˆ†ç»“æœ\n")

    # Step 4: ç­›é€‰ä½è´¨é‡æ•°æ®
    print("ğŸ” ç­›é€‰ä½è´¨é‡æ•°æ®...")
    filter_and_save_low_quality(SCORED_FILE, OUTPUT_FILE, TOP_K)

    print("\n" + "=" * 60)
    print("âœ¨ å¤„ç†å®Œæˆï¼")
    print(f"   - å®Œæ•´æ¯é›†(alpacaæ ¼å¼): {ALPACA_FULL_FILE}")
    print(f"   - ä½è´¨é‡å­é›†(alpacaæ ¼å¼): {OUTPUT_FILE}")
    print(f"   - è¯„åˆ†è¯¦æƒ…: {SCORED_FILE}")
    print("=" * 60)


if __name__ == "__main__":
    asyncio.run(main())