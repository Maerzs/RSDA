[
    {
        "sample_id": 0,
        "original_data": {
            "instruction": "What are some popular tourist destination areas in Kenya?",
            "input": "Tourism in Kenya is the second-largest source of foreign exchange revenue following agriculture. The Kenya Tourism Board is responsible for maintaining information pertaining to tourism in Kenya. The main tourist attractions are photo safaris through the 60 national parks and game reserves. Other attractions include the wildebeest migration at the Masaai Mara, which is considered to be the 7th wonder of the world; historical mosques, and colonial-era forts at Mombasa, Malindi, and Lamu; renowned scenery such as the white-capped Mount Kenya and the Great Rift Valley; tea plantations at Kericho; coffee plantations at Thika; a splendid view of Mount Kilimanjaro across the border into Tanzania; and the beaches along the Swahili Coast, in the Indian Ocean. Tourists, the largest number being from Germany and the United Kingdom, are attracted mainly to the coastal beaches and the game reserves, notably, the expansive East and Tsavo West National Park, 20,808 square kilometres (8,034 sq mi) to the southeast.[citation needed]",
            "output": "Tourists commonly visit the Masaai Mara; forts at Mombasa, Malindi and Lamu; tea and coffee plantations in Kericho and Thika; Mount Kilimanjaro; and the Swahili Coast.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.7463719844818115,
            "normalized": 0.927534852709089
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 1,
        "original_data": {
            "instruction": "Why did Kim Kardashian and Ye divorce?",
            "input": "",
            "output": "Kim and Ye were living two separate lives. Kim was in Hidden Hills, CA taking care of their four kids while Ye was developing the acres of land he bought in Wyoming. Ye began creating a whole self-sustaining community of christian devoted followers while Kim was filming her show and studying to become a lawyer. They began to grow a part, not to mention claims of both of them cheating. After two years trying to become divorced, they most recently officially became divorced and Ye must pay Kim $200,000 per month for security of their kids. Kim bought their house from Ye and other financials were settled.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.422304153442383,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 2,
        "original_data": {
            "instruction": "Tell me whether these athletes play golf, soccer, or football: Tiger Woods, Lionel Messi, Lavonte David, Russell Wilson, Derek Carr, Rafael Leao, Tyler Adams, Dustin Johnson, Jordan Spieth",
            "input": "",
            "output": "Tiger Woods: Golf\nLionel Messi: Soccer\nLavonte David: Football\nRussell Wilson: Football\nDerek Carr: Football\nRafael Leao: Soccer\nTyler Adams: Soccer\nDustin Johnson: Golf\nJordan Spieth: Golf",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.8,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.0469846725463867,
            "normalized": 0.7277099064418248
        },
        "potential_entropy": {
            "strategy_component": 0.515,
            "strategy_details": {
                "raw": 1.03,
                "normalized": 1.03,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 3,
        "original_data": {
            "instruction": "What is the best drum kit?",
            "input": "",
            "output": "There are many different manufacturers of drum kits, with a wide range of models and pricing. Some of the most popular brands include DW, Tama, Sonor, Mapex, Pearl, Yamaha, Ludwig, Gretsch and the British Drum Company. There are also excellent custom drum builders with high quality products such as SJC and A&F.\nThe majority of these brands offer kits for beginners all the way up to touring and studio professionals.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.9644508361816406,
            "normalized": 0.9898430960518974
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 4,
        "original_data": {
            "instruction": "Which characters belong to DC or Marvel Universe? Wasp, Bumblebee",
            "input": "",
            "output": "Wasp is Marvel, Bumblebee is DC",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.2,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.8,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.756150722503662,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0699999999999998,
            "strategy_details": {
                "raw": 2.1399999999999997,
                "normalized": 2.1399999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.8,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 5,
        "original_data": {
            "instruction": "Who was found guilty for the murder of Nualchawee Petchrung?",
            "input": "Nualchawee Petchrung (Thai: นวลฉวี เพชรรุ่ง, also spelled Nuanchawee) was a Thai nurse who was murdered by her medical doctor husband, Athip Suyansethakarn, on 10 September 1959. The investigation and trial received sensational coverage in the media—Siang Ang Thong newspaper, which later became the country's top circulating daily Thai Rath, gained popularity from its coverage of the case, which regularly filled the front page—and the case became one of the best known murders in Thailand. Athip was found guilty and sentenced to death, but was later pardoned. Nonthaburi Bridge, where her body was disposed into the Chao Phraya River, is still commonly known as Nualchawee Bridge, and evidence from the case is on display at the Songkran Niyomsane Forensic Medicine Museum.",
            "output": "Athip Suyansethakarn was found guilty of murdering his wife, Nualchawee Petchrung.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.851909160614014,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 6,
        "original_data": {
            "instruction": "What are five popular Taylor Swift songs?",
            "input": "",
            "output": "Five popular Taylor Swift songs are Shake it Off, Wildest Dreams, Bad Blood, Look What You Made Me Do, and Love Story",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.3631346225738525,
            "normalized": 0.8180384635925293
        },
        "potential_entropy": {
            "strategy_component": 1.1800000000000002,
            "strategy_details": {
                "raw": 2.3600000000000003,
                "normalized": 2.3600000000000003,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 7,
        "original_data": {
            "instruction": "Who is best college football coach?",
            "input": "",
            "output": "As of April 2023, Nick Saban is the greatest coach of all-time with 7 modern championships at LSU and Alabama.  However, Kirby Smart is right on his heels with the last two National Championships at Georgia after masterminding four of Saban's elite championship defenses at Alabama.  It is conceivable Kirby passes Saban when he wins back-to-back-to-back championships, something Saban has never done.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 4.156578063964844,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.465,
            "strategy_details": {
                "raw": 2.93,
                "normalized": 2.93,
                "components": {
                    "instruction": 0.6,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.5,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 8,
        "original_data": {
            "instruction": "Where should I go for vacation?",
            "input": "",
            "output": "Where you should go for vacation depends on your interests and budget. For example, if you want to immerse yourself in nature, you could visit a local national park and stay in a nearby town. You could spend your time hiking, picnicking, and sightseeing in the national park in a cost effective way. If you prefer nightlife, visiting a city may be for you. In the United States, some options are Los Angeles and New York City. Finally, if you prefer spending your time in museums, find a vacation spot with a rich history, and enjoy immersing yourself in the local culture. For example, India, Peru and Paris are three destinations with and endless supply of local history and culture.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 2.8134193420410156,
            "normalized": 0.6609769548688617
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 9,
        "original_data": {
            "instruction": "How hard is it to learn how to ski?",
            "input": "",
            "output": "Learning how to ski is moderately hard. Depending on if you learn on your own or hire a professional instructor, you can expect to go skiing at least five to eight times before you feel comfortable on skis. There are also different mountains that offer different terrain which can impact how quickly you pick up skiing. It is advised that beginners stick to ski runs that are labeled as a green circle on any mountain as these are classified as the easiest way down the mountain.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.645242214202881,
            "normalized": 0.8986406326293945
        },
        "potential_entropy": {
            "strategy_component": 0.785,
            "strategy_details": {
                "raw": 1.57,
                "normalized": 1.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 10,
        "original_data": {
            "instruction": "Alfred White was a famous author under which name",
            "input": "",
            "output": "James Herriot",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 8.968696594238281,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 11,
        "original_data": {
            "instruction": "Given this paragraph are Beavers the largest living rodents?",
            "input": "Beavers (genus Castor) are large, semiaquatic rodents of the Northern Hemisphere. There are two existing species: the North American beaver (Castor canadensis) and the Eurasian beaver (C. fiber). Beavers are the second-largest living rodents, after capybaras, weighing up to 30 kilograms (66 lb). Beavers have stout bodies with large heads, long chisel-like incisors, brown or gray fur, hand-like front feet, webbed back feet, and tails that are flat and scaly. The two species differ in skull and tail shape and fur color. Beavers can be found in a number of freshwater habitats, such as rivers, streams, lakes and ponds. They are herbivorous, consuming tree bark, aquatic plants, grasses and sedges.",
            "output": "No, Beavers are the second-largest living rodents, second to Capybaras.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 5.109283447265625,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 12,
        "original_data": {
            "instruction": "Which of these sentences is correct: \"If I was you, I would do it\" or \"If I were you, I would do it\"?",
            "input": "",
            "output": "The correct sentence is \"If I were you, I would do it\".\" This is the subjunctive form of English. It should be used when a hypothetical is contrary to fact. In this case, \"I\" is not \"you,\" so the subjunctive form should be used. In first person, the form of \"be\" used for the subjunctive mood is \"were.\"",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.344947576522827,
            "normalized": 0.527127879006522
        },
        "potential_entropy": {
            "strategy_component": 1.0099999999999998,
            "strategy_details": {
                "raw": 2.0199999999999996,
                "normalized": 2.0199999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 13,
        "original_data": {
            "instruction": "Extract from the text the main characteristics of the city of São Paulo. Separate them with a comma.",
            "input": "São Paulo is the most populous city in Brazil, and is the capital of the state of São Paulo, the most populous and wealthiest Brazilian state, located in the country's Southeast Region. Listed by the GaWC as an alpha global city, São Paulo is the most populous city proper in the Americas, the Western Hemisphere, and the Southern Hemisphere, as well as the world's 4th largest city proper by population. Additionally, São Paulo is the largest Portuguese-speaking city in the world. It exerts strong international influences in commerce, finance, arts and entertainment. The city's name honors the Apostle, Saint Paul of Tarsus. The city's metropolitan area, the Greater São Paulo, ranks as the most populous in Brazil and the 12th most populous on Earth. The process of conurbation between the metropolitan areas around the Greater São Paulo (Campinas, Santos, Jundiaí, Sorocaba and São José dos Campos) created the São Paulo Macrometropolis, a megalopolis with more than 30 million inhabitants, one of the most populous urban agglomerations in the world.",
            "output": "most populous city in Brazil, most populous and wealthiest Brazilian state, located in the country's Southeast Region, Listed by the GaWC as an alpha global city, the most populous city proper in the Americas, the world's 4th largest city proper by population, São Paulo is the largest Portuguese-speaking city in the world, city's name honors the Apostle, Saint Paul of Tarsus, ranks as the most populous in Brazil and the 12th most populous on Earth",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1781017780303955,
            "normalized": 0.765171936580113
        },
        "potential_entropy": {
            "strategy_component": 0.32000000000000006,
            "strategy_details": {
                "raw": 0.6400000000000001,
                "normalized": 0.6400000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 14,
        "original_data": {
            "instruction": "Was Tyler Thompson a diplomat",
            "input": "",
            "output": "Tyler Thompson (1907–2002) was an American Ambassador to Finland and Iceland. He also served as the Director General of the Foreign Service.\n\nTyler entered the Foreign Service in 1931 after graduating from Princeton University.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 2.632960557937622,
            "normalized": 0.609417302267892
        },
        "potential_entropy": {
            "strategy_component": 1.045,
            "strategy_details": {
                "raw": 2.09,
                "normalized": 2.09,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 15,
        "original_data": {
            "instruction": "Given this paragraph about blockchain, how did blockchain influence bitcoin cryptocurrency?",
            "input": "Blockchains are typically managed by a peer-to-peer (P2P) computer network for use as a public distributed ledger, where nodes collectively adhere to a consensus algorithm protocol to add and validate new transaction blocks. Although blockchain records are not unalterable, since blockchain forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. A blockchain was created by a person (or group of people) using the name (or pseudonym) Satoshi Nakamoto in 2008 to serve as the public distributed ledger for bitcoin cryptocurrency transactions, based on previous work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain may be considered a type of payment rail.",
            "output": "Blockchain is the technology that enables the existence of cryptocurrency and bitcoin is the name of the best-known cryptocurrency. The implementation of bitcoin using blockchain made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. Bitcoin has inspired other applications and blockchains that are highly available, public, and are widely used by cryptocurrencies.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.397771120071411,
            "normalized": 0.8279346057346889
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 16,
        "original_data": {
            "instruction": "What are some common ways that adults break bones?",
            "input": "",
            "output": "- Falling off of a ladder\n- Falling down the stairs\n- Playing recreational sports\n- Being in a car crash",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.8081977367401123,
            "normalized": 0.6594850676400321
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 17,
        "original_data": {
            "instruction": "Explain gravity",
            "input": "",
            "output": "Gravity is a fundamental interaction which causes mutual attraction between all things with mass or energy. Gravity is the weakest of the four fundamental interactions namely the strong, weak, electromagnetic and gravity. However, it is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light. \nGravity is most accurately described by the general theory of relativity that was proposed by Albert Einstein. The theory describes gravity not as a force, but as the curvature of spacetime, caused by the uneven distribution of mass, and causing masses to move along geodesic lines.\nHowever, for most applications, gravity is well approximated by Newton's law of universal gravitation, which describes gravity as a force causing any two bodies to be attracted toward each other, with magnitude proportional to the product of their masses and inversely proportional to the square of the distance between them.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 1.8036926984786987,
            "normalized": 0.37248362813677105
        },
        "potential_entropy": {
            "strategy_component": 1.18,
            "strategy_details": {
                "raw": 2.36,
                "normalized": 2.36,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 18,
        "original_data": {
            "instruction": "Given a reference text about the Mauritius fody, tell me what they look like.",
            "input": "The Mauritius fody (Foudia rubra) is a rare species of bird in the weaver family. It is endemic to the island of Mauritius. It is classified by BirdLife International as being endangered. It is also on the United States' Endangered Species List with an endangered status.\n\nThis bird is 14 centimeters long. Breeding males are olive brown with a red head, breast and rump patch and black lores. While females, non-breeding males and juveniles are olive brown with white wing bars and a brown bill.\n\nThe bird lives in several types of forest, including degraded areas, as well as plantations. Stands of Japanese cedar (Cryptomeria japonica) have replaced native vegetation and now provide protection against predators. It feeds on insects like grasshoppers, beetle larvae, caterpillars, and also spiders. Berries are eaten regularly by some individuals. It feeds on nectar regularly, using its specialised brush-tipped tongue.\n\nThe bird is a weaver, the male and female cooperating to weave each nest, from material like grass, moss and small twigs.",
            "output": "Male and female Mauritius fody birds have different appearances. Breeding males are olive brown with red heads, breast and rump patches, and black lores. Females and non-breeding males are olive brown with white wing bars and brown bills.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.699298620223999,
            "normalized": 0.9140853200639997
        },
        "potential_entropy": {
            "strategy_component": 0.38250000000000006,
            "strategy_details": {
                "raw": 0.7650000000000001,
                "normalized": 0.7650000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 19,
        "original_data": {
            "instruction": "From the information below, extract the date the Battle of Entebbe took place.",
            "input": "The Battle of Entebbe was a battle of the Uganda–Tanzania War that took place on 7 April 1979 on the Entebbe peninsula in Uganda between Tanzanian units and Ugandan and Libyan units. The Tanzanians occupied the area, killed hundreds of Libyans, and ended the Libyan airlift in support of the Ugandan Government.",
            "output": "The Battle of Entebbe took place on April 7, 1979.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.7995803356170654,
            "normalized": 0.6570229530334473
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 20,
        "original_data": {
            "instruction": "Why is the University of Waterloo the best school for software engineering?",
            "input": "",
            "output": "The University of Waterloo boasts an impressive history of successful alumni in the software industry. One major reason for this success is its cooperative education program. It implements a system in which six four month internships are spread across a four year degree, combining experiential learning with theoretical education. Because of this, Waterloo's students, who are selected from the brightest in Canada, gain extensive exposure to the software engineering industry prior to graduation.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.2218265533447266,
            "normalized": 0.7776647295270648
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 21,
        "original_data": {
            "instruction": "List all the school districts in the following paragraph in a bullet list.",
            "input": "The City of Drew is served by the Sunflower County Consolidated School District. Elementary school students attend A. W. James Elementary School in Drew and middle school students attend Drew Hunter Middle School in Drew. High school students attend Thomas E. Edwards, Sr. High School (formerly Ruleville Central High School) in Ruleville.\n\nResidents were previously zoned to the Drew School District. Children attended A.W. James Elementary School and Drew Hunter High School in Drew. Prior to the 2010-2011 school year the Drew School District secondary schools were Hunter Middle School and Drew High School. As of July 1, 2012, the Drew School District was consolidated with the Sunflower County School District. Drew Hunter's high school division closed as of that date, with high school students rezoned to Ruleville Central High School.\n\nMississippi Delta Community College has the Drew Center in Drew. Sunflower County Library System operates the Drew Public Library in Drew.",
            "output": "Sure, that paragraph references these school districts:\n- Sunflower County Consolidated School District\n- Drew School District\n- Sunflower County School District\n\nNote that \"Sunflower County School District\" may be a duplicate of \"Sunflower County Consolidated School District\", but it is unclear from the context of this paragraph",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.6218321323394775,
            "normalized": 0.6062377520969936
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 22,
        "original_data": {
            "instruction": "Give me a bulleted list of the seven more recent Indian Prime Ministers",
            "input": "",
            "output": "- Narendra Modi (2014– )\n- Manmohan Singh (2004–14)\n- Atal Bihari Vajpayee (1998–2004; 2nd time)\n- Inder K. Gujral (1997–98)\n- H.D. Deve Gowda (1996–97)\n- Atal Bihari Vajpayee (1996; 1st time)\n- P.V. Narasimha Rao (1991–96)",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 1.5854525566101074,
            "normalized": 0.3101293018886021
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 23,
        "original_data": {
            "instruction": "When running a marathon and attempting to run a personal best time, categorize the following as either ‘useful’ or ‘not useful’, ‘dangerous’ or ‘neither. Fast digesting carbohydrates, a fire extinguisher, a weight vest, scissors, running shoes",
            "input": "",
            "output": "Fast-digesting carbohydrates are useful to make sure your body has fuel through all 26.2 miles. A fire extinguisher is neither useful nor not useful. A weight vest is not useful if you’re attempting to run your personal best. The extra weight will slow you down. Scissors are dangerous, you should never run with scissors. Running shoes are very useful to protect your feet and enhance your biomechanics and the efficiency of your stride to run your best time.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.88,
            "s_out_bg": 0.82
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.12,
            "s_out_bg": 0.18000000000000005
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.6787636280059814,
            "normalized": 0.9082181794302804
        },
        "potential_entropy": {
            "strategy_component": 0.3375,
            "strategy_details": {
                "raw": 0.675,
                "normalized": 0.675,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.12,
                    "out_bg": 0.18000000000000005
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 24,
        "original_data": {
            "instruction": "Is Castle Island really an Island?",
            "input": "Castle Island is a peninsula in South Boston on the shore of Boston Harbor. In 1928, Castle Island was connected to the mainland by a narrow strip of land and is thus no longer an island. It has been the site of a fortification since 1634, and is currently a 22-acre (8.9 ha) recreation site and the location of Fort Independence.",
            "output": "No, Castle Island is a peninsula off Boston Harbor as of 1928, when it was connected to the Massachusetts mainland.  It is a recreation area that is the site of Fort Independence.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.9779632091522217,
            "normalized": 0.9937037740434919
        },
        "potential_entropy": {
            "strategy_component": 0.23000000000000004,
            "strategy_details": {
                "raw": 0.4600000000000001,
                "normalized": 0.4600000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 25,
        "original_data": {
            "instruction": "How many seasons did Shaq have over 30 ppg",
            "input": "On the team's first full day at the Olympics in Atlanta, the media announced that O'Neal would join the Los Angeles Lakers on a seven-year, $121 million contract. O'Neal insisted he did not choose Los Angeles for the money; discussing the signing he referred to a couple of his product endorsements, saying: \"I'm tired of hearing about money, money, money, money, money. I just want to play the game, drink Pepsi, wear Reebok.\" The Lakers won 56 games during the 1996–97 season. O'Neal averaged 26.2 points and 12.5 rebounds in his first season with Los Angeles; however, he again missed over 30 games due to injury. The Lakers made the playoffs, but were eliminated in the second round by the Utah Jazz in five games. In his first playoff game for the Lakers, O'Neal scored 46 points against the Portland Trail Blazers, the most for the Lakers in a playoff game since Jerry West had 53 in 1969. On December 17, 1996, O'Neal shoved Dennis Rodman of the Chicago Bulls; Rodman's teammates Scottie Pippen and Michael Jordan restrained Rodman and prevented further conflict. The Los Angeles Daily News reported that O'Neal was willing to be suspended for fighting Rodman, and O'Neal said: \"It's one thing to talk tough and one thing to be tough.\"\n\nThe following season, O'Neal averaged 28.3 points and 11.4 rebounds. He led the league with a 58.4 field goal percentage, the first of five consecutive seasons in which he did so. The Lakers finished the season 61–21, first in the Pacific Division, and were the second seed in the western conference during the 1998 NBA Playoffs. After defeating the Portland Trail Blazers and Seattle SuperSonics in the first two rounds, the Lakers again fell to the Jazz, this time in a 4–0 sweep.",
            "output": "Shaquille O'Neal had three seasons over 30 points per game, all of which were with the Los Angeles Lakers. He had his first season with the Lakers in 1995-96, when he averaged 30.4 points per game. He had his second season in 1996-97, when he averaged 29.7 points per game. He had his final season with the Lakers in 2000-01, when he averaged 31.2 points per game.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.6,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.4,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 1.2380222082138062,
            "normalized": 0.21086348806108748
        },
        "potential_entropy": {
            "strategy_component": 0.685,
            "strategy_details": {
                "raw": 1.37,
                "normalized": 1.37,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.65,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.4,
                    "out_div": 0.6,
                    "out_dens": 0.5,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 26,
        "original_data": {
            "instruction": "What is the Simpson family dog named?",
            "input": "",
            "output": "Santa's little helper",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 11.575427055358887,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 27,
        "original_data": {
            "instruction": "When was \"I'm Just a Singer (In a Rock and Roll Band)\" released?",
            "input": "\"I'm Just a Singer (In a Rock and Roll Band)\" is a 1973 hit single by the English progressive rock band the Moody Blues, written by the band's bassist, John Lodge. It was first released in 1972 as the final track on the album Seventh Sojourn and was later released as a single in 1973, with \"For My Lady\" as its B-side. It was the second single released from Seventh Sojourn, with the first being \"Isn't Life Strange\", which was also written by Lodge.\n\nThe song reached number 12 on the Billboard Hot 100 in the US, becoming one of their highest-charting hits in that country, but fared less well in their native UK, where it managed number 36. It was also the final single released by the Moody Blues prior to their five-year hiatus, which was agreed upon so each of the band members could pursue their own solo careers. Their next single would not be until 1978, with \"Steppin' in a Slide Zone\".",
            "output": "\"I'm Just a Singer (In a Rock and Roll Band)\" was originally released in 1972 as the final track on the album Seventh Sojourn and was later released as a single in 1973.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.7188751697540283,
            "normalized": 0.6339643342154366
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 28,
        "original_data": {
            "instruction": "What are the five biggest centers of Sufism?",
            "input": "Sufism in Bangladesh is more or less similar to that in the whole Indian subcontinent. India, it is claimed, is one of the five great centers of Sufism, the other four being Persia (including central Asia), Baghdad, Syria, and North Africa. Sufi saints flourished in Hindustan (India) preaching the mystic teachings of Sufism that easily reached the common people, especially the spiritual truth seekers in India. Sufism in Bangladesh is also called pirism, after the pirs or teachers in the Sufi tradition (also called Fakir).",
            "output": "The five main centers of Sufism are India, Persia, Baghdad, Syria, and North Africa.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.420315742492676,
            "normalized": 0.8343759264264788
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.55,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 29,
        "original_data": {
            "instruction": "Give me a quick exercise I can do every day",
            "input": "",
            "output": "Choose a body-weight exercise of your choice - for example push ups, squats or lunges. Set a timer for 2 minutes. Do as many repetitions as you can before the 2 minutes are up. Take break as often as you need. Do this every day and track the number of repetitions.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.8,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.765737533569336,
            "normalized": 0.6473535810198102
        },
        "potential_entropy": {
            "strategy_component": 1.185,
            "strategy_details": {
                "raw": 2.37,
                "normalized": 2.37,
                "components": {
                    "instruction": 0.3,
                    "input": 1.65,
                    "output": 0.42
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 30,
        "original_data": {
            "instruction": "What is the best season to visit Japan?",
            "input": "",
            "output": "The best season to visit Japan is well known as Spring, especially between March and April when you can enjoy cherry blossom viewing. Because it is so popular, you will expect a lot of crowds especially in famous cherry blossom spots. Hotels and flights are hard to be booked and perhaps the most expensive. \n\nAnother popular season is Fall. The weather is pleasant and many foreigners come to visit especially for fall foliage in November. And the good thing is that despite its beauty, it is expected to be less crowded than Spring during cherry blossom season.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.3278696537017822,
            "normalized": 0.8079627582005092
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 31,
        "original_data": {
            "instruction": "Think up different ways to make a baby stop crying.",
            "input": "",
            "output": "Here are different ways to make a baby stop crying:\n1. Give the baby milk\n2. Bounce the baby up and down\n3. Take the baby on a walk\n4. Take the baby for a ride in the car\n5. Give the baby food\n6. Rock the baby to sleep",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.1006696224212646,
            "normalized": 0.457334177834647
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 32,
        "original_data": {
            "instruction": "How does the Stock Market work?",
            "input": "",
            "output": "The stock market helps companies raise money to fund operations by selling shares of stock, and it creates and sustains wealth for individual investors.\n\nCompanies raise money on the stock market by selling ownership stakes to investors. These equity stakes are known as shares of stock. By listing shares for sale on the stock exchanges that make up the stock market, companies get access to the capital they need to operate and expand their businesses without having to take on debt. In exchange for the privilege of selling stock to the public, companies are required to disclose information and give shareholders a say in how their businesses are run.\n\nInvestors benefit by exchanging their money for shares on the stock market. As companies put that money to work growing and expanding their businesses, investors reap the benefits as their shares of stock become more valuable over time, leading to capital gains. In addition, companies pay dividends to their shareholders as their profits grow.\n\nThe performances of individual stocks vary widely over time, but taken as a whole the stock market has historically rewarded investors with average annual returns of around 10%, making it one of the most reliable ways of growing your money.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.4668056964874268,
            "normalized": 0.5619444847106934
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 33,
        "original_data": {
            "instruction": "how is kirkland central park tennis center",
            "input": "",
            "output": "Kirkland Central Park Tennis Center is a great place to play tennis. It has 18 courts (12 indoor & 6 outdoor), a pro shop, and a mini-kitchen. Overall the courts are in good condition and the staff is friendly and helpful. The center offers a variety of programs and clinics for all levels of players from adults to young kids. The center is located in a convenient location.\nSomething you may want to know when you plan a visit to the center: the center can be busy during peak hours or during tournaments, it's not open all hours in a day and the membership fee is not cheap.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.8554365634918213,
            "normalized": 0.6729818752833775
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 34,
        "original_data": {
            "instruction": "Which characters belong to DC or Marvel Universe? Kent Nelson, Doctor Strange",
            "input": "",
            "output": "Kent Nelson is DC, Doctor Strange is Marvel",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 8.426360130310059,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.95,
            "strategy_details": {
                "raw": 1.9,
                "normalized": 1.9,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.42000000000000004
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 35,
        "original_data": {
            "instruction": "Give me a list of certain insurances that I should take when moving to Germany.",
            "input": "",
            "output": "1. Health Insurance (Krankenversicherung): It is mandatory to register to in Germany. Many people prefer Public but you can also enroll for private insurance.\n2. Private Liability Insurance (Haftpflichtversicherung): For accidental damage coverages. Although optional but highly recommended by residents.\n3. Legal Protection Insurance (Rechtsschutzversicherung): This is totally optional but might come handy to deal and send notices for legal or public disorder cases e.g. problems with landlord, etc.\n4. Household Insurance (Hausratversicherung): This is another good to have insurance to cover the damages related to house specially for rented homes.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.4040815830230713,
            "normalized": 0.8297375951494489
        },
        "potential_entropy": {
            "strategy_component": 0.9349999999999998,
            "strategy_details": {
                "raw": 1.8699999999999997,
                "normalized": 1.8699999999999997,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 36,
        "original_data": {
            "instruction": "Please list a few of the professional organizations that Judith Stern participated in during her career.",
            "input": "In 1974, Judith Stern joined the faculty of the University of California, Davis as an assistant professor in the Department of Nutrition. There her early research centered on differences in the adipose tissue of obese individuals – using Zucker rats as a research model. Eventually her research program grew to include human studies, focused on various obesity treatment regimens.\n\nIn 1995, Stern was elected to the National Academy of Medicine. At various points she was elected president of the NAASO (1992–1993, now The Obesity Society) and the American Society for Clinical Nutrition. Stern co-founded the American Obesity Association, an advocacy group that lobbied in favor of obesity research and treatment.",
            "output": "Some of the professional organizations that Judith Stern created or participated in are the National Academy of Medicine, the Obesity Society, and the American Society for Clinical Nutrition.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2809510231018066,
            "normalized": 0.7945574351719448
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 37,
        "original_data": {
            "instruction": "How can I initiate conversation with a stranger?",
            "input": "",
            "output": "First impression is one thing which leaves an everlasting impact on people we meet. So, meeting and greeting everyone with a smile is the key to leave that pleasant effect. \n\nNext, talking about general topics based on where you are meeting interests the person to talk to you. Make sure to ask open ended questions so that the person have the opportunity to speak a bit more.\n\nListening is the key to have a good conversation. During initial general conversation, try to find common interests to take the conversation forward. Make sure to show interest in the person you are speaking to.\n\nBody postures also is an important part of being a pro at conversation striker. Positive gestures and body postures makes one take interest in what you are speaking and also, make anyone feel you are interested in listening and making friends.\n\nFinally, practice makes a man perfect. If you are a beginner, try setting goal for yourself. For example, determine yourself to talk to at least three strangers in a week and try increasing the goal and one day you would be able to initiate a conversation effortlessly.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.3268821239471436,
            "normalized": 0.807680606842041
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 38,
        "original_data": {
            "instruction": "Which sub-Saharan colony was the first to gain independence?",
            "input": "Imperial rule by Europeans would continue until after the conclusion of World War II, when almost all remaining colonial territories gradually obtained formal independence. Independence movements in Africa gained momentum following World War II, which left the major European powers weakened. In 1951, Libya, a former Italian colony, gained independence. In 1956, Tunisia and Morocco won their independence from France. Ghana followed suit the next year (March 1957), becoming the first of the sub-Saharan colonies to be granted independence. Most of the rest of the continent became independent over the next decade.",
            "output": "Ghana was the first sub-Saharan colony to gain independence.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.802103519439697,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 39,
        "original_data": {
            "instruction": "Determine the company which produces the following automotive brands: Chevrolet, GMC, Dodge, Lexus, Buick, Cadillac, Pontiac, Acura, Lincoln, and Jeep.",
            "input": "",
            "output": "Chevrolet, GMC, Cadillac, and Buick are all brands of General Motors. Pontiac was also a brand of General Motors, but it was phased out in 2010.  Lincoln, Jeep, and Dodge are Stellantis brands. Lexus and Acura is the luxury brands of Toyota and Honda respectively.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.758085250854492,
            "normalized": 0.6451672145298549
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 40,
        "original_data": {
            "instruction": "Where was poutine invented?",
            "input": "",
            "output": "While it is the national dish of Quebec (a province of Canada) there are multiple cities that are claiming that poutine is from that city. For example Drummondville, Victoriaville, Warwick and Princeville.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            1,
            2
        ],
        "entropy": {
            "raw": 4.24021577835083,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 41,
        "original_data": {
            "instruction": "I'm considering buying a pocket knife for my child. How do I know which one I should buy?",
            "input": "",
            "output": "On the market there are many different versions of pocket knives for different purposes and qualities. There are even pocket knives designed specifically for younger children who have no or very little experience with pocket knives, where the blades are not as sharp as usual. Color, size, functions and cost are other factors to consider.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.1960604190826416,
            "normalized": 0.7703029768807548
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 42,
        "original_data": {
            "instruction": "Is there a high school in Fairdale",
            "input": "Fairdale High School is a high school located in the Fairdale area of Louisville, Kentucky, United States. It is part of the Jefferson County Public Schools. As of the 2013–14 school year, there were 1,804 students at the school.",
            "output": "Yes. The Fairdale High School is a high school located in the Fairdale area of Louisville, Kentucky, United States. It is part of the Jefferson County Public Schools. As of the 2013–14 school year, there were 1,804 students at the school.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 1.964843511581421,
            "normalized": 0.4185267175946917
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 43,
        "original_data": {
            "instruction": "What kind of immigration system is the best",
            "input": "",
            "output": "The best immigration system is a skills based system. One which does not have a fixed quota of immigrant allowance per year. The quota should vary based on the job market for skills in which the country is facing shortage. Students who come in to study in the country can be given preference but again dependent on the requirements of the job market. Immediate family immigration should be allowed. All processing should be done in a timely manner. Once the labor market is saturated make it extremely difficult to work in an undocumented status. Give options for undocumented immigrants to become legal based on their ties to the country and good conduct. Once people know that they have options to immigrate legally in both high and low skilled jobs, new illegal crossings will reduce.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.6279067993164062,
            "normalized": 0.8936876569475446
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 44,
        "original_data": {
            "instruction": "Is Paris a good city to live in?",
            "input": "",
            "output": "Paris is the so called city of light. Other people call it the museum city. The reason for this is that you can find art in almost every corner in Paris, and beauty is everywhere. However, the price for living in the city can be very high. It would be a good city lo live in if you enjoy being surrounded by culture, art, good restaurants and museums; however, you have to take into account that you will probably be living surrounded by tourist and paying a very high price.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.294174909591675,
            "normalized": 0.7983356884547642
        },
        "potential_entropy": {
            "strategy_component": 0.75,
            "strategy_details": {
                "raw": 1.5,
                "normalized": 1.5,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 45,
        "original_data": {
            "instruction": "Classify each of the following colors as primary or secondary: orange, red, blue, green, purple, yellow.",
            "input": "",
            "output": "Red, yellow and blue are primary colors. Orange, green and purple are secondary colors.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.2642524242401123,
            "normalized": 0.5040721212114606
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 46,
        "original_data": {
            "instruction": "separate the following into chilldrens and adults cartoons.\nrecess\nrugrats\nbig mouth\nswat cats\ninside job\npopeyes\ndexters lab\nking of the hill\nsouth park",
            "input": "",
            "output": "childrens cartoons: recess, rugrats, swat cats, popeyes, dexters lab\n\nadult cartoons: big mouth, inside job, king of the hill, south park",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 5.048919200897217,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 47,
        "original_data": {
            "instruction": "What animals can I see on safari?",
            "input": "",
            "output": "Typically, you can spot a variety of wildlife on Safari. Depending on the region, you can see lions, giraffes, elephants, leopards, wild birds, and more. The most popular safaris are found in Africa, in countries like Tanzania, South Africa and Kenya.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.4997432231903076,
            "normalized": 0.5713552066258022
        },
        "potential_entropy": {
            "strategy_component": 0.735,
            "strategy_details": {
                "raw": 1.47,
                "normalized": 1.47,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 48,
        "original_data": {
            "instruction": "Who are the top 5 basketball players of all time?",
            "input": "",
            "output": "The top 5 basketball players of all time will differ in opinion, depending on who you are talking to and in what region. But, a common list includes Michael Jordan, Lebron James, Kobe Bryant, Kareem Abdul Jabbar and Magic Johnson. Other players that are often mentioned in the conversation are Larry Bird, Bill Russel and Julius Erving.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.9786758422851562,
            "normalized": 0.7081930977957589
        },
        "potential_entropy": {
            "strategy_component": 1.1400000000000001,
            "strategy_details": {
                "raw": 2.2800000000000002,
                "normalized": 2.2800000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 49,
        "original_data": {
            "instruction": "What is a viral video?",
            "input": "",
            "output": "Viral video is one that is popular and generate a lot of views which is appealing for advertisers",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.6,
            "s_out_div": 0.3,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.4,
            "s_out_div": 0.7,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.031739234924316,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3,
            "strategy_details": {
                "raw": 2.6,
                "normalized": 2.6,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.4,
                    "out_div": 0.7,
                    "out_dens": 0.6,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 50,
        "original_data": {
            "instruction": "Summarize the effectiveness of anaerobic and aerobic exercises on muscle hypertropy.",
            "input": "The best approach to specifically achieve muscle growth remains controversial (as opposed to focusing on gaining strength, power, or endurance); it was generally considered that consistent anaerobic strength training will produce hypertrophy over the long term, in addition to its effects on muscular strength and endurance. Muscular hypertrophy can be increased through strength training and other short-duration, high-intensity anaerobic exercises. Lower-intensity, longer-duration aerobic exercise generally does not result in very effective tissue hypertrophy; instead, endurance athletes enhance storage of fats and carbohydrates within the muscles, as well as neovascularization.",
            "output": "Short-duration, high-intensity anaerobic exercises are more effective for increasing muscular tissue hypertrophy compared to lower-intensity, longer-duration aerobic exercises.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.631289482116699,
            "normalized": 0.6089398520333427
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 51,
        "original_data": {
            "instruction": "I want to watch a movie that won the Oscar for best picture. Here are my choices: Moonlight, Wizard of Oz, Arrival, Elvis, Everything Everywhere All at Once, Chicago, Gone with the Wind, Top Gun. What should I choose?",
            "input": "",
            "output": "These movies are Oscar winners for Best Picture: Moonlight, Everything Everywhere All at Once, Gone with the Wind and Chicago.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.5431582927703857,
            "normalized": 0.8694737979343959
        },
        "potential_entropy": {
            "strategy_component": 0.7124999999999999,
            "strategy_details": {
                "raw": 1.4249999999999998,
                "normalized": 1.4249999999999998,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 52,
        "original_data": {
            "instruction": "Name five NFL teams in the NFC.",
            "input": "",
            "output": "Five NFL teams in the NFC are the Carolina Panthers, New Orleans Saints, Philadelphia Eagles, New York Giants, and Detroit Lions.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 2.582390546798706,
            "normalized": 0.5949687276567731
        },
        "potential_entropy": {
            "strategy_component": 1.0075,
            "strategy_details": {
                "raw": 2.015,
                "normalized": 2.015,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 53,
        "original_data": {
            "instruction": "Write about apple",
            "input": "",
            "output": "Apple is a fruit. \n\nIn history, there are three apples that profoundly changed the world. \n- Eve's apple\n- Newton's apple\n- Steve Jobs' Apple\n\nBiblical Adam and Eve ate the forbidden fruit and committed the original sin. \nNewton discovered the law of gravity when an apple fell from tree. \nSteve Jobs co-founded Apple and several products like the iPhone changed the world.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.265688180923462,
            "normalized": 0.7901966231209892
        },
        "potential_entropy": {
            "strategy_component": 1.1875,
            "strategy_details": {
                "raw": 2.375,
                "normalized": 2.375,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 54,
        "original_data": {
            "instruction": "What is parkrun",
            "input": "Parkrun (stylised as parkrun) is a collection of 5-kilometre (3.1 mi) events for walkers, runners and volunteers that take place every Saturday morning at more than 2,000 locations in 22 countries across six continents. Junior Parkrun (stylised as junior parkrun) is a spin-off event that provides a 2 kilometres (1+1⁄4 mi) event for children aged 4–14 on a Sunday morning. Parkrun events are free to enter and are delivered by volunteers, supported by a small group of staff at its headquarters.",
            "output": "Parkrun is a community event that takes place each Saturday morning where entrants walk, jog, or run a 5-kilometre course. Events take place at a range of general locations including parks, forests, rivers, lakes, beaches, and nature reserves. The first ever parkrun event took place on 2 October 2004 in Bushy Park, London involving 13 runners. Today more than 3 million people across more than 2,000 locations in 22 countries participate in parkrun each week. Parkrun events are free to enter and are delivered by volunteers.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.432060480117798,
            "normalized": 0.5520172800336566
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 55,
        "original_data": {
            "instruction": "What is Skyrim?",
            "input": "The game is set 200 years after the events of Oblivion, and takes place in Skyrim, the northernmost province of Tamriel. Its main story focuses on the player's character, the Dragonborn, on their quest to defeat Alduin the World-Eater, a dragon who is prophesied to destroy the world. Over the course of the game, the player completes quests and develops the character by improving skills. The game continues the open world tradition of its predecessors by allowing the player to travel anywhere in the game world at any time, and to ignore or postpone the main storyline indefinitely.",
            "output": "Skyrim is a medieval fantasy game based in the fictional world of Tamriel. With it being the fifth game in the Elder Scrolls franchise, it has plenty of lore to draw upon. Like its predecessors, the game is an open world RPG with plenty of quests to do, monsters to kill, items to loot, skills to level and weapons to craft. The map and setting of Skyrim is heavily influenced by Norse mythology. It has become one of the most famous games to be created by Bethesda.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 2.913576126098633,
            "normalized": 0.6895931788853237
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 56,
        "original_data": {
            "instruction": "What is random access memory?",
            "input": "",
            "output": "Random access memory (RAM) is a type of computer memory used to store data that can be accessed quickly. It is also known as dynamic random access memory (DRAM) and is used to store the instructions and data for a computer's operating system and applications. RAM is volatile memory, meaning it will lose its data if the power is turned off. It is also much faster than other types of memory such as hard disk drives and solid state drives.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 1.336638331413269,
            "normalized": 0.239039523260934
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 57,
        "original_data": {
            "instruction": "Give me 3 ideas for hobbies that I can recommend to somebody who is retiring. These should all be activities that require gentle movement and are not overly demanding on the body.",
            "input": "",
            "output": "Retirement is an exciting opportunity for people to develop and cultivate their passions. Here are a few ideas you could recommend that are both gentle on the body but require some movement:\n- Gardening: while this is a seasonal pursuit, deciding on which seeds to plant, nursing them through a growing season, and learning the ins and outs of the hobby can become a lifelong passion for those with a \"green thumb\"\n- Walking and picking up trash: carry a bag and \"grabber\" pinchers to collect trash in your community while staying active on a walk. Rubber-tipped pinchers means you will not need to bend or stretch too far to reach, and you will be surprised how much of a difference this makes to your community members!\n- Fishing: most fishing can be done from the banks of a river or lake, and is a great way to experience the calming effects of a body of water while still being physically active. Whether fly fishing or reel fishing, many people who have entered retirement find this an engaging way to engage with nature either solo or as part of a community of fisher-people.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.3036816120147705,
            "normalized": 0.8010518891470773
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 58,
        "original_data": {
            "instruction": "Given this paragraph about the Southwestern United States, name the largest cities in that area",
            "input": "The Southwestern United States, also known as the American Southwest or simply the Southwest, is a geographic and cultural region of the United States. Its exact boundaries are not officially defined, but attempts to do so generally include Arizona and New Mexico as the core of the region. Adjacent portions of California, Colorado, Nevada, and Utah are also sometimes included. Oklahoma and Texas may also be included, particularly the trans-pecos \"horn\" portion of the state(i.e. the far western extension of the state of the state south of the New Mexico border, but west and north of the Pecos and Rio Gande Rivers). The largest cities by metropolitan area are Phoenix, Las Vegas, El Paso, Albuquerque, and Tucson. Before 1848, in the historical region of Santa Fe de Nuevo México as well as parts of Alta California and Coahuila y Tejas, settlement was almost non-existent outside of Nuevo México's Pueblos and Spanish or Mexican municipalities. Much of the area had been a part of New Spain and Mexico until the United States acquired the area through the Treaty of Guadalupe Hidalgo in 1848 and the smaller Gadsden Purchase in 1854.",
            "output": "The largest cities in the Southwestern United States are Phoenix, Las Vegas, El Paso, Albuquerque, and Tucson.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.950267791748047,
            "normalized": 0.7000765119280133
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 59,
        "original_data": {
            "instruction": "What is a mobile phone?",
            "input": "",
            "output": "A mobile phone is a device that lets the world (e.g. people, companies, friends, and family) stay in contact with each other.  They other capability such as text messaging, internet browsing, watching movies, reading news, navigation, and taking photos.  They service a core component in everyday living similar to refrigerators and microwaves.  They also serve as a way for people to interact with different social media platforms and entertainment.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.2773804664611816,
            "normalized": 0.7935372761317662
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 60,
        "original_data": {
            "instruction": "Who painted the ‘Girl with the Pearl Earring’ and the ‘Milkmaid’?",
            "input": "",
            "output": "The ‘Girl with the Pearl Earring’ and the ‘Milkmaid’ were painted by Johannes Vermeer.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.5,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.5,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.579281806945801,
            "normalized": 0.8797948019845145
        },
        "potential_entropy": {
            "strategy_component": 0.59,
            "strategy_details": {
                "raw": 1.18,
                "normalized": 1.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.5,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 61,
        "original_data": {
            "instruction": "Given a reference text about Will Ferrell, tell me where he kickstarted his career, what group of comic actors he is considered part of, and how many Golden Globe Award nominations he has.",
            "input": "John William Ferrell (/ˈfɛrəl/; born July 16, 1967) is an American actor, comedian, writer, and producer. Ferrell first established himself in the mid-1990s as a cast member on the NBC sketch comedy series Saturday Night Live, where he performed from 1995 to 2002, and has subsequently starred in comedy films such as Elf (2003), Anchorman: The Legend of Ron Burgundy (2004), Kicking & Screaming (2005), Talladega Nights: The Ballad of Ricky Bobby (2006), Semi-Pro (2008), Step Brothers (2008), and Land of the Lost (2009). He founded the comedy website Funny or Die in 2007 with his writing partner Adam McKay. Other notable film roles include Stranger than Fiction (2006), The Other Guys (2010), The Campaign (2012), Get Hard (2015), Holmes & Watson (2018), and the animated films Curious George (2006), Megamind (2010) and The Lego Movie film franchise (2014–2019).\n\nFerrell is considered a member of the \"Frat Pack\", a generation of leading Hollywood comic actors who emerged in the late 1990s and the 2000s, including Jack Black, Ben Stiller, Steve Carell, Vince Vaughn, Paul Rudd, and brothers Owen and Luke Wilson. He received an Emmy Award nomination for his work on Saturday Night Live, and three Golden Globe Award nominations for acting in The Producers (2005) and Stranger than Fiction (2006) and for producing Vice (2018). He was also named the best comedian of 2015 in the British GQ Men of the Year awards. Ferrell received a star on the Hollywood Walk of Fame on March 24, 2015.",
            "output": "Will Ferrell kickstarted his career on the NBC sketch comedy series \"Saturday Night Live\". He is considered part of the \"Frat Pack\", and has received three Golden Globe Award nominations.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.9486162662506104,
            "normalized": 0.9853189332144601
        },
        "potential_entropy": {
            "strategy_component": 0.29500000000000004,
            "strategy_details": {
                "raw": 0.5900000000000001,
                "normalized": 0.5900000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 62,
        "original_data": {
            "instruction": "Summarize what GDP means as well as what it is and is not used for.",
            "input": "Gross domestic product (GDP) is a monetary measure of the market value of all the final goods and services produced and sold in a specific time period by a country or countries, generally \"without double counting the intermediate goods and services used up to produce them\".[citation needed] GDP is most often used by the government of a single country to measure its economic health. Due to its complex and subjective nature, this measure is often revised before being considered a reliable indicator.[citation needed] GDP (nominal) per capita does not, however, reflect differences in the cost of living and the inflation rates of the countries; therefore, using a basis of GDP per capita at purchasing power parity (PPP) may be more useful when comparing living standards between nations, while nominal GDP is more useful comparing national economies on the international market. Total GDP can also be broken down into the contribution of each industry or sector of the economy. The ratio of GDP to the total population of the region is the per capita GDP (also called the Mean Standard of Living).\n\nGDP definitions are maintained by a number of national and international economic organizations. The Organisation for Economic Co-operation and Development (OECD) defines GDP as \"an aggregate measure of production equal to the sum of the gross values added of all resident and institutional units engaged in production and services (plus any taxes, and minus any subsidies, on products not included in the value of their outputs)\". An IMF publication states that, \"GDP measures the monetary value of final goods and services—that are bought by the final user—produced in a country in a given period of time (say a quarter or a year).\"\n\nGDP is often used as a metric for international comparisons as well as a broad measure of economic progress. It is often considered to be the world's most powerful statistical indicator of national development and progress. However, critics of the growth imperative often argue that GDP measures were never intended to measure progress, and leave out key other externalities, such as resource extraction, environmental impact and unpaid domestic work. Critics frequently propose alternative economic models such as doughnut economics which use other measures of success or alternative indicators such as the OECD's Better Life Index as better approaches to measuring the effect of the economy on human development and well being.",
            "output": "Gross Domestic Product (GDP) is a measurement of all of the goods and services sold in a specific country over a specific time period. It is used as a metric for economic health, economic progress, and for international comparisons. GDP is not a used to measure cost of living differences and inflation rates between countries.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.2530181407928467,
            "normalized": 0.5008623259408134
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 63,
        "original_data": {
            "instruction": "Is it fun to play guitar? How do you start playing guitar?",
            "input": "",
            "output": "Playing guitar is a worthwhile and super fun exercise for anyone, but especially those that are musically inclined.  Even for a beginner, its not difficult to be able to start playing simple songs from great artists like the Rolling Stones, The Beatles, Jimi Hendrix, or even Led Zeppelin.  The best way to get started is by understanding open chords, both major and minor, and simple string picking patterns.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.926274299621582,
            "normalized": 0.6932212284633091
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 64,
        "original_data": {
            "instruction": "When is the Indian Teachers day celebrated?",
            "input": "",
            "output": "Indian Teachers day is celebrated on 5th September",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.434751033782959,
            "normalized": 0.8385002953665597
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 65,
        "original_data": {
            "instruction": "Please give an overview of the show \"Scrubs\" highlighting unique characteristics of the sitcom.",
            "input": "Scrubs focuses on the unique point of view of its main character and narrator, Dr. John Michael \"J.D.\" Dorian (Zach Braff) for the first eight seasons, with season nine being narrated by the new main character Lucy Bennett (Kerry Bishé). Most episodes feature multiple story lines thematically linked by voice-overs done by Braff, as well as the comical daydreams of J.D. According to Bill Lawrence, \"What we decided was, rather than have it be a monotone narration, if it's going to be Zach's voice, we're going to do everything through J.D.'s eyes. It opened up a visual medium that those of us as comedy writers were not used to.\" Actors were given the chance to improvise their lines on set with encouragement by series creator Bill Lawrence, with Neil Flynn and Zach Braff being the main improvisors.\n\nAlmost every episode title for the first eight seasons begins with the word \"My\". Bill Lawrence says this is because each episode is Dr. John Dorian writing in his diary (revealed in the commentary on the DVD of the first-season episode \"My Hero\"). A few episodes are told from another character's perspective and have episode titles such as \"His Story\" or \"Her Story\". Apart from a brief period of narration from J.D. at the beginning and the end, these episodes primarily contain internal narration from other characters besides J.D. The transfer of the narration duties usually occurs at a moment of physical contact between two characters. Starting with season nine, the episode titles start with \"Our...\" as the focus has shifted from the perspective of J.D. to a new group of medical students. The webisodes that accompanied season eight, Scrubs: Interns, also were named \"Our...\".",
            "output": "The television show Scrubs aired from 2001 to 2010 and was filmed primarily through the point of view of the main character, John Michael \"J.D.\" Dorian. These episodes were all titled with the format of \"My...\" to further highlight J.D.'s point of view. Episodes not filmed through J.D.'s point of view were titled with \"His...,\" \"Her...,\" or \"Their...\" to allow other characters to narrate and tell a story through their point of view.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.6114559173583984,
            "normalized": 0.6032731192452567
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 66,
        "original_data": {
            "instruction": "Give me a list of fruit that are yellow color",
            "input": "",
            "output": "Lemon, banana, mango, pineapple, yellow dragonfruit, yellow watermelon, golden kiwi",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.772430896759033,
            "normalized": 0.9349802562168666
        },
        "potential_entropy": {
            "strategy_component": 1.105,
            "strategy_details": {
                "raw": 2.21,
                "normalized": 2.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 67,
        "original_data": {
            "instruction": "Classify each of the following as either titles by J.K. Rowling or Suzanne Collins: Harry Potter and the Sorcerer's Stone, The Ickabog, Fantastic Beasts and Where to Find Them, The Cuckoo's Calling, Harry Potter and the Order of the Phoenix, Catching Fire, Mockingjay, Gregor the Overlander, Year of the Jungle, The Ballad of Songbirds and Snakes",
            "input": "",
            "output": "J.K. Rowling: Harry Potter and the Sorcerer's Stone, The Ickabog, Fantastic Beasts and Where to Find Them, The Cuckoo's Calling, Harry Potter and the Order of the Phoenix\nSuzanne Collins: Catching Fire, Mockingjay, Gregor the Overlander, Year of the Jungle, The Ballad of Songbirds and Snakes",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.761627197265625,
            "normalized": 0.64617919921875
        },
        "potential_entropy": {
            "strategy_component": 0.265,
            "strategy_details": {
                "raw": 0.53,
                "normalized": 0.53,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 68,
        "original_data": {
            "instruction": "Which F1 drivers race for Red Bull Racing in the 2023 season?",
            "input": "",
            "output": "Max Verstappen and Sergio Perez",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.196799278259277,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.815,
            "strategy_details": {
                "raw": 1.63,
                "normalized": 1.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 69,
        "original_data": {
            "instruction": "how do i heal my mind",
            "input": "",
            "output": "slow down your breathe",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.3,
            "s_out_div": 0.1,
            "s_out_dens": 0.4,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.6,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 9.132806777954102,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.48,
            "strategy_details": {
                "raw": 2.96,
                "normalized": 2.96,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.7000000000000002,
                    "output": 0.8999999999999999
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.7,
                    "out_div": 0.9,
                    "out_dens": 0.6,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 70,
        "original_data": {
            "instruction": "Why is Pickleball getting so popular?",
            "input": "",
            "output": "Pickleball has been getting popular for a number of reasons. First, it is a relatively easy game to get into; you don't need that much equipment or skill to be able to start to play. Second, you can play with people of all ages. Because the games are short, you mix with a lot of people and it's a great way to meet others, making it a great social activity as well. Lastly, covid-19 limited many activities and sports that could be played. But Pickleball can be played in a small space (mainly outdoors) and thus many people picked up this sport during that time. Popularity has continued to grow since now athletes are purchasing professional teams and Pickleball is being televised more often!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.9830620288848877,
            "normalized": 0.7094462939671108
        },
        "potential_entropy": {
            "strategy_component": 0.735,
            "strategy_details": {
                "raw": 1.47,
                "normalized": 1.47,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 71,
        "original_data": {
            "instruction": "When was The White Mandigos band formed?",
            "input": "The White Mandingos are a rock supergroup from Woodstock, New York consisting of rapper Murs, former Rolling Stone journalist and MTV / VH1 producer Sacha Jenkins and Bad Brains' bassist Darryl Jenifer.\n\nBiography\nThe band was formed in late 2012 when Jenkins met up at Jenifer's house in Woodstock to discover if there was anything in common between their respective favourite music genres. They considered their initial collaborations unimpressive, so Jenkins suggested collaborating with Murs, who provided lyrics.\n\nTheir first album, The Ghetto Is Tryna Kill Me was released in June 2013, and followed with a short tour of the eastern United States, including gigs in New York's New Museum, Boston and Washington DC. The album is a concept album around Tyrone White, a young black man from a New York City housing project, who subsequently obtains a recording contract and gets a white girlfriend. Jenifer and Jenkins have described the album Tommy by The Who as an important influence. Reviewing the album, Baltimore City Paper 's Baynard Woods thought the group \"actually manage to do service to punk and hip hop\" and praised the band's sense of humour, particularly the music video for their first single, \"My First White Girl\". Washington City Paper's Marcus J Moore described the video for the group's \"Warn A Brotha\" as \"a cool ode to skateboarding\".",
            "output": "The White Mandingos was formed in late 2012, and their first album 'The Guetto is Tryna Kill Me' was released in June 2013.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.5926239490509033,
            "normalized": 0.8836068425859723
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 72,
        "original_data": {
            "instruction": "Who are the best hitters in major league baseball I've never seen?",
            "input": "",
            "output": "Understanding your birth date is in 1973, there are hundreds of great hitters that played before you were born. In the 1940's you had Joe Dimaggio and Ted Williams. The 1950's featured Mickey Mantle and Stan Musial. By the time the 1960's started Roger Maris and Willie Mays were leading their leagues in hitting. This is just a short list of great hitters that played before you were born.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.8875772953033447,
            "normalized": 0.6821649415152413
        },
        "potential_entropy": {
            "strategy_component": 0.735,
            "strategy_details": {
                "raw": 1.47,
                "normalized": 1.47,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 73,
        "original_data": {
            "instruction": "What film won Best Picture at the 95th Academy Awards in 2023?",
            "input": "",
            "output": "Everything Everywhere All at Once",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 10.764814376831055,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3150000000000002,
            "strategy_details": {
                "raw": 2.6300000000000003,
                "normalized": 2.6300000000000003,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.85,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 74,
        "original_data": {
            "instruction": "Who are the almoravids ?",
            "input": "The Almoravid dynasty (Arabic: المرابطون, romanized: Al-Murābiṭūn, lit. 'those from the ribats') was an imperial Berber Muslim dynasty centered in the territory of present-day Morocco. It established an empire in the 11th century that stretched over the western Maghreb and Al-Andalus, starting in the 1050s and lasting until its fall to the Almohads in 1147. The Almoravid capital was Marrakesh, a city founded by the Almoravid leader Abu Bakr ibn Umar c. 1070. The dynasty emerged from a coalition of the Lamtuna, Gudala, and Massufa, nomadic Berber tribes living in what is now Mauritania and the Western Sahara, traversing the territory between the Draa, the Niger, and the Senegal rivers.\n\n.",
            "output": "The Almoravid was imperial Berber Muslim Dynasty. They rulled in Morocco from 1050 until 1147 and Marrakech were their capital. They emerged from a coalition of the Lamtuna, Massufa and Gudala, nomad berbers l.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.462965965270996,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43999999999999995,
            "strategy_details": {
                "raw": 0.8799999999999999,
                "normalized": 0.8799999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 75,
        "original_data": {
            "instruction": "what is rain ?",
            "input": "",
            "output": "Water from various water bodies like lakes, oceans along with water from plants & trees evaporate as vapour due to the heat from sun. This rises, cools and changes into tiny droplets of water which form clouds. Multiple tiny droplets join to form bigger water drops and once they become heavy enough, they come down as rain. Small droplets are called drizzles and the bigger ones as rain !",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.8,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.2825515270233154,
            "normalized": 0.7950147220066616
        },
        "potential_entropy": {
            "strategy_component": 1.18,
            "strategy_details": {
                "raw": 2.36,
                "normalized": 2.36,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 76,
        "original_data": {
            "instruction": "How much time violinist Ling Ling spent practicing each day?",
            "input": "In 2017, TwoSet Violin made a comedic reference to Ling Ling, a fictional violinist who \"practices 40 hours a day\". In an interview with Yle Uutiset, they described Ling Ling as the final boss of a video game: the Chuck Norris of violinists. Chen said they improvised the character from their comedy sketch video concerning a teenage violin student's tiger mom comparing the student to her friend's child. In 2018, they released a series of videos called the Ling Ling Workout. In these challenges, the duo play classical pieces (or contemporary music) with handicaps such as playing at double speed, with scordatura, while dancing or hula hooping, with hand positions reversed, or while upside down. Prominent violinists such as Ray Chen, Ziyu He, and Hilary Hahn have also attempted the challenge on their channel.",
            "output": "Ling Ling is the Chuck Norris of violinists.  Her violin performance has far surpassed all living violin virtuosos today.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.5,
            "s_out_div": 0.4,
            "s_out_dens": 0.3,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.5,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.736175060272217,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.86,
            "strategy_details": {
                "raw": 1.72,
                "normalized": 1.72,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.78
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.5,
                    "out_div": 0.6,
                    "out_dens": 0.7,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 77,
        "original_data": {
            "instruction": "Where is the Willis Tower located?",
            "input": "",
            "output": "The Willis Tower, formerly known as the Sears Tower, is located in the city of Chicago.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.108917236328125,
            "normalized": 0.45969063895089285
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 78,
        "original_data": {
            "instruction": "What is baseball?",
            "input": "",
            "output": "Baseball is a sport.  It is played on a baseball field by 9 players on each team.  Each team takes turn fielding and pitching.  A normal game is nine innings.  An inning is when each team gets three outs while on defense.  Baseball is considered America's pastime due to its popularity.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.044421672821045,
            "normalized": 0.7269776208060128
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 79,
        "original_data": {
            "instruction": "Who are the top NFL Quarterbacks?",
            "input": "",
            "output": "Tom Brady\nPeyton Manning\nDrew Brees\nAaron Rodgers\nJosh Allen\nPatrick Mahomes\nJalen Hurts\nLamar Jackson\nRussell Wilson\nPhilip Rivers\nEli Manning\nJustin Herbert",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.6281023025512695,
            "normalized": 0.6080292293003627
        },
        "potential_entropy": {
            "strategy_component": 0.9349999999999999,
            "strategy_details": {
                "raw": 1.8699999999999999,
                "normalized": 1.8699999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 80,
        "original_data": {
            "instruction": "What mammal lays the largest eggs?",
            "input": "",
            "output": "The echidna, an Australian monotreme, is the largest egg-laying mammal.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.795452833175659,
            "normalized": 0.6558436666216169
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 81,
        "original_data": {
            "instruction": "classify the following as mammals vs reptiles:\ngoat\nsnake\ncockroach\npig\ndog\ncaterpillar\nlion",
            "input": "",
            "output": "mammals: goat, pig, dog, lion\nreptiles: snack, cockroach, caterpillar",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.6577811241149902,
            "normalized": 0.9022231783185687
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 82,
        "original_data": {
            "instruction": "Based on the above passage name the regions which is at the North border of Saudi Arabia . Display the results in a comma separated format.",
            "input": "Saudi Arabia, officially the Kingdom of Saudi Arabia (KSA), is a country in Western Asia. It covers the bulk of the Arabian Peninsula, and has a land area of about 2,150,000 km2 (830,000 sq mi), making it the fifth-largest country in Asia, the second-largest in the Arab world, and the largest in Western Asia and the Middle East. It is bordered by the Red Sea to the west; Jordan, Iraq, and Kuwait to the north; the Persian Gulf, Qatar and the United Arab Emirates to the east; Oman to the southeast; and Yemen to the south. Bahrain is an island country off its east coast. The Gulf of Aqaba in the northwest separates Saudi Arabia from Egypt and Israel. Saudi Arabia is the only country with a coastline along both the Red Sea and the Persian Gulf, and most of its terrain consists of arid desert, lowland, steppe, and mountains. Its capital and largest city is Riyadh. The country is home to Mecca and Medina, the two holiest cities in Islam.\n\nPre-Islamic Arabia, the territory that constitutes modern-day Saudi Arabia, was the site of several ancient cultures and civilizations; the prehistory of Saudi Arabia shows some of the earliest traces of human activity in the world. The world's second-largest religion, Islam, emerged in what is now Saudi Arabia. In the early 7th century, the Islamic prophet Muhammad united the population of the Arabian Peninsula and created a single Islamic religious polity. Following his death in 632, his followers rapidly expanded the territory under Muslim rule beyond Arabia, conquering huge and unprecedented swathes of territory (from the Iberian Peninsula in the west to parts of Central and South Asia in the east) in a matter of decades. Arab dynasties originating from modern-day Saudi Arabia founded the Rashidun (632–661), Umayyad (661–750), Abbasid (750–1517), and Fatimid (909–1171) caliphates, as well as numerous other dynasties in Asia, Africa, and Europe.",
            "output": "Jordan, Iraq, Kuwait",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 7.422006130218506,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 83,
        "original_data": {
            "instruction": "Given this reference text about the Grateful Dead, tell me who their keyboard players were over the years.",
            "input": "The Grateful Dead was founded in the San Francisco Bay Area amid the rise of the counterculture of the 1960s. The founding members were Jerry Garcia (lead guitar, vocals), Bob Weir (rhythm guitar, vocals), Ron \"Pigpen\" McKernan (keyboards, harmonica, vocals), Phil Lesh (bass, vocals), and Bill Kreutzmann (drums). Members of the Grateful Dead, originally known as the Warlocks, had played together in various Bay Area ensembles, including the traditional jug band Mother McCree's Uptown Jug Champions. Lesh was the last member to join the Warlocks before they changed their name to the Grateful Dead; replacing Dana Morgan Jr., who had played bass for a few gigs. Drummer Mickey Hart and non-performing lyricist Robert Hunter joined in 1967. With the exception of McKernan, who died in 1973, and Hart, who took time off from 1971 to 1974, the core of the band stayed together for its entire 30-year history. The other official members of the band are Tom Constanten (keyboards; 1968–1970), John Perry Barlow (non-performing lyricist; 1971–1995), Keith Godchaux (keyboards, occasional vocals; 1971–1979), Donna Godchaux (vocals; 1972–1979), Brent Mydland (keyboards, vocals; 1979–1990), and Vince Welnick (keyboards, vocals; 1990–1995). Bruce Hornsby (accordion, piano, vocals) was a touring member from 1990 to 1992, as well as a guest with the band on occasion before and after the tours.",
            "output": "Ron \"Pigpen\" McKernan was the original keyboard player of the Grateful Dead. Others to play keyboards for the Grateful Dead include Tom Constanten, Keith Godchaux, Brent Mydland and Vince Welnick.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.308950901031494,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.61,
            "strategy_details": {
                "raw": 1.22,
                "normalized": 1.22,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 84,
        "original_data": {
            "instruction": "List the first four main characters of Ann M. Martin's \"The Baby-Sitters Club\" books.",
            "input": "",
            "output": "Kristy Thomas, Mary Anne Spier, Claudia Kishi, and Stacey McGill",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.544833183288574,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 85,
        "original_data": {
            "instruction": "Given the lineage of this historical military aircraft, when was the Avro Arrow first flown and what were the operating characteristics of that flight?",
            "input": "The Avro Canada CF-105 Arrow was a delta-winged interceptor aircraft designed and built by Avro Canada. The CF-105 held the promise of Mach 2 speeds at altitudes exceeding 50,000 feet (15,000 m) and was intended to serve as the Royal Canadian Air Force's (RCAF) primary interceptor into the 1960s and beyond.\n\nThe Arrow was the culmination of a series of design studies begun in 1953 that examined improved versions of the Avro Canada CF-100 Canuck. After considerable study, the RCAF selected a dramatically more powerful design, and serious development began in March 1955. The aircraft was intended to be built directly from the production line, skipping the traditional hand-built prototype phase. The first Arrow Mk. 1, RL-201, was rolled out to the public on 4 October 1957, the same day as the launch of Sputnik I.\n\nFlight testing began with RL-201 on 25 March 1958, and the design quickly demonstrated excellent handling and overall performance, reaching Mach 1.9 in level flight. Powered by the Pratt & Whitney J75, another four Mk. 1s were completed, RL-202, RL-203, RL-204 and RL-205. The lighter and more powerful Orenda Iroquois engine was soon ready for testing, and the first Mk 2 with the Iroquois, RL-206, was ready for taxi testing in preparation for flight and acceptance tests by RCAF pilots by early 1959.",
            "output": "The Avro Canada RL-201 Arrow was first flown in a test flight on March 25, 1958. During that flight, handling and performance proved excellent and the aircraft reached speeds of up to Mach 1.9 in level flight.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.054286241531372,
            "normalized": 0.7297960690089634
        },
        "potential_entropy": {
            "strategy_component": 0.2875,
            "strategy_details": {
                "raw": 0.575,
                "normalized": 0.575,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.16499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 86,
        "original_data": {
            "instruction": "Name the main crochet stitches",
            "input": "",
            "output": "The main crochet stitches include the single crochet stitch, the double crochet stitch, the half double crochet stitch, the treble stitch and the slip stitch",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.4360735416412354,
            "normalized": 0.553163869040353
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 87,
        "original_data": {
            "instruction": "Given a reference text about the Recording Industry Association of America (RIAA) awards, tell me how many albums need to be sold to get an award.",
            "input": "In the United States, the Recording Industry Association of America (RIAA) awards certification based on the number of albums and singles sold through retail and other ancillary markets. Other countries have similar awards (see music recording certification). Certification is not automatic; for an award to be made, the record label must first request certification. The audit is conducted against net shipments after returns (most often an artist's royalty statement is used), which includes albums sold directly to retailers and one-stops, direct-to-consumer sales (music clubs and mail order) and other outlets.\n\nDescription and qualifications\n\nA platinum award for the album GHV2 of Madonna\nA Gold record is a song or album that sells 500,000 units (records, tapes, and compact discs). The award was launched in 1958; originally, the requirement for a Gold single was one million units sold and a Gold album represented $1 million in sales (at wholesale value, around a third of the list price). In 1975, the additional requirement of 500,000 units sold was added for Gold albums. Reflecting growth in record sales, the Platinum award was added in 1976, for albums able to sell one million units, and singles selling two million units. The Multi-Platinum award was introduced in 1984, signifying multiple Platinum levels of albums and singles. In 1989, the sales thresholds for singles were reduced to 500,000 for Gold and 1,000,000 for Platinum, reflecting a decrease in sales of singles. In 1992, RIAA began counting each disc in a multi-disc set as one unit toward certification. Reflecting additional growth in music sales, the Diamond award was instituted in 1999 for albums or singles selling ten million units. Because of these changes in criteria, the sales level associated with a particular award depends on when the award was made.",
            "output": "As of 1989, a song or album needed to exceed a sales threshold of 500,000 for Gold and 1,000,000 for Platinum. In 1999 after a growth in music sales, the Diamond award was introduced for songs or albums that sold over 10,000,000 units.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.304293155670166,
            "normalized": 0.515512330191476
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 88,
        "original_data": {
            "instruction": "Tell me some of the rivers that flow through Europe, in alphabetical order.",
            "input": "",
            "output": "In alphabetical order, some of the major European rivers are: Danube River, Elbe River, Loire River, Oder River, Rhine River, River Shannon and the Tagus River.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.775829553604126,
            "normalized": 0.6502370153154645
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 89,
        "original_data": {
            "instruction": "What equipment is required for scuba diving?",
            "input": "",
            "output": "Scuba diving requires\n* An air (or enriched air) tank\n* Breathing device - typically an open circuit regulator  or a closed circuit rebreather\n* Buoyancy Control Device (bcd)\n* Depth gauge\n* Submersible Pressure Gauge\n* Dive Computer or Watch\n* Mask\n* Fins\nOptional equipment\n* Wetsuit\n* Weights\n* Surface Marker Buoy\n* Dive Light",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.976824998855591,
            "normalized": 0.7076642853873116
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 90,
        "original_data": {
            "instruction": "What is Pinot Grigio?",
            "input": "Pinot Gris, Pinot Grigio (US: /ˈpiːnoʊ ˈɡriːdʒioʊ, -dʒoʊ/, UK: /ˈɡrɪdʒioʊ/) or Grauburgunder is a white wine grape variety of the species Vitis vinifera. Thought to be a mutant clone of the Pinot Noir variety, it normally has a grayish-blue fruit, accounting for its name, but the grapes can have a brownish pink to black and even white appearance. The word pinot could have been given to it because the grapes grow in small pinecone-shaped clusters. The wines produced from this grape also vary in color from a deep golden yellow to copper and even a light shade of pink, and it is one of the more popular grapes for skin-contact wine.\n\nPinot Gris is grown around the globe, with the \"spicy\" full-bodied Alsatian and lighter-bodied, more acidic Italian styles being most widely recognized. The Alsatian style, often duplicated in New World wine regions such as Marlborough, Tasmania, South Australia, Washington, Oregon, and South Africa tend to have moderate to low acidity, higher alcohol levels and an almost \"oily\" texture that contributes to the full-bodied nature of the wine. The flavors can range from ripe tropical fruit notes of melon and mango to some botrytis-influenced flavors. In Italy, Pinot grigio grapes are often harvested early to retain the refreshing acidity and minimize some of the overt-fruitiness of the variety, creating a more neutral flavor profile. This style is often imitated in other Old World wine regions, such as Germany, where the grape is known as Ruländer, or more commonly, Grauburgunder.",
            "output": "Pinot Gris, Pinot Grigio (US: /ˈpiːnoʊ ˈɡriːdʒioʊ, -dʒoʊ/, UK: /ˈɡrɪdʒioʊ/) or Grauburgunder is a white wine grape variety of the species Vitis vinifera. Pinot Gris is grown around the globe, with the \"spicy\" full-bodied Alsatian and lighter-bodied, more acidic Italian styles being most widely recognized.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1673707962036133,
            "normalized": 0.7621059417724609
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 91,
        "original_data": {
            "instruction": "What are the best Sherlock Holmes short stories?",
            "input": "",
            "output": "While \"best\" can be subjective, the author of the Sherlock Holmes series, Sir Arthur Conan Doyle, selected his own list of 12 short stories that he thought were the best. His list was this: \n1. The Speckled Band\n2. The Red-Headed League\n3. The Dancing Men\n4. The Final Problem\n5. A Scandal in Bohemia\n6. The Empty House\n7. The Five Orange Pips\n8. The Second Stain\n9. The Devil’s Foot\n10. The Priory School\n11. The Musgrave Ritual\n12. The Reigate Squires",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.424569845199585,
            "normalized": 0.5498770986284528
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 92,
        "original_data": {
            "instruction": "Fix spelling, grammar and factual mistakes in the text below:\n\nFinland prime minister, Sana Marrin, has lost her battle to stay in power after her centre-left Social Democratic party (SDP) was narrowly beat into third place in a cliffhanger election by its conservative and far-right rivals.\n\nWith 99.4% of votes counted on sunday, the right-wing National Coalition party was on course to secure 20.7% of the vote, with the populist, nation-first Finns party predicted to score 20.1%. Marin’s SDP was forecast to collect 19.9%.\n\nMarin congratulated the election winners during her concesion speech, but hailed an improvement in both her party’s vote share and its projected number of MPs. “It’s a really good achievement, even though I didn’t finish first today,” she told supporters in Helsinki.\n\n“Democrcy has spoken, the Finnish people have casted their vote and the celebration of democracy is always a wonderful thing,” she added. “We have good reason to be hapy about this result.”",
            "input": "",
            "output": "Finland’s prime minister, Sanna Marin, has lost her battle to stay in power after her centre-left Social Democratic party (SDP) was narrowly beaten into third place in a cliffhanger election by its conservative and far-right rivals.\n\nWith 99.4% of votes counted on Sunday, the right-wing National Coalition party was on course to secure 20.7% of the vote, with the populist, nation-first Finns party predicted to score 20.1%. Marin’s SDP was forecast to collect 19.9%.\n\nMarin congratulated the election winners during her concession speech, but hailed an improvement in both her party’s vote share and its projected number of MPs. “It’s a really good achievement, even though I didn’t finish first today,” she told supporters in Helsinki.\n\n“Democracy has spoken, the Finnish people have cast their vote, and the celebration of democracy is always a wonderful thing,” she added. “We have good reason to be happy about this result.”",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.98,
            "s_out_div": 0.9,
            "s_out_dens": 0.92,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.020000000000000018,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.07999999999999996,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.1350932121276855,
            "normalized": 0.7528837748936245
        },
        "potential_entropy": {
            "strategy_component": 0.253,
            "strategy_details": {
                "raw": 0.506,
                "normalized": 0.506,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.09599999999999999
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.020000000000000018,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.07999999999999996,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 93,
        "original_data": {
            "instruction": "What is mitosis",
            "input": "",
            "output": "A type of cell division that results in two daughter cells each having the same number and kind of chromosomes as the \"parent\" chromosome.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.3,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.7,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.02370285987854,
            "normalized": 0.7210579599652972
        },
        "potential_entropy": {
            "strategy_component": 1.4049999999999998,
            "strategy_details": {
                "raw": 2.8099999999999996,
                "normalized": 2.8099999999999996,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 2.0,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.7,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 94,
        "original_data": {
            "instruction": "What family do tigers, lions and panthers all belong to?",
            "input": "",
            "output": "Tigers, lions and panthers are all a part of the cat family",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.2,
            "s_out_dens": 0.95,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.8,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 3.261383295059204,
            "normalized": 0.7889666557312012
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.8,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 95,
        "original_data": {
            "instruction": "The first human case of Monkeypox was recorded in",
            "input": "",
            "output": "Democratic Republic of the Congo(DRC)",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.7907211780548096,
            "normalized": 0.9402060508728027
        },
        "potential_entropy": {
            "strategy_component": 1.2925,
            "strategy_details": {
                "raw": 2.585,
                "normalized": 2.585,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.40499999999999997
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 96,
        "original_data": {
            "instruction": "Recommend me a SCI FI fiction book",
            "input": "",
            "output": "The Three body could be a good beginning",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.3,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.7,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.121970176696777,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.7,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 97,
        "original_data": {
            "instruction": "What is the legal drinking age in the USA?",
            "input": "",
            "output": "The legal drinking age in the USA is 21.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.6029741764068604,
            "normalized": 0.6008497646876744
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 98,
        "original_data": {
            "instruction": "List the vegetation found in Antarctica in a common separated manner.",
            "input": "Antarctica (/ænˈtɑːrktɪkə/) is Earth's southernmost and least-populated continent. Situated almost entirely south of the Antarctic Circle and surrounded by the Southern Ocean (also known as the Antarctic Ocean), it contains the geographic South Pole. Antarctica is the fifth-largest continent, being about 40% larger than Europe, and has an area of 14,200,000 km2 (5,500,000 sq mi). Most of Antarctica is covered by the Antarctic ice sheet, with an average thickness of 1.9 km (1.2 mi).\nAntarctica is, on average, the coldest, driest, and windiest of the continents, and it has the highest average elevation. It is mainly a polar desert, with annual precipitation of over 200 mm (8 in) along the coast and far less inland. About 70% of the world's freshwater reserves are frozen in Antarctica, which, if melted, would raise global sea levels by almost 60 metres (200 ft). Antarctica holds the record for the lowest measured temperature on Earth, −89.2 °C (−128.6 °F). The coastal regions can reach temperatures over 10 °C (50 °F) in summer. Native species of animals include mites, nematodes, penguins, seals and tardigrades. Where vegetation occurs, it is mostly in the form of lichen or moss.\nThe ice shelves of Antarctica were probably first seen in 1820, during a Russian expedition led by Fabian Gottlieb von Bellingshausen and Mikhail Lazarev. The decades that followed saw further exploration in French, American, and British expeditions. The first confirmed landing was by a Norwegian team in 1895. In the early 20th century, there were a few expeditions into the interior of the continent. British explorers were the first to reach the magnetic South Pole in 1909, and the geographic South Pole was first reached in 1911 by Norwegian explorers.",
            "output": "Lichen, moss.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.5,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.5,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 8.929521560668945,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4225,
            "strategy_details": {
                "raw": 0.845,
                "normalized": 0.845,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.5,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 99,
        "original_data": {
            "instruction": "How tall can a human being theoretically be?",
            "input": "",
            "output": "Theoretically, a human being can reach a maximum height of around 7 feet, 9 inches (2.3 meters) if they are fully grown. This is based on the average height of a male adult in the United States. According to the Guinness Book of World Records, the tallest man was Sultan Kösen. Sultan Kösen (born 10 December 1982) is a Turkish farmer of Kurdish ethnicity who holds the Guinness World Record for tallest living male at 251 cm (8 ft 2.82 in). Kösen's growth resulted from the conditions gigantism and acromegaly, caused by a tumor affecting his pituitary gland.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.673922538757324,
            "normalized": 0.6211207253592355
        },
        "potential_entropy": {
            "strategy_component": 1.1875,
            "strategy_details": {
                "raw": 2.375,
                "normalized": 2.375,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 100,
        "original_data": {
            "instruction": "What makes sourdough special?",
            "input": "",
            "output": "Sourdough is made using sourdough starter, which is a fermented flour and water mixture that contains natural yeast. This gives the bread a distinct \"sour\" flavor which is different than breads made with commercial yeast.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.15,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.8,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.531435251235962,
            "normalized": 0.5804100717817035
        },
        "potential_entropy": {
            "strategy_component": 1.2574999999999998,
            "strategy_details": {
                "raw": 2.5149999999999997,
                "normalized": 2.5149999999999997,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.75,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.85,
                    "inp_complex": 0.9,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 101,
        "original_data": {
            "instruction": "How do I become a pilot?",
            "input": "",
            "output": "In the U.S. are several requirements to become a pilot. These vary based on what type of pilot you want to become (e.g., private or commercial), what aircraft you want to fly, and under what conditions. Generally speaking, all pilots must get a medical certificate to prove they're in good enough health to fly, pass the FAA written exam, complete sufficient training hours, then pass a check ride. Do all this, and congratulations, you're now a pilot!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.020840883255005,
            "normalized": 0.7202402523585728
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 102,
        "original_data": {
            "instruction": "Based on this paragraph, who directed \"Before Morning?\"",
            "input": "Before Morning is a 1933 American pre-Code crime drama directed by Arthur Hoerl, and starring Leo Carrillo, Lora Baxter, and Taylor Holmes. The film was adapted for the screen by Arthur Hoerl, from the 1933 Broadway play of the same name by Edward and Edna Riley.",
            "output": "Arthur Hoerl",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 6.347568511962891,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3975,
            "strategy_details": {
                "raw": 0.795,
                "normalized": 0.795,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 103,
        "original_data": {
            "instruction": "List some of the endocrinology societies and organizations within Europe and specifically talk about the United Kingdom.",
            "input": "Because endocrinology encompasses so many conditions and diseases, there are many organizations that provide education to patients and the public. The Hormone Foundation is the public education affiliate of The Endocrine Society and provides information on all endocrine-related conditions. Other educational organizations that focus on one or more endocrine-related conditions include the American Diabetes Association, Human Growth Foundation, American Menopause Foundation, Inc., and Thyroid Foundation of America.[citation needed]\n\nIn North America the principal professional organizations of endocrinologists include The Endocrine Society, the American Association of Clinical Endocrinologists, the American Diabetes Association, the Lawson Wilkins Pediatric Endocrine Society, and the American Thyroid Association.\n\nIn Europe, the European Society of Endocrinology (ESE) and the European Society for Paediatric Endocrinology (ESPE) are the main organisations representing professionals in the fields of adult and paediatric endocrinology, respectively.\n\nIn the United Kingdom, the Society for Endocrinology and the British Society for Paediatric Endocrinology and Diabetes are the main professional organisations.\n\nThe European Society for Paediatric Endocrinology is the largest international professional association dedicated solely to paediatric endocrinology. There are numerous similar associations around the world.",
            "output": "Specifically in the United Kingdom, the Society for Endocrinology and the British Society for Paediatric Endocrinology and Diabetes are the main professional endocrinology organisations. Europe also has the following organisations:  the European Society of Endocrinology (ESE) and the European Society for Paediatric Endocrinology (ESPE).",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.030444860458374,
            "normalized": 0.43726996013096403
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.55,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 104,
        "original_data": {
            "instruction": "Why should I visit Netherlands?",
            "input": "",
            "output": "There are many museums in the Netherlands that you can visit. Netherlands has had many famous painters. The city center of Amsterdam is also very nice. It is also considered as one of the best party places in the Europe. There are many famous events that you can join and enjoy.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.6,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.4,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.1700782775878906,
            "normalized": 0.7628795078822544
        },
        "potential_entropy": {
            "strategy_component": 1.2075,
            "strategy_details": {
                "raw": 2.415,
                "normalized": 2.415,
                "components": {
                    "instruction": 0.3,
                    "input": 1.65,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.4,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 105,
        "original_data": {
            "instruction": "Where is Cabernet Sauvignon grown?",
            "input": "Cabernet Sauvignon (French: [kabɛʁnɛ soviɲɔ̃]) is one of the world's most widely recognized red wine grape varieties. It is grown in nearly every major wine producing country among a diverse spectrum of climates from Australia and British Columbia, Canada to Lebanon's Beqaa Valley. Cabernet Sauvignon became internationally recognized through its prominence in Bordeaux wines, where it is often blended with Merlot and Cabernet Franc. From France and Spain, the grape spread across Europe and to the New World where it found new homes in places like California's Santa Cruz Mountains, Paso Robles, Napa Valley, New Zealand's Hawke's Bay, South Africa's Stellenbosch region, Australia's Margaret River, McLaren Vale and Coonawarra regions, and Chile's Maipo Valley and Colchagua. For most of the 20th century, it was the world's most widely planted premium red wine grape until it was surpassed by Merlot in the 1990s. However, by 2015, Cabernet Sauvignon had once again become the most widely planted wine grape, with a total of 341,000 hectares (3,410 km2) under vine worldwide.",
            "output": "France, Spain, California, New Zealand, South Africa, Australia, Chile, Canada",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.0526726245880127,
            "normalized": 0.7293350355965751
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 106,
        "original_data": {
            "instruction": "Give me a list of all the planets in our solar system.",
            "input": "",
            "output": "The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 1.4772794246673584,
            "normalized": 0.2792226927621024
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 107,
        "original_data": {
            "instruction": "Identify the bird from the list: Mouse, Balearic shearwater, Giraffe",
            "input": "",
            "output": "Balearic shearwater",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 6.811074256896973,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.8374999999999999,
            "strategy_details": {
                "raw": 1.6749999999999998,
                "normalized": 1.6749999999999998,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.0,
                    "output": 0.49499999999999994
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 108,
        "original_data": {
            "instruction": "Is there an answer to the universe?",
            "input": "",
            "output": "According to the novel Hitchhikers Guide to the Galaxy, the answer to life, the universe and everything is 42.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.3,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.7,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.5104897022247314,
            "normalized": 0.5744256292070661
        },
        "potential_entropy": {
            "strategy_component": 1.2550000000000001,
            "strategy_details": {
                "raw": 2.5100000000000002,
                "normalized": 2.5100000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.57
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.7,
                    "out_dens": 0.5,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 109,
        "original_data": {
            "instruction": "What are some ideas for fun rainy day activities to do with a 7 year old?",
            "input": "",
            "output": "* Play board games! Kids 6+ love to play board games of all kinds. You likely have a few sitting at home - dust them off and have at it!\n* Learn Chess! You may be surprised, but chess comes naturally to most kids, and many of them love the idea of horses, castles, kings and queens all battling against each other. While chess can take years to master, it is easy and fun to get started.\n* Go to the library! Libraries are great places to spend a rainy day, while giving you a chance to get out of the house. Many libraries have kids areas and besides books, you may be able to borrow legos, art supplies and puzzles, among other items.\n* Go outside and jump in some puddles! Put on your rain boots and embrace the rainy weather by jumping in some puddles!\n* Make origami! Use some unfolded scrap paper (or origami paper if you have some), fire up some online tutorials, and make anything from ninja stars to paper airplanes and boats (bonus: take the boat outside and sail it in a puddle!)\n* Go to your local rec center! If you're lucky enough to have a recreation center close by, then a rainy day is a great day to visit, to play basketball indoors, for example.\n* Take a bus or drive to a museum! 7 year olds tend to be very curious and as long as you don't overdo it, a museum can be a fun place to kill an hour or two!",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1608803272247314,
            "normalized": 0.760251522064209
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 110,
        "original_data": {
            "instruction": "What recording artist has won the most Grammys?",
            "input": "",
            "output": "Beyonce, a singer-songwriter who has been active since the 1990s, holds the record for most Grammys won as of 2023 with 32 awards.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 1.9668054580688477,
            "normalized": 0.4190872737339565
        },
        "potential_entropy": {
            "strategy_component": 1.24,
            "strategy_details": {
                "raw": 2.48,
                "normalized": 2.48,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 111,
        "original_data": {
            "instruction": "Is this hockey, baseball, or both?",
            "input": "",
            "output": "Puck, Ball, Ice, Dirt, Skates, Cleats, Gloves, Glove, Bat, Stick, Pads, Helmet",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            0
        ],
        "entropy": {
            "raw": 3.514216423034668,
            "normalized": 0.8612046922956195
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 112,
        "original_data": {
            "instruction": "Who is better : Superman or Batman?",
            "input": "",
            "output": "When it comes down to it, in every aspect, Superman is better than Batman. Superman is stronger, he's been around longer, he's a hard-working man, and he's just a better person.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.3,
            "s_out_div": 0.1,
            "s_out_dens": 0.25,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.18758225440979,
            "normalized": 0.7678806441170829
        },
        "potential_entropy": {
            "strategy_component": 1.5975000000000001,
            "strategy_details": {
                "raw": 3.1950000000000003,
                "normalized": 3.1950000000000003,
                "components": {
                    "instruction": 0.6,
                    "input": 1.65,
                    "output": 0.9450000000000001
                },
                "gaps": {
                    "ins_tone": 0.5,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.7,
                    "out_div": 0.9,
                    "out_dens": 0.75,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 113,
        "original_data": {
            "instruction": "What should I look for when looking for a used car?",
            "input": "",
            "output": "Items to look for when looking to purchase a used car:\n\n* First of all, ask about any major work done to the car like engine, suspension, body work. If it's been in any accidents that may have impacted it's chassis at all. Later on when we're examining the car we may find evidence of damage or repair work, and it's good to know if the seller is lying to cover up any issues before we start negotiating price with them. Pulling the carfax can also be a good way to check if the seller is lying since it often contains detailed information about repair that has been performed to the car. If you notice that the seller is lying either after pulling the carfax or after inspecting the body, it's best not to purchase the car.\n\n* Ask the owner if they have any maintenance records. If they do, that not only means that the car has received the proper care but it also illustrates that the buyer is the kind of person that is very organized and likes to keep their processions in good condition, which is super important. That's the kind of person you want to be buying a car from. Also note that if the owner has been taking the car to the dealership or to an independent shop it's very easy to obtain these service records using the VIN number either online or by calling the mechanic directly.\n\n* Cars having a timing belt needs to be replaced every 70,000-100,000 miles. You can easily check if the car you're planning on purchasing has a timing belt or timing chain. Typically timing chains do not need to be replaced for the life of the car, so finding a car with a timing chain means that you have to worry less about this aspect. Depending on the car replacing the timing belt can be from 1,000 to 6,000 dollars so it's a pretty significant expense that you should take into account.\n\n* Ask how frequently the owner has been changing the oil. In most cases changing it every 3,000 to 6,000 or every year (whichever comes first) is ideal. Many owner manuals mention that you can go over this mileage. In some cases manuals mention oil change intervals of 10,000 or 20,000 miles. Personally, I would not consider a car that has its oil replaced this infrequently. In many cases, these longer intervals are an attempt by the manufacturer's marketing department  to make the car more appealing to buyers if it requires less maintenance. Also ask if they've been using synthetic oil. Typically synthetic oil doesn't break down over time so longer intervals might be ok.\n\n* Ask when was the last time brake pads and rotors have been replaced. Typically brake pads need to be replaced every 50000-100000 miles. Brake rotors can be trickier because whether or not needs replacing depends on its thickness, however if during the test drive you notice vibration when braking, it's likely due to the brake rotors. If the owner hasn't replaced the brakes recently there might be a good opportunity for negotiating the price.\n\n* If the car is a manual ask if the clutch has been replaced. This is a wear item and the time to replace it depends on the usage (cars driven in cities with a lot of stop and go traffic will need a replacement more frequently than cars driving long distances on the freeway).\n\n* For automatics ask when was the last time the transmission fluid was changed. Typically this needs to be replaced every 50-60k miles. Again many service manuals mention that the transmission fluid does not need to be replaced at all, but that's arguably again due to marketing reasons.\n\n* Examine the tires and note how much tread is on the tires. If you were to imagine putting a penny into the tire's tread, a new tire would take up 1/3 - 1/2 of the penny. Also tires need to be replaced every 5 years. The tire sidewall has a 4 digit number which shows the week and year it was manufactured, so it's easy to check if it's due for a replacement. Lastly check if the tires (especially the front) are worn evenly. If the interior part is more worn compared to the exterior, they might indicate suspension or alignment issues. So these issues will need to be addressed and the tires will need to be replaced to fix that problem.\n\n* Ask for and look for any interior problems, bulbs, gauges, AC, power windows not working, stuff like that.\n\n* Check for rust in the wheel wells and rocker panels. Stand back from the car and look for any slight differences in color between it's exterior panels that may indicate they were replaced, also look for dents. The best and easiest indication that a car was repaired or in an accident is if the gaps between body panels is uneven. A good way to identify body panel issues is by running your finger across a body panel, if you feel that one part of the body is lower than the other or if that gap is not consistent, this might mean that the car has been in an accident.\n\n* Look for any signs of fluids, greasy stains on metal, anything that would indicate that it's leaked something. Oil leaks would look dark and greasy. Battery leaks look white and powdery. Coolant leaks look dark and smell sweet. Look at the bottom of the hood, if there are any leak stains there too. Also looks underneath the engine for any greasy spots, because fluids always drip down.\n\n* Do some parts on the car look newer/cleaner than others? A new radiator could mean that the car was recently in a frontal collision.\n\n* Listen to the engine. Does it sound like it's keeping a constant RPM? You may hear some ticking caused by piston slapping or poorly lubricated valves. These are signs that the engine is old, but isn't that big of a deal. Grinding noises are very bad and expensive to fix. Older engines can be pretty noisy until they warm up which is normal.\n\n* Ask the owner to rev the engine while you look at it. Does the engine hop violently when it's revved? This might mean that the engine mounts might need to be replaced.\n\n* Again, ask the owner to rev the engine, this time look at the exhaust. If any blue smoke comes out, or (if it's a warm day outside) any dense white smoke comes out it may indicate that the engine is burning oil, and unless you like adding oil to your engine as often as you fill up your gas tank, it'll be expensive to fix.\n\nTime to take it for a test drive!\n\n* Look at the gauge cluster, is the check engine light on? Keep an eye on the temperature gauge during your trip, does it fluctuate at all? A good car's temperature stays constant during it's trip.\n\n* Take some sharp turns at speed and listen to any thumping or squeeking noises that may come from the suspension or drive train.\n\n* Does the steering wheel shake at all? (when driving it could mean bad alignment or bad steering, when braking it could mean warped rotors)\n\n* Make sure you take the car in and out of all it's gears.\n\n* Listen for funny noises, rattles, squeeks, clanks, vibrations, etc. It's inevitable that older cars will have some interior panels rattle.\n\n* Accelerate hard and brake hard. If the car has ABS does it activate or do you feel that the wheels stop moving?\n\n* Go over some speed bumps, or go on a bumpy road. The car shouldn't be bouncy when recovering from a bump, and neither should it feel like riding over a bump in a shopping cart. It should absorb the bump in a firm but smooth manner.\n\n* After driving for a while, pull over to the side of the road and turn off the car. Pop the hood and look at the coolant resevoir (DON'T OPEN IT!!!) and check if the fluid inside is bubbling or boiling. If it is then the headgasket is blown.\n\n* Remember when we asked the owner when they changed their oil? New oil should look like honey, old oil should have the same color as dark coffee (dark coffee color is a good thing). If the oil is brown or any other funky colors or smells funny something might be wrong with the engine.\n\n* Once you let the car rest for a few minutes check the oil level by using the dipstick if it's not all the way to full, it might be burning or leaking oil.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.95,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.870103359222412,
            "normalized": 0.6771723883492606
        },
        "potential_entropy": {
            "strategy_component": 0.5275,
            "strategy_details": {
                "raw": 1.055,
                "normalized": 1.055,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 114,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Slenthem, Scheiholt",
            "input": "",
            "output": "Scheiholt is string, Slenthem is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.963737487792969,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 115,
        "original_data": {
            "instruction": "What is the best way to get faster at running long distances?",
            "input": "",
            "output": "To become faster at running requires practicing a lot of long slow distance running mixed in with occasional interval training. It's also important to avoid injuries by stretching, investing in good running shoes, and sleeping and eating well.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.829725980758667,
            "normalized": 0.951350280216762
        },
        "potential_entropy": {
            "strategy_component": 0.6050000000000001,
            "strategy_details": {
                "raw": 1.2100000000000002,
                "normalized": 1.2100000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 116,
        "original_data": {
            "instruction": "Write an argument for keeping public lands under federal control in the Intermountain West.",
            "input": "",
            "output": "Public lands make up a huge proportion of the region between the crest of the Sierra Nevada and the Continental Divide in the Rockies, also known as the Intermountain West, and most of these lands are owned by the Federal government. Many residents of this sparsely populated region resent the control of these lands by distant bureaucrats whose regulations place significant restrictions on how the land is used. For this reason, movements have been ongoing for many years to work for the transfer of those lands to local authorities. While this sense of disenfranchisement is understandable, the truth is that local residents would suffer more from the loss of centralized management of these lands than they would gain through local control. This region is mostly desert, a landscape and ecosystem that is unusually vulnerable to degradation. Without the concerted efforts of an administration that can apply consistent regulation and stewardship of these lands across a broad territory that crosses state boundaries, the value of these lands to local residents would decline even further, becoming less productive and less attractive to the hunters, fishermen, and other tourists whose spending represents the majority of income for many of these Intermountain communities.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.65,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.35,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0869476795196533,
            "normalized": 0.7391279084341866
        },
        "potential_entropy": {
            "strategy_component": 0.48500000000000004,
            "strategy_details": {
                "raw": 0.9700000000000001,
                "normalized": 0.9700000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.23999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.35,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 117,
        "original_data": {
            "instruction": "Where does this company produce its products?",
            "input": "Sweet Tree Holdings is a Vermont, United States, company that produces maple products from a sugarbush and processing facility in Island Pond.",
            "output": "Island Pond, Vermont",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.5,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.5,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 8.742988586425781,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6024999999999999,
            "strategy_details": {
                "raw": 1.2049999999999998,
                "normalized": 1.2049999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.5,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 118,
        "original_data": {
            "instruction": "What is a mortgage?",
            "input": "",
            "output": "A mortgage loan or simply mortgage, in civil law jurisdicions known also as a hypothec loan, is a loan used either by purchasers of real property to raise funds to buy real estate, or by existing property owners to raise funds for any purpose while putting a lien on the property being mortgaged. The loan is \"secured\" on the borrower's property through a process known as mortgage origination. This means that a legal mechanism is put into place which allows the lender to take possession and sell the secured property (\"foreclosure\" or \"repossession\") to pay off the loan in the event the borrower defaults on the loan or otherwise fails to abide by its terms. The word mortgage is derived from a Law French term used in Britain in the Middle Ages meaning \"death pledge\" and refers to the pledge ending (dying) when either the obligation is fulfilled or the property is taken through foreclosure. A mortgage can also be described as \"a borrower giving consideration in the form of a collateral for a benefit (loan)\".",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            1,
            0
        ],
        "entropy": {
            "raw": 3.0135092735290527,
            "normalized": 0.7181455067225865
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 119,
        "original_data": {
            "instruction": "Extract the two main component of C++ in a bulleted list based on this passage.",
            "input": "The C++ language has two main components: a direct mapping of hardware features provided primarily by the C subset, and zero-overhead abstractions based on those mappings. Stroustrup describes C++ as \"a light-weight abstraction programming language  for building and using efficient and elegant abstractions\"; and \"offering both hardware access and abstraction is the basis of C++. Doing it efficiently is what distinguishes it from other languages.\" C++ inherits most of C's syntax.",
            "output": "-direct mapping of hardware features provided primarily by the C subset\n-zero-overhead abstractions based on those mappings",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 6.47821044921875,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.29800000000000004,
            "strategy_details": {
                "raw": 0.5960000000000001,
                "normalized": 0.5960000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.186
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 120,
        "original_data": {
            "instruction": "What is difference between ECS and EC2?",
            "input": "",
            "output": "The largest difference for EC2 is that it deploys isolated VM instances with auto scaling support, and ECS deploys scalable clusters of managed Docker containers.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.055994510650635,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 121,
        "original_data": {
            "instruction": "What does the Constitution do?",
            "input": "",
            "output": "Protects basic rights of Americans",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 6.135544776916504,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2925,
            "strategy_details": {
                "raw": 2.585,
                "normalized": 2.585,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.40499999999999997
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 122,
        "original_data": {
            "instruction": "What are some quick ways to lose all of my money?",
            "input": "",
            "output": "Here are some quick ways to lose all of your money:\n1. Place all of your money on red at the roulette table.\n2. Play high-stakes blackjack everyday for the next week.\n3. Buy random crypto currencies like Solana.\n4. Buy extremely out of the money options on trading applications like Robinhood.\n5. Loan it to friends and families with a history of not paying you back.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.607285499572754,
            "normalized": 0.8877958570207868
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 123,
        "original_data": {
            "instruction": "Why you need to be honest?",
            "input": "",
            "output": "Because you should not cheat to anyone and that will give you a good live",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.3,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.7,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.809910297393799,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.7,
                    "out_dens": 0.6,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 124,
        "original_data": {
            "instruction": "Can you explain how I can analyze Simba logs?",
            "input": "",
            "output": "A typical Simba driver log line to would like like below\nFeb 21 20:17:16.525 TRACE 1542911745 Simba::ODBC::Statement::SQLFetch: +++++ enter +++++\n\nOne can use regular expression based match to parse this log line to split the fields to derive meaningful insights.\n\nFor example, the fields can be split into\n- timestamp\n- loglevel\n- pid\n- message\n\nThe last message field is a long string which encapsulates a lot of important information but can be classified as below by splitting this further based on the :: delimiter.\n- Namespace\n- Class\n- Method\n- Details\n\n\nThere are 2 types of Simba logs\nConnection Logs - Useful for getting connection attributes like agent, driver versions, operation status calls, connection failures, SSL errors etc\nDriver Logs - This helps to analyze actual details around data fetch performance.\n\nSome examples to derive meaningful insights from connection logs\nExtract unique query identifier using keyword search within the message for \"THandleIdentifier\" or a guid pattern\n\nSome examples to derive meaningful insights from driver logs\nHow long did an actual data fetch block take can be inspected using the time difference between the blocks.\nSQLFetch*Enter\nSQLFetch*Exit\n\nWithin each fetch block, you can look for below log lines to understand if it was a row-by-row fetch if you see a log of SQLGetData blocks.\nSQLGetData*Enter\nSQLGetData*Exit\n\nThere is other miscellaneous information logged by Simba driver for preceding requests such as closing a query or a session where an actual identifier needs to be sent along with the request, think of it as a payload for a REST API post request where you also need to send a payload along with the request.\n\nOverall, deriving meaningful information from the Simba driver logs require a bit of analysis\nbefore actually realizing the benefits of such analysis, most of the details are cryptic.\n\nBrainstorming other ideas with Simba product team would be fantastic.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.65,
            "s_out_dens": 0.6,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.35,
            "s_out_dens": 0.4,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.7893295288085938,
            "normalized": 0.9398084368024554
        },
        "potential_entropy": {
            "strategy_component": 0.5975,
            "strategy_details": {
                "raw": 1.195,
                "normalized": 1.195,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.35,
                    "out_dens": 0.4,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 125,
        "original_data": {
            "instruction": "Why do people sleep?",
            "input": "",
            "output": "Sleep gives your body an opportunity to heal and regenerate. It also gives your brain a chance to reprocess new information and explore possibilities in the form of dreams.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.879809856414795,
            "normalized": 0.67994567326137
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 126,
        "original_data": {
            "instruction": "From the given text, list some of the accomplishments of pitcher Mariano Rivera in his baseball career, in a comma-separated list.",
            "input": "Mariano Rivera (born November 29, 1969) is a Panamanian-American former professional baseball pitcher who played 19 seasons in Major League Baseball (MLB) for the New York Yankees, from 1995 to 2013. Nicknamed \"Mo\" and \"Sandman\", he spent most of his career as a relief pitcher and served as the Yankees' closer for 17 seasons. A thirteen-time All-Star and five-time World Series champion, he is MLB's career leader in saves (652) and games finished (952). Rivera won five American League (AL) Rolaids Relief Man Awards and three Delivery Man of the Year Awards, and he finished in the top three in voting for the AL Cy Young Award four times. He was inducted into the Baseball Hall of Fame as part of its class of 2019 in his first year of eligibility, and was the first and to date only player ever to be elected unanimously by the Baseball Writers' Association of America (BBWAA).\n\nRaised in the modest Panamanian fishing village of Puerto Caimito, Rivera was an amateur player until he was signed by the Yankees organization in 1990. He debuted in the major leagues in 1995 as a starting pitcher, before permanently converting to a relief pitcher late in his rookie year. After a breakthrough season in 1996 as a setup man, he became the Yankees' closer in 1997. In the following seasons, he established himself as one of baseball's top relievers, leading the major leagues in saves in 1999, 2001, and 2004. Rivera primarily threw a sharp-moving, mid-90s mile-per-hour cut fastball that frequently broke hitters' bats and earned a reputation as one of the league's toughest pitches to hit. With his presence at the end of games, signaled by his foreboding entrance song \"Enter Sandman\", Rivera was a key contributor to the Yankees' success in the late 1990s and early 2000s. An accomplished postseason performer, he was named the 1999 World Series Most Valuable Player (MVP) and the 2003 AL Championship Series MVP, and he holds several postseason records, including lowest earned run average (ERA) (0.70) and most saves (42).",
            "output": "All-Star, World Series champion, MLB's career leader in saves, MLB's career leader in games finished, AL Rolaids Relief Man Award, Delivery Man of the Year Award, Baseball Hall of Fame, World Series Most Valuable Player, AL Championship Series MVP, postseason lowest earned run average, postseason most saves",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9836790561676025,
            "normalized": 0.9953368731907436
        },
        "potential_entropy": {
            "strategy_component": 0.26,
            "strategy_details": {
                "raw": 0.52,
                "normalized": 0.52,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 127,
        "original_data": {
            "instruction": "Extract the names of the teams Bennie John Reiges was a part of from the text. Separate them with a comma.",
            "input": "Bennie John Reiges (March 4, 1920 – October 22, 2020) was an American football player and coach. He served as the head football coach at Arizona State Teachers College at Flagstaff—now known as Northern Arizona University—in 1950, compiling a record of 2–7. Reiges played college football as a quarterback at the University of California, Los Angeles (UCLA) during the 1946 and 1947 seasons. He was selected by the Los Angeles Rams in the 1947 NFL Draft. He died in October 2020 at the age of 100.",
            "output": "University of California, Los Angeles (UCLA), Los Angeles Rams, Arizona State Teachers College at Flagstaff",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.087691783905029,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.41250000000000003,
            "strategy_details": {
                "raw": 0.8250000000000001,
                "normalized": 0.8250000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 128,
        "original_data": {
            "instruction": "What is the best way to make an AreoPress coffee",
            "input": "",
            "output": "Ingredients: 30g of Coffee Medium Grind, 150-250ml of 86º Water\nSteps: Rinse AreoPress filter and set aside. Invert AreoPress, place on scale, and fill with ground coffee. Tare the scale, start a timer and begin filling AreoPress with water making sure to saturate the grounds. Stop pouring when when the scale reads 100g, adjust your pour so that this process takes around 10s. Stir vigorously for 20s then place your filter and lid on the AreoPress. When the timer reads 40s press the air out of the AreoPress. When the timer reads 45s flip the areopress, placing it on top of your cup. Press the body of the AreoPress to push out the coffee. Now add between 50-150ml of water according to your taste and stir to combine. Finally, enjoy your coffee.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.9582183361053467,
            "normalized": 0.7023480960300991
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 129,
        "original_data": {
            "instruction": "What is an onigiri made of?",
            "input": "Onigiri (お握り or 御握り), also known as omusubi (お結び), nigirimeshi (握り飯), or rice ball, is a Japanese food made from white rice formed into triangular or cylindrical shapes and often wrapped in nori. Traditionally, an onigiri is filled with pickled ume (umeboshi), salted salmon, katsuobushi, kombu, tarako, mentaiko, takanazuke (pickled takana, Japanese giant red mustard greens) or any other salty or sour ingredient as a natural preservative. Most Japanese convenience stores stock their onigiri with various fillings and flavors. There are even specialized shops which only sell onigiri to take out. Due to the popularity of this trend in Japan, onigiri has become a popular staple in Japanese restaurants worldwide.",
            "output": "Onigiri is a Japanese food made from white rice formed into triangular or cylindrical shapes and often wrapped in nori (seaweed). Traditionally, an onigiri is filled with pickled ume (umeboshi), salted salmon, katsuobushi, kombu, tarako, mentaiko, takanazuke (pickled takana, Japanese giant red mustard greens) or any other salty or sour ingredient as a natural preservative.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.9896090030670166,
            "normalized": 0.9970311437334333
        },
        "potential_entropy": {
            "strategy_component": 0.43249999999999994,
            "strategy_details": {
                "raw": 0.8649999999999999,
                "normalized": 0.8649999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 130,
        "original_data": {
            "instruction": "How can I get my dog to stop barking?",
            "input": "",
            "output": "There are a few ways to reduce the amount your dog. Dogs can bark for few reasons. Perhaps they want your attention, or they might be fearful or anxious and are barking to alert you. Also, barking habits can also be very breed specific. Some breeds are known to be more vocal than other breeds. It's important to try and understand why they are barking to then initiate the proper course of action to address their barking habits. If you believe they are barking for attention, one idea is to make sure you are not rewarding their barking by distracting them with a treat or playtime. By doing this, you are teaching them that if they bark, the dog will get what they want. Instead, you might want to consider a couple items. First, is the dog getting enough exercise or releasing their energy. Increase their stimulation by walking them more, playing with them, or socializing them with other dogs or humans. This added stimulation may reduce their need for attention. If they are barking because they are fearful or anxious, think about what is happening or what the environment is that may be cause their fear or anxiety. Then, try and change the environment or situation so the dog feels safe. One action that may help is providing a safe place for your dog, such as a dog crate or dog bed, so they know exactly what to do when they are afraid or anxious. Training them to go to their safe place may provide the comfort they need to reduce their barking habits.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.8,
            "s_out_dens": 0.6,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.4,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.503710985183716,
            "normalized": 0.5724888529096331
        },
        "potential_entropy": {
            "strategy_component": 1.1425,
            "strategy_details": {
                "raw": 2.285,
                "normalized": 2.285,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.4,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 131,
        "original_data": {
            "instruction": "Given this paragraph about the Northamptonshire County Cricket Club, can you list down some points that can be inferred about the County Championship?",
            "input": "Northamptonshire County Cricket Club is in Division Two of the County Championship; the team (also known as The Steelbacks) play their home games at the County Cricket Ground, Northampton. They finished as runners-up in the Championship on four occasions in the period before it split into two divisions.\n\nIn 2013 the club won the Friends Life t20, beating Surrey in the final. Appearing in their third final in four years, the Steelbacks beat Durham by four wickets at Edgbaston in 2016 to lift the Natwest t20 Blast trophy for the second time. The club also won the NatWest Trophy on two occasions, and the Benson & Hedges Cup once.",
            "output": "Based on this paragraph, it seems like the Country Championship refers to some form of organized competition between cricket teams belonging to different counties. The Championship has been organized for at least four times given that the Northamptonshire County Cricket Club finished as runners-up in it for four times. It also seems like the Championship was split into two divisions at some point.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.4585282802581787,
            "normalized": 0.8452937943594796
        },
        "potential_entropy": {
            "strategy_component": 0.43500000000000005,
            "strategy_details": {
                "raw": 0.8700000000000001,
                "normalized": 0.8700000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 132,
        "original_data": {
            "instruction": "What is rheological weldability?",
            "input": "Rheological weldability (RW) of thermoplastics considers the materials flow characteristics in determining the weldability of the given material. The process of welding thermal plastics requires three general steps, first is surface preparation. The second step is the application of heat and pressure to create intimate contact between the components being joined and initiate inter-molecular diffusion across the joint and the third step is cooling. RW can be used to determine the effectiveness of the second step of the process for given materials.",
            "output": "Rheological weldability describes the effectiveness of applying heat and pressure to join two materials.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.7516348361968994,
            "normalized": 0.9290385246276855
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 133,
        "original_data": {
            "instruction": "Make an argument for making buses free to ride in cities.",
            "input": "",
            "output": "Making buses free to ride would increase ridership. This benefits cities by reducing cars on the road, reducing traffic congestion and pollution. People who rely on buses often have lower income, and not charging bus fare would be disproportionately beneficial to them. Not charging a fare also avoids the need to maintain payment infrastructure on buses.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.780026912689209,
            "normalized": 0.9371505464826312
        },
        "potential_entropy": {
            "strategy_component": 0.5875,
            "strategy_details": {
                "raw": 1.175,
                "normalized": 1.175,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 134,
        "original_data": {
            "instruction": "In the series A Song of Ice and Fire, who is the founder of House Seaworth?",
            "input": "",
            "output": "Davos Seaworth is the founder of House Seaworth, a knighthood that was granted after Robert's Rebellion",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.998903751373291,
            "normalized": 0.9996867861066546
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 135,
        "original_data": {
            "instruction": "GIven this paragraph about American film director David Fincher, what are his reported upcoming projects.",
            "input": "In late 2019, Fincher began developing a television prequel to the 1974 film Chinatown with its screenwriter Robert Towne, and he has also expressed interest in eventually making a third season of Mindhunter, which was put on indefinite hold in 2020. In 2023, Fincher confirmed that Netflix will not be making a third season of Mindhunter, saying \"I’m very proud of the first two seasons. But it’s a very expensive show and, in the eyes of Netflix, we didn’t attract enough of an audience to justify such an investment [for Season 3].\"\n\nIn February 2021, it was reported that Fincher will direct an adaptation of the graphic novel The Killer for Netflix, with Andrew Kevin Walker writing the screenplay and Michael Fassbender attached to star.",
            "output": "David Fincher has begun development on a prequel to the 1974 film Chinatown.  It has also been reported that he will direct an adaptation of the graphic novel The Killer.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.3646411895751953,
            "normalized": 0.8184689113071987
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 136,
        "original_data": {
            "instruction": "What factors contributed to the decline of AI research in the 1970s?",
            "input": "TThe first AI winter 1974–1980\nIn the 1970s, AI was subject to critiques and financial setbacks. AI researchers had failed to appreciate the difficulty of the problems they faced. Their tremendous optimism had raised expectations impossibly high, and when the promised results failed to materialize, funding for AI disappeared. At the same time, the field of connectionism (or neural nets) was shut down almost completely for 10 years by Marvin Minsky's devastating criticism of perceptrons. Despite the difficulties with public perception of AI in the late 70s, new ideas were explored in logic programming, commonsense reasoning and many other areas.\n\nThe problems\nIn the early seventies, the capabilities of AI programs were limited. Even the most impressive could only handle trivial versions of the problems they were supposed to solve; all the programs were, in some sense, \"toys\". AI researchers had begun to run into several fundamental limits that could not be overcome in the 1970s. Although some of these limits would be conquered in later decades, others still stymie the field to this day.\n\nLimited computer power: There was not enough memory or processing speed to accomplish anything truly useful. For example, Ross Quillian's successful work on natural language was demonstrated with a vocabulary of only twenty words, because that was all that would fit in memory. Hans Moravec argued in 1976 that computers were still millions of times too weak to exhibit intelligence. He suggested an analogy: artificial intelligence requires computer power in the same way that aircraft require horsepower. Below a certain threshold, it's impossible, but, as power increases, eventually it could become easy. With regard to computer vision, Moravec estimated that simply matching the edge and motion detection capabilities of human retina in real time would require a general-purpose computer capable of 109 operations/second (1000 MIPS). As of 2011, practical computer vision applications require 10,000 to 1,000,000 MIPS. By comparison, the fastest supercomputer in 1976, Cray-1 (retailing at $5 million to $8 million), was only capable of around 80 to 130 MIPS, and a typical desktop computer at the time achieved less than 1 MIPS.\nIntractability and the combinatorial explosion. In 1972 Richard Karp (building on Stephen Cook's 1971 theorem) showed there are many problems that can probably only be solved in exponential time (in the size of the inputs). Finding optimal solutions to these problems requires unimaginable amounts of computer time except when the problems are trivial. This almost certainly meant that many of the \"toy\" solutions used by AI would probably never scale up into useful systems.\nCommonsense knowledge and reasoning. Many important artificial intelligence applications like vision or natural language require simply enormous amounts of information about the world: the program needs to have some idea of what it might be looking at or what it is talking about. This requires that the program know most of the same things about the world that a child does. Researchers soon discovered that this was a truly vast amount of information. No one in 1970 could build a database so large and no one knew how a program might learn so much information.\nMoravec's paradox: Proving theorems and solving geometry problems is comparatively easy for computers, but a supposedly simple task like recognizing a face or crossing a room without bumping into anything is extremely difficult. This helps explain why research into vision and robotics had made so little progress by the middle 1970s.\nThe frame and qualification problems. AI researchers (like John McCarthy) who used logic discovered that they could not represent ordinary deductions that involved planning or default reasoning without making changes to the structure of logic itself. They developed new logics (like non-monotonic logics and modal logics) to try to solve the problems.\nThe end of funding\nSee also: AI winter\nThe agencies which funded AI research (such as the British government, DARPA and NRC) became frustrated with the lack of progress and eventually cut off almost all funding for undirected research into AI. The pattern began as early as 1966 when the ALPAC report appeared criticizing machine translation efforts. After spending 20 million dollars, the NRC ended all support. In 1973, the Lighthill report on the state of AI research in England criticized the utter failure of AI to achieve its \"grandiose objectives\" and led to the dismantling of AI research in that country. (The report specifically mentioned the combinatorial explosion problem as a reason for AI's failings.) DARPA was deeply disappointed with researchers working on the Speech Understanding Research program at CMU and canceled an annual grant of three million dollars. By 1974, funding for AI projects was hard to find.\n\nHans Moravec blamed the crisis on the unrealistic predictions of his colleagues. \"Many researchers were caught up in a web of increasing exaggeration.\" However, there was another issue: since the passage of the Mansfield Amendment in 1969, DARPA had been under increasing pressure to fund \"mission-oriented direct research, rather than basic undirected research\". Funding for the creative, freewheeling exploration that had gone on in the 60s would not come from DARPA. Instead, the money was directed at specific projects with clear objectives, such as autonomous tanks and battle management systems.\n\nCritiques from across campus\nSee also: Philosophy of artificial intelligence\nSeveral philosophers had strong objections to the claims being made by AI researchers. One of the earliest was John Lucas, who argued that Gödel's incompleteness theorem showed that a formal system (such as a computer program) could never see the truth of certain statements, while a human being could. Hubert Dreyfus ridiculed the broken promises of the 1960s and critiqued the assumptions of AI, arguing that human reasoning actually involved very little \"symbol processing\" and a great deal of embodied, instinctive, unconscious \"know how\". John Searle's Chinese Room argument, presented in 1980, attempted to show that a program could not be said to \"understand\" the symbols that it uses (a quality called \"intentionality\"). If the symbols have no meaning for the machine, Searle argued, then the machine can not be described as \"thinking\".\n\nThese critiques were not taken seriously by AI researchers, often because they seemed so far off the point. Problems like intractability and commonsense knowledge seemed much more immediate and serious. It was unclear what difference \"know how\" or \"intentionality\" made to an actual computer program. Minsky said of Dreyfus and Searle \"they misunderstand, and should be ignored.\" Dreyfus, who taught at MIT, was given a cold shoulder: he later said that AI researchers \"dared not be seen having lunch with me.\" Joseph Weizenbaum, the author of ELIZA, felt his colleagues' treatment of Dreyfus was unprofessional and childish. Although he was an outspoken critic of Dreyfus' positions, he \"deliberately made it plain that theirs was not the way to treat a human being.\"\n\nWeizenbaum began to have serious ethical doubts about AI when Kenneth Colby wrote a \"computer program which can conduct psychotherapeutic dialogue\" based on ELIZA. Weizenbaum was disturbed that Colby saw a mindless program as a serious therapeutic tool. A feud began, and the situation was not helped when Colby did not credit Weizenbaum for his contribution to the program. In 1976, Weizenbaum published Computer Power and Human Reason which argued that the misuse of artificial intelligence has the potential to devalue human life.\n\nPerceptrons and the attack on connectionism\nA perceptron was a form of neural network introduced in 1958 by Frank Rosenblatt, who had been a schoolmate of Marvin Minsky at the Bronx High School of Science. Like most AI researchers, he was optimistic about their power, predicting that \"perceptron may eventually be able to learn, make decisions, and translate languages.\" An active research program into the paradigm was carried out throughout the 1960s but came to a sudden halt with the publication of Minsky and Papert's 1969 book Perceptrons. It suggested that there were severe limitations to what perceptrons could do and that Frank Rosenblatt's predictions had been grossly exaggerated. The effect of the book was devastating: virtually no research at all was done in connectionism for 10 years. Eventually, a new generation of researchers would revive the field and thereafter it would become a vital and useful part of artificial intelligence. Rosenblatt would not live to see this, as he died in a boating accident shortly after the book was published.\n\nLogic and symbolic reasoning: the \"neats\"\nLogic was introduced into AI research as early as 1959, by John McCarthy in his Advice Taker proposal. In 1963, J. Alan Robinson had discovered a simple method to implement deduction on computers, the resolution and unification algorithm. However, straightforward implementations, like those attempted by McCarthy and his students in the late 1960s, were especially intractable: the programs required astronomical numbers of steps to prove simple theorems. A more fruitful approach to logic was developed in the 1970s by Robert Kowalski at the University of Edinburgh, and soon this led to the collaboration with French researchers Alain Colmerauer and Philippe Roussel who created the successful logic programming language Prolog. Prolog uses a subset of logic (Horn clauses, closely related to \"rules\" and \"production rules\") that permit tractable computation. Rules would continue to be influential, providing a foundation for Edward Feigenbaum's expert systems and the continuing work by Allen Newell and Herbert A. Simon that would lead to Soar and their unified theories of cognition.\n\nCritics of the logical approach noted, as Dreyfus had, that human beings rarely used logic when they solved problems. Experiments by psychologists like Peter Wason, Eleanor Rosch, Amos Tversky, Daniel Kahneman and others provided proof. McCarthy responded that what people do is irrelevant. He argued that what is really needed are machines that can solve problems—not machines that think as people do.\n\nFrames and scripts: the \"scuffles\"\nAmong the critics of McCarthy's approach were his colleagues across the country at MIT. Marvin Minsky, Seymour Papert and Roger Schank were trying to solve problems like \"story understanding\" and \"object recognition\" that required a machine to think like a person. In order to use ordinary concepts like \"chair\" or \"restaurant\" they had to make all the same illogical assumptions that people normally made. Unfortunately, imprecise concepts like these are hard to represent in logic. Gerald Sussman observed that \"using precise language to describe essentially imprecise concepts doesn't make them any more precise.\" Schank described their \"anti-logic\" approaches as \"scruffy\", as opposed to the \"neat\" paradigms used by McCarthy, Kowalski, Feigenbaum, Newell and Simon.\n\nIn 1975, in a seminal paper, Minsky noted that many of his fellow \"scruffy\" researchers were using the same kind of tool: a framework that captures all our common sense assumptions about something. For example, if we use the concept of a bird, there is a constellation of facts that immediately come to mind: we might assume that it flies, eats worms and so on. We know these facts are not always true and that deductions using these facts will not be \"logical\", but these structured sets of assumptions are part of the context of everything we say and think. He called these structures \"frames\". Schank used a version of frames he called \"scripts\" to successfully answer questions about short stories in English.",
            "output": "Unrealistic expectations, a lack of funding, the challenge of producing significant results, and the discovery of a number of fundamental limitations that AI scientists could not overcome in the 1970s all contributed to the decline of AI research in that decade",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.6475038528442383,
            "normalized": 0.8992868150983538
        },
        "potential_entropy": {
            "strategy_component": 0.27,
            "strategy_details": {
                "raw": 0.54,
                "normalized": 0.54,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.15000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 137,
        "original_data": {
            "instruction": "What is the source or cause of the Sun’s light?",
            "input": "",
            "output": "Much like other stars the Sun is powered by nuclear reactions. The process of nuclear fusion takes place in the Sun’s core. The intense pressure and temperature at the core causes nuclear fusion - in this case hydrogen atoms fusing to create helium this process releases energy. The energy is emitted as gamma rays which travel slowly from the Sun’s core to the outer region. During the rise to the surface gamma rays collide with electrons degrading in temperature and energy to finally emerge  as visible radiation - this is the light Sun produces.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.9949727058410645,
            "normalized": 0.7128493445260184
        },
        "potential_entropy": {
            "strategy_component": 0.34,
            "strategy_details": {
                "raw": 0.68,
                "normalized": 0.68,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 138,
        "original_data": {
            "instruction": "Locate what the player should defend against in The Last of Us using the sample as a reference, extract them, and place them in a numbered list.",
            "input": "The Last of Us is an action-adventure game played from a third-person perspective. The player traverses post-apocalyptic environments such as towns, buildings, forests, and sewers to advance the story. The player can use firearms, improvised weapons, hand-to-hand combat, and stealth to defend against hostile humans and cannibalistic creatures infected by a mutated strain of the Cordyceps fungus. For most of the game, the player takes control of Joel, a man tasked with escorting a young girl, Ellie, across the United States. The player also controls Ellie throughout the game's winter segment and briefly controls Joel's daughter, Sarah, in the opening sequence.",
            "output": "1. Hostile humans;\n2. Cannibalistic creatures infected by a mutated strain of the Cordyceps fungus.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.831282138824463,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 139,
        "original_data": {
            "instruction": "What is a typical Dutch tradition when a baby is born?",
            "input": "",
            "output": "When a baby is born in the Netherlands, the typical traditional Dutch snack you can expect to be served when visiting the proud new parents is \"Beschuit met muisjes\". It consists of beschuit, which is a round, toasted, dry, and crunchy rusk,  smeared butter, covered in \"muisjes\" (literal translation is little mice), which are sugar-coated aniseeds. The colour of the mice will pink if the baby is a girl, and blue for a boy.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 4.160935401916504,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 140,
        "original_data": {
            "instruction": "What were the key points of the Treaty of Versailles?",
            "input": "The most critical and controversial provision in the treaty was: \"The Allied and Associated Governments affirm and Germany accepts the responsibility of Germany and her allies for causing all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies.\" The other members of the Central Powers signed treaties containing similar articles. This article, Article 231, became known as the War Guilt clause. The treaty required Germany to disarm, make ample territorial concessions, and pay reparations to certain countries that had formed the Entente powers. In 1921 the total cost of these reparations was assessed at 132 billion gold marks (then $31.4 billion or £6.6 billion, roughly equivalent to US$442 billion or UK£284 billion in 2023). Because of the way the deal was structured, the Allied Powers intended Germany would only ever pay a value of 50 billion marks.",
            "output": "The Treaty of Versailles was a peace treaty to end World War I. The critical point in the treaty between the Germans and the Allied powers was that Germany accepted full responsibility for the loss and damage incurred to the Allies. Germany was required to disarm and pay reparations estimated at $31.4 billion in 1921.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.504859209060669,
            "normalized": 0.5728169168744769
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 141,
        "original_data": {
            "instruction": "What are hoagies, or sub sandwiches called in the state of Rhode Island?",
            "input": "",
            "output": "In the state of Rhode Island, these types of sandwiches are called grinders.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.457943916320801,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0099999999999998,
            "strategy_details": {
                "raw": 2.0199999999999996,
                "normalized": 2.0199999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 142,
        "original_data": {
            "instruction": "Name a very famous family of composers & musicians of late baroque",
            "input": "",
            "output": "Bach family gave to the world few famous composers - Johann Sebastian Bach, Carl Philipp Emanuel Bach, Wilhelm Friedemann Bach, Johann Christian Bach and others",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.254296064376831,
            "normalized": 0.7869417326790946
        },
        "potential_entropy": {
            "strategy_component": 1.005,
            "strategy_details": {
                "raw": 2.01,
                "normalized": 2.01,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.65,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 143,
        "original_data": {
            "instruction": "Why do people like soccer?",
            "input": "",
            "output": "People like soccer as it is a creative worldwide game. Unlike other sports that have frequent stoppages and timeouts soccer has two 45 minute halves. On top of that it is a very creative sport as it requires 11 team members to work in union to achieve a common goal. Everybody can play soccer no matter your age or athletic level so it is extremely appealing to many. Additionally, unlike other sports that require significant associated cost, soccer is rather cheap, all you need is a ball.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.5818123817443848,
            "normalized": 0.8805178233555385
        },
        "potential_entropy": {
            "strategy_component": 0.735,
            "strategy_details": {
                "raw": 1.47,
                "normalized": 1.47,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 144,
        "original_data": {
            "instruction": "What states do not have a state gambling lottery?",
            "input": "",
            "output": "Alabama, Alaska, Hawaii, Nevada, and Utah.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.1195976734161377,
            "normalized": 0.7484564781188965
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 145,
        "original_data": {
            "instruction": "Jot down some points about University of Liverpool from given text",
            "input": "The University of Liverpool (abbreviated UOL) is a public research university based in the city of Liverpool, England. Founded as a college in 1881, it gained its Royal Charter in 1903 with the ability to award degrees, and is also known to be one of the six 'red brick' civic universities, the first to be referred to as The Original Red Brick. It comprises three faculties organised into 35 departments and schools. It is a founding member of the Russell Group, the N8 Group for research collaboration and the university management school is triple crown accredited.",
            "output": "1. The institution of Liverpool is a public research institution with its main campus in Liverpool, England (abbreviated UOL).\n2. The Original Red Brick was the first of the six civic universities known as \"red bricks,\" and it was established in 1881 as a college before receiving its Royal Charter in 1903, enabling it to grant degrees.\n3. It has 35 departments and schools divided into three faculties.\n4. The university management school has earned the triple crown of accreditation, and it is a founding member of the Russell Group and the N8 Group for research collaboration.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.88,
            "s_out_dens": 0.92,
            "s_out_bg": 0.87
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.12,
            "s_out_dens": 0.07999999999999996,
            "s_out_bg": 0.13
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.351634979248047,
            "normalized": 0.8147528512137276
        },
        "potential_entropy": {
            "strategy_component": 0.2695,
            "strategy_details": {
                "raw": 0.539,
                "normalized": 0.539,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.12899999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.12,
                    "out_dens": 0.07999999999999996,
                    "out_bg": 0.13
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 146,
        "original_data": {
            "instruction": "The Awards ceremony for 63rd Grammy Awards was held at which place?",
            "input": "",
            "output": "Los Angeles",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 10.037908554077148,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 147,
        "original_data": {
            "instruction": "Where are Vermeer's paintings set",
            "input": "Johannes Vermeer (/vərˈmɪər, vərˈmɛər/ vər-MEER, vər-MAIR, Dutch: [vərˈmeːr], see below; also known as Jan Vermeer; October 1632 – 15 December 1675) was a Dutch Baroque Period painter who specialized in domestic interior scenes of middle-class life. During his lifetime, he was a moderately successful provincial genre painter, recognized in Delft and The Hague. Nonetheless, he produced relatively few paintings and evidently was not wealthy, leaving his wife and children in debt at his death.\n\nVermeer worked slowly and with great care, and frequently used very expensive pigments. He is particularly renowned for his masterly treatment and use of light in his work.\n\n\"Almost all his paintings\", Hans Koningsberger wrote, \"are apparently set in two smallish rooms in his house in Delft; they show the same furniture and decorations in various arrangements and they often portray the same people, mostly women.\"\n\nHis modest celebrity gave way to obscurity after his death. He was barely mentioned in Arnold Houbraken's major source book on 17th-century Dutch painting (Grand Theatre of Dutch Painters and Women Artists) and was thus omitted from subsequent surveys of Dutch art for nearly two centuries. In the 19th century, Vermeer was rediscovered by Gustav Friedrich Waagen and Théophile Thoré-Bürger, who published an essay attributing 66 pictures to him, although only 34 paintings are universally attributed to him today. Since that time, Vermeer's reputation has grown, and he is now acknowledged as one of the greatest painters of the Dutch Golden Age.\n\nSimilar to other major Dutch Golden Age artists such as Frans Hals and Rembrandt, Vermeer never went abroad. Also, like Rembrandt, he was an avid art collector and dealer.",
            "output": "Most paintings by Johannes Vermeer are set in Delft in the Netherlands.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.601750612258911,
            "normalized": 0.8862144606454032
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 148,
        "original_data": {
            "instruction": "How long does it typically take a battery storage power station to transition to full power from standby?",
            "input": "A battery storage power station is a type of energy storage power station that uses a group of batteries to store electrical energy. Battery storage is the fastest responding dispatchable source of power on electric grids, and it is used to stabilise those grids, as battery storage can transition from standby to full power in under a second to deal with grid contingencies.\n\nAt full rated power, battery storage power stations are generally designed to output for up to a few hours. Battery storage can be used for short-term peak power and ancillary services, such as providing operating reserve and frequency control to minimize the chance of power outages. They are often installed at, or close to, other active or disused power stations and may share the same grid connection to reduce costs. Since battery storage plants require no deliveries of fuel, are compact compared to generating stations and have no chimneys or large cooling systems, they can be rapidly installed and placed if necessary within urban areas, close to customer load.",
            "output": "Under a second",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 11.556879997253418,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 149,
        "original_data": {
            "instruction": "Was Dracula a real person?",
            "input": "",
            "output": "Dracula is a fictitious character created by the British novelist Bram Stoker. Stoker drew inspiration from a Romanian prince called Vlad Dracul. Vlad led a Romanian province situated in the south of Transylvania for parts of the 15th century. Vlad was renown at the time for his cruel measures to punish his enemies through impaling them. For this reason, he was also called Vlad the Impaler or, in Romanian, Vlad Tepeș",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.2257134914398193,
            "normalized": 0.7787752832685199
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 150,
        "original_data": {
            "instruction": "Which of the star signs are earth signs?",
            "input": "",
            "output": "Virgo, Taurus and Capricorn are the three astrological earth signs in the Zodiac.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.061304807662964,
            "normalized": 0.7318013736179897
        },
        "potential_entropy": {
            "strategy_component": 1.1800000000000002,
            "strategy_details": {
                "raw": 2.3600000000000003,
                "normalized": 2.3600000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.42000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 151,
        "original_data": {
            "instruction": "What are your favorite musical instruments?",
            "input": "",
            "output": "Guitar, flute, bagpipes, cello, saxophone.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.741750955581665,
            "normalized": 0.9262145587376186
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 152,
        "original_data": {
            "instruction": "What is guitar amp modeling?",
            "input": "",
            "output": "Guitar amp modeling is a digital technology for reproducing the sound and feel of playing guitar through physical amplifiers.  This technology may be embodied in a piece of outboard gear, or as a software plugin for a digital audio workstation.  In recent years, guitar amp modeling has advanced to the point of being difficult to discern by ear from the sound of the amplifiers being modeled.  Some brands associated with advanced guitar amp modeling include Line 6, Fractal Audio, and Neural DSP.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            1,
            3
        ],
        "entropy": {
            "raw": 3.16457462310791,
            "normalized": 0.7613070351736886
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 153,
        "original_data": {
            "instruction": "What is a mulligan in Magic the Gathering?",
            "input": "",
            "output": "A mulligan in Magic the Gathering is an action you can take if you are not happy with your opening hand in a game. Depending on the format you play, different rules apply. For competitive one versus one formats, the rule is that you draw an initial hand of seven cards. If you are unhappy with the hand, you can shuffle it back in your deck, and again draw a hand of seven cards. If you are then happy with the hand, you can keep it but you have to choose and put one card from your hand and put it on the bottom of the deck. If you are unhappy, you can mulligan again, but then you'll have to put back two cards, and so forth.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 2.525846004486084,
            "normalized": 0.5788131441388812
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 154,
        "original_data": {
            "instruction": "In the series A Song of Ice and Fire, who is the founder of House Durrandon?",
            "input": "",
            "output": "Durran, also known as Durran Godsgrief",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 6.772637844085693,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2775,
            "strategy_details": {
                "raw": 2.555,
                "normalized": 2.555,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 155,
        "original_data": {
            "instruction": "Why are all the planets and moons round in shape? What makes them round? Does there exist another shape?",
            "input": "",
            "output": "Not all planets and moons are round. There is some variety in their shapes depending on their masses, and curiously looking worlds might even exist, unlike anything we know from our system.\n\nEverything within a planet or a moon is under the influence of gravity in such a way that bits of it would move to the center if they could. Under ideal conditions, it would acquire a spherical shape. For objects made of something other than liquids, it varies at what point they become round. It is generally assumed that they mostly become spheres around 400 kilometers/248 miles in diameter when made from icy material commonly found in our Solar System. Still, if something is made from something more rigid, it needs to be larger in diameter than that - around 750 km/466 miles. At a sufficient mass, even rock becomes fluid-like. The moon of Mars Deimos is only 12.6 km/7.8 miles across and is not spherical, and there are many other small moons of other planets that don’t have a round shape.\n\nThis, however, is not all. Astronomical objects spin, giving them the flattened shape of a sphere the faster their rotation if they are massive enough. On top of it, they can have moons. This causes a bulge to form at the position of their orbiting body around them. In the case of Earth, our Moon causes tides via this process. Also, our Sun gives our planet tides. Elsewhere in the Universe, there could be planets with massive moons or two similarly massive planets, called double planets, orbiting their common center of mass, the barycenter, at a close distance, which can cause even stronger bulging. Their shape would be affected this way more visibly.\n\nFinally, a bizarre, toroidal planet, shown above, can be stable from what we know from physics, at least for a while. As I mentioned earlier, even rigid rock behaves like a fluid at large masses. An adequate rotation speed can counterbalance the strength of gravity with centrifugal acceleration, and a torus-shaped planet could exist. A possible scenario for such a world to come to be would be if two sufficiently fast rotating planets collided and formed a doughnut of vaporized rock, a so-called synestia.\n\nWe currently think that the events that would lead to the formation of a world of this shape are so unlikely that there might not be a single toroidal planet in the Visible Universe, but we were quite a few times surprised by what we discovered during our exploration of Cosmos. Perhaps we will even find a doughnut shape world one day.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.353731632232666,
            "normalized": 0.8153518949236188
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 156,
        "original_data": {
            "instruction": "Which US astronauts that landed on the moon were Scouts?",
            "input": "",
            "output": "Since NASA's formation in 1958, 181 of the 320 pilots and scientists selected by NASA were Scouts.\n\nOf the 24 people to travel to the Moon on the Apollo missions, 20 were Scouts, including 11 of the 12 Moonwalkers.\n\nAll three of the Apollo 13 astronauts, who had to abort their mission and return to Earth were also Scouts.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.65,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.35,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.9820098876953125,
            "normalized": 0.7091456821986607
        },
        "potential_entropy": {
            "strategy_component": 0.7274999999999999,
            "strategy_details": {
                "raw": 1.4549999999999998,
                "normalized": 1.4549999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.35,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 157,
        "original_data": {
            "instruction": "When was Santa Monica founded?",
            "input": "Santa Monica (Spanish for 'Saint Monica'; Santa Mónica) is a city in Los Angeles County, situated along Santa Monica Bay on California's South Coast. Santa Monica's 2020 U.S. Census population was 93,076. Santa Monica is a popular resort town, owing to its climate, beaches, and hospitality industry. It has a diverse economy, hosting headquarters of companies such as Hulu, Universal Music Group, Lionsgate Films, and The Recording Academy.\n\nSanta Monica traces its history to Rancho San Vicente y Santa Mónica, granted in 1839 to the Sepúlveda family of California. The rancho was later sold to John P. Jones and Robert Baker, who in 1875, along with his Californio heiress wife Arcadia Bandini de Stearns Baker, founded Santa Monica, which incorporated as a city in 1886. The city developed into a seaside resort during the late 19th and early 20th centuries, with the creation of tourist attractions such as Palisades Park, the Santa Monica Pier, Ocean Park, and the Hotel Casa del Mar.",
            "output": "Santa Monica was founded in 1886.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.95,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8043570518493652,
            "normalized": 0.6583877290998187
        },
        "potential_entropy": {
            "strategy_component": 0.44749999999999995,
            "strategy_details": {
                "raw": 0.8949999999999999,
                "normalized": 0.8949999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 158,
        "original_data": {
            "instruction": "Who wrote The Picture of Dorian Grey in 1891",
            "input": "",
            "output": "Oscar Wilde",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.1,
            "s_out_div": 0.0,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.9,
            "s_out_div": 1.0,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 7.680021286010742,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.5099999999999998,
            "strategy_details": {
                "raw": 3.0199999999999996,
                "normalized": 3.0199999999999996,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 2.0,
                    "output": 0.6599999999999999
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.9,
                    "out_div": 1.0,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 159,
        "original_data": {
            "instruction": "Classify each of the following as either solid, liquid or gas: fart, urine, poop, toilet paper, toilet, sink, water, bar of soap, underwear, belt, steam",
            "input": "",
            "output": "The solids include poop, toilet paper, the toilet, the bar of soap, underwear and belt. The liquids include urine and water. The gases include the fart and steam.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.729205846786499,
            "normalized": 0.9226302419389997
        },
        "potential_entropy": {
            "strategy_component": 0.5974999999999999,
            "strategy_details": {
                "raw": 1.1949999999999998,
                "normalized": 1.1949999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 160,
        "original_data": {
            "instruction": "In the series A Song of Ice and Fire, who is the founder of House Bar Emmon?",
            "input": "",
            "output": "Togarion Bar Emmon, also known as Togarion the Terrible",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.4,
            "s_out_div": 0.3,
            "s_out_dens": 0.25,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.6,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.290396690368652,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3575,
            "strategy_details": {
                "raw": 2.715,
                "normalized": 2.715,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.65,
                    "output": 0.8849999999999999
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.6,
                    "out_div": 0.7,
                    "out_dens": 0.75,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 161,
        "original_data": {
            "instruction": "What causes ocean tides?",
            "input": "",
            "output": "Tides are very long waves moving across the ocean and are caused by the gravitational forces exerted on the earth by the moon. When the highest point in the long-period wave reaches a shoreline the area experiences high tide. When the lowest point of the wave, the trough, reaches shore the area experiences low tide. The earth is mostly water and the water \"points\" at the moon, the parts of the Earth experiencing high tide is the water drawing towards the moon. The \"pointing\" water that faces the moon is formed because the gravitational pull of the moon is strongest on whichever side of the Earth faces it. Gravity pulls the ocean towards the moon and high tide occurs.\n\nThe bulge on the far side of the Earth is caused by inertia. The water moving away from the moon resists the gravitational forces that attempt to pull it in the opposite direction. Because the gravitational pull of the moon is weaker on the far side of the Earth, inertia wins, the ocean bulges out and high tide occurs. As the Earth spins, different areas of the planet face the moon, and this rotation causes the tides to cycle around the planet.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.542114734649658,
            "normalized": 0.5834613527570452
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 162,
        "original_data": {
            "instruction": "Which is a species of fish? Mullet or Afro",
            "input": "",
            "output": "Mullet",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 11.68283462524414,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 163,
        "original_data": {
            "instruction": "best ways to search google",
            "input": "",
            "output": "Use of Tabs\nUse a hyphen to exclude words\nuse of colon when searching specific sites\nUse asterisk when necessary\nFind search sites similar to other",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.65,
            "s_out_dens": 0.55,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.35,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 5.981155872344971,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.825,
            "strategy_details": {
                "raw": 1.65,
                "normalized": 1.65,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.35,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 164,
        "original_data": {
            "instruction": "How do I avoid and protect myself of being hacked?",
            "input": "",
            "output": "There are multiple ways to be vigilant and protect your digital identity and presence in the Internet. First turn on MFA (Multiple Factor Authentication) everywhere for all your accounts work (must be turned on if your employer takes security seriously) and personal accounts. Second NEVER reuse passwords, instead use a Password Manager. There are several for instance Google Chrome browser provides a password manager out of the box. Same if you use Apple products (iOS, Safari web browser, etc.). There are also commercial password managers such as 1Password and LastPass. Third never click or open URL links that look suspicious or from unknown senders. Fourth use a digital key like a Yubikey. The YubiKey is an easy to use extra layer of security for your online accounts. A single YubiKey has multiple functions for securing your login to email, online services, apps, computers, and even physical spaces. These are some basic steps that will protect your digital identity. Finally ALWAYS use a Lockscreen for your desktop computer, laptop, etc. All your devices should always auto lock after 3-5 minutes of inactivity. These basic steps provide a solid foundation so you will never be hacked.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.243133068084717,
            "normalized": 0.783752305167062
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 165,
        "original_data": {
            "instruction": "How should I maintain my bicycle?",
            "input": "",
            "output": "Keep your bicycle clean. Most important of all, keep your bicycle’s drivetrain clean. A bicycle’s drivetrain is the chain, chainrings and pulleys which make the rear wheel turn. If you can, wipe down the drivetrain with a cleaning lubricant made for bicycle drivetrains after each long ride, or every other short ride. Doing so will ensure that your ride is noticeably more enjoyable. Shifting will remain smooth and reliable, and pedaling will be easier. Dirty drivetrains are squeaky and loud. Your clean drivetrain will be nice and quiet. Your clean drivetrain will also last much longer since you are regularly cleaning away the dirt and other road grit and sweat which collects in the chain and grinds away everything it touches. If you are unable to maintain your bicycle, take it to the local bike shop at least once a year for maintenance and tune up. You won’t regret it.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.852627754211426,
            "normalized": 0.6721793583461216
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.85,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 166,
        "original_data": {
            "instruction": "What does WHO refer to?",
            "input": "",
            "output": "WHO is an acronym for the World Health Organization.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.039114475250244,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 167,
        "original_data": {
            "instruction": "What is a newspaper?",
            "input": "",
            "output": "A newspaper is a daily printed publication delivered to your house or place of business.  It is made our of folded sheets of printed paper, and it contains current and prior day events also called news.  You will also find advertisements and different correspondences in it as well.  It is a great way to stay up to date with current events, and you will have to pay to get a newspaper subscription delivered daily or weekly.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.2698450088500977,
            "normalized": 0.7913842882428851
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 168,
        "original_data": {
            "instruction": "Where is the Eclectochromis lobochilus fish generally found?",
            "input": "Eclectochromis lobochilus is a species of fish in the family Cichlidae. It is found in Malawi, Mozambique, and Tanzania. Its endemic to Lake Malawi.\n\nThis species is found along rocky shores and sometimes in the intermediate habitat between sandy and rock areas. It occurs at depths of 5–15 metres (16–49 ft). It feeds on invertebrates, mainly adult and larval in insects, that hide among aquatic vegetation or algae. The breeding males defend a cave among the rocks. The females mouthbrood the eggs and fry, guarding them after they have been released. This species is known as \"Haplochromis Hertae\" in the aquarium trade.",
            "output": "The Eclectochromis lobochilus fish is generally found in Malawi, Mozambique, and Tanzania.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.0811381340026855,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 169,
        "original_data": {
            "instruction": "Write an essay about whether corona policy measures are beneficial towards society or not",
            "input": "",
            "output": "After nearly two years of the COVID-19 pandemic and yet another lockdown, many people around the Netherlands are growing restless and fed up with the restrictions. According to the Rijksoverheid, a record-breaking 74,000 cases were confirmed in the Netherlands on January 28th, with just ten corona-related ICU admissions or deaths (“Infections- Confirmed Cases”, 2022). This raises the debate: should the government continue enforcing these measures or not? In order to formulate an answer, one must first understand the virus and how it spreads, and then consider the debate from political, ethical, social and economic perspectives.\nThe first reported case of SARS-CoV-2, which causes the COVID-19 disease, was on December 31, 2019 (\"Coronavirus disease\", 2022), and it set off a chain reaction of political instability worldwide. What first started as an epidemic within China had reached almost every country in the world within months. In the last 20 years, many countries have been affected by viruses related to Covid-19 such as SARS. Because of previous experience, they took appropriate measures immediately. For example, China has had no more than 110,000 cases since the start of the pandemic, even though it has almost a population of 1.5 billion (Worldometer, 2022). However, a less politically stable country, such as Turkey, was less prepared and suffered from 12 million cases, even though it has a population of 85 million. The virus spreads very quickly because of its airborne transmission mechanism. If one is near an infected person, the carrier’s exhalation may carry the virus and can be inhaled by healthy individuals. Putting on a mask and keeping distance help prevent infected individuals from spreading virus particles into the vicinity of healthy people. However, in countries where measures were not enforced, there was a large increase in excessive deaths. The new omicron variant is proving to spread at least 20 times faster than the original strain because of its 30 mutations (Grossman and Stahel, 2022). Fortunately, it is also far less deadly because it does not attack lung tissue in its earlier stages (Zimmer and Ghorayshi, 2021).\nOne can also analyze coronavirus restrictions from an ethical perspective: should we  lower corona measures even though doing so may put more lives at risk? As of January 28th 2021, the coronavirus has a death rate of 0.51%. This is relatively high compared to other respiratory viruses such as influenza, which has a mortality rate of 0.01%. COVID-19 disproportionally affects vulnerable groups, such as those with underlying health conditions such as diabetes, obesity, or autoimmune diseases. This is especially true in the elderly, because of the increased amount of health conditions as well as an immune system deteriorated by age. The coronavirus infects the upper respiratory tract, including the lungs. It breaks into cells and orders them to create copies of the virus. The cell then bursts open and sends new copies of the virus toward other cells. Eventually, if there is not an effective immune response, the patient will perish due to lung damage, especially with the strains of COVID-19 prior to the Omicron variant. During a lockdown, there is minimal human contact, which slows down the spread of the virus. However, preventing social contact has its own devastating effects. For example, during the lockdown there were over 2.5 million children at risk of falling behind on learning (“Jongeren” 2021') (\"Education and COVID-19\", 2022). Furthermore, according to OSF Healthcare, “Average weekly visits to the emergency department for suspected suicide attempts among young girls were up more than 50% from the same period a year ago” (OSF Health Care, 2021). Thus, even though more lives are put at stake, by taking extra measures for those in the risk group, one can safely reopen society and prevent suicides and learning disbilities in the youth. Moreover, vaccines can prevent major damage to tissues from happening in the first place.\nFrom a societal perspective, coronavirus has proven to be a “splinter issue”, pushing some people into vaccine denial. Many citizens are refraining from getting vaccines without truly understanding the consequences. There are two types of vaccines: mRNA and viral vector. The mRNA COVID-19 vaccine was the first mRNA vaccine to be used in the general public. mRNA vaccines have RNA inside lipids which help the RNA enter the cell. Ribosomes read the RNA and instruct the cells to make the same spike proteins as SARS-CoV-2. The body then recognizes that the Corona spike proteins are invaders and creates antibodies to destroy them. In comparison, viral vector vaccines insert a harmless copy of the virus with the same spike protein DNA, causing the immune system to attack it and remember it. For both types of vaccines, memory B cells will continue to create antibodies in case the person gets exposed to the real virus in the future. According to Dr. Stahel, “Omicron has mutated so much that it has less affinity in terms of the antibodies from the vaccine recognizing it, so, therefore, the answer is the booster shot, because you will overwhelm the virus with the quantity of the immune response even if there is not a perfect match” (Grossman and Stahel, 2022). Thus, the original vaccine doses and, more recently, the booster shot play an important role in preventing infections, but more importantly in reducing the chance of severe illness and death. However, new studies have shown that effective immunity can wear off within six months, making it necessary to get annual doses, similar to influenza (Doucleff). 13.6% of the population over 18 refuses to get vaccinated, primarily because of distrust in the government (“Vaccinations COVID-19 vaccinations”, 2022). This limits the effects of herd immunity, a concept in which if enough people are vaccinated the virus will no longer be able to spread due to all the targets being immune.\nFinally, one must consider the economic implications of a lockdown. The lockdown caused colossal unemployment, particularly for those who worked in the travel industry. For example, according to the world economic forum, 114 million people lost their jobs during 2020 (Richter, 2021). Furthermore, the lockdowns across the world also caused an estimated 3.2 trillion euro loss in labor income. Even with the devastating economic effects, one must consider the fact that lives are put at stake when appropriate corona measures are not taken. According to NPR, there would have been approximately 488 times the number of cases in China if insufficient measures were taken, hence saving millions of lives (Beaubien, 2020). \n\tIn conclusion, by considering COVID-19 from a political, ethical, social and economic lens, I conclude that even though the virus is highly contagious and fatal towards certain groups, lockdowns are only temporary solutions. A lockdown is only beneficial to our society once excessive mortality starts rising. Hence, by taking measures early on, countries can limit the spread of the virus. For example, it took six months before mask-wearing was seen as a viable measure. As seen in the first lockdown, tens of thousands of people lost their jobs and over 2.5 million children had to do remote learning in the Netherlands (\"Jongeren\", 2021). Thus, by taking measures on time and being prepared, one need not go into lockdown. However, if the deaths start to become excessive, it is worth enforcing a proportional amount of corona measures.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.7387330532073975,
            "normalized": 0.6396380152021136
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 170,
        "original_data": {
            "instruction": "What are the 4 oceans on earth",
            "input": "",
            "output": "Atlantic Ocean\nArctic Ocean\nPacific Ocean\nIndian Ocean",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.663435697555542,
            "normalized": 0.6181244850158691
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 171,
        "original_data": {
            "instruction": "Tell me whether a human has two or one of each of the following body parts: eye, mouth, nose, leg, ear, neck",
            "input": "",
            "output": "Eye: Two\nMouth: One\nNose: One\nLeg: Two\nEar: Two\nNeck: One",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.3323328495025635,
            "normalized": 0.5235236712864467
        },
        "potential_entropy": {
            "strategy_component": 0.9649999999999999,
            "strategy_details": {
                "raw": 1.9299999999999997,
                "normalized": 1.9299999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 172,
        "original_data": {
            "instruction": "Name three professional sports teams in Boston.",
            "input": "",
            "output": "Three professional sports teams in Boston are the Boston Celtics, Boston Bruins, and Boston Red Sox.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.48376202583313,
            "normalized": 0.5667891502380371
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 173,
        "original_data": {
            "instruction": "What are the top nominated films to the Academy Awards in 2023?",
            "input": "",
            "output": "The top nominated films for the 2023 Academy Awards are as follows:\n\nEverything Everywhere All at Once - 11 nominations\nAll Quiet on the Western Front - 9 nominations\nThe Banshees of Inisherin - 9 nominations\nElvis - 8 nominations\nThe Fabelmans - 7 nominations\nTár - 6 nominations\nTop Gun: Maverick - 6 nominations\nBlack Panther: Wakanda Forever - 5 nominations\nAvatar: The Way of Water - 4 nominations\nBabylon - 3 nominations\nThe Batman - 3 nominations\nTriangle of Sadness - 3 nominations\nThe Whale - 3 nominations",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 1.9113675355911255,
            "normalized": 0.40324786731175016
        },
        "potential_entropy": {
            "strategy_component": 0.36250000000000004,
            "strategy_details": {
                "raw": 0.7250000000000001,
                "normalized": 0.7250000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 174,
        "original_data": {
            "instruction": "Who are the three main characters of the Harry Potter Series?",
            "input": "",
            "output": "Harry Potter, Hermione Granger and Ron Weasley are the main characters of Harry Potter",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 3.052269458770752,
            "normalized": 0.7292198453630719
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 175,
        "original_data": {
            "instruction": "When did the Portuguese royal family move to Brazil?",
            "input": "",
            "output": "The Portuguese royal family, along with its court, servants and a library with more than 60,000 books, settled in Brazil between 1808 and 1821, having the first wave of migration starting on November 29th of 1807.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.749915838241577,
            "normalized": 0.6428330966404506
        },
        "potential_entropy": {
            "strategy_component": 1.2025,
            "strategy_details": {
                "raw": 2.405,
                "normalized": 2.405,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 176,
        "original_data": {
            "instruction": "What is physical security?",
            "input": "",
            "output": "Physical security describes security measures that are designed to deny unauthorized access to facilities, equipment, and resources and to protect personnel and property from damage or harm (such as espionage, theft, or terrorist attacks).  Physical security involves the use of multiple layers of interdependent systems that can include CCTV surveillance, security guards, protective barriers, locks, access control, perimeter intrusion detection, deterrent systems, fire protection, and other systems designed to protect persons and property.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.6128854751586914,
            "normalized": 0.6036815643310547
        },
        "potential_entropy": {
            "strategy_component": 1.2025,
            "strategy_details": {
                "raw": 2.405,
                "normalized": 2.405,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 177,
        "original_data": {
            "instruction": "What are the best investments ideas?",
            "input": "",
            "output": "How you invest depends on some important factors:\n\n- Objective: What do you want to do with the money?\n- Timeline: What is the period that you want to invest and/or need to withdraw from the investment?\n- Risk tolerance: How do you feel comfortable with market variations?\n\nDepending on these factors you can select the types of investments that best fit your goals such as:\n\n- Treasury/Certificate of Deposits/Bonds\n- Fixed Income\n- REIT (Real Estate Investment Trust)\n- Mutual Funds\n- Stock Market\n\nYou can choose between one type of investment or adopt strategies to diversify your portfolio.\n\nAlso, there are more advanced types of investment strategies such as Options and Derivatives, for more investment ideas and how to adopt a strategy you can consult your financial advisor.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.5909693241119385,
            "normalized": 0.5974198068891253
        },
        "potential_entropy": {
            "strategy_component": 1.0750000000000002,
            "strategy_details": {
                "raw": 2.1500000000000004,
                "normalized": 2.1500000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 178,
        "original_data": {
            "instruction": "Explain the black and white smoke during the Pope election",
            "input": "",
            "output": "Beginning in the early 1800s, the ballots used by cardinals were burned after each ballot to indicate a failed election. The lack of smoke instead signalled a successful election. Since 1914, black smoke (fumata nera) emerging from a temporary chimney installed on the roof of the Sistine Chapel indicates that the ballot did not result in an election, while white smoke (fumata bianca) announces that a new pope has been chosen.\n\nPrior to 1945 (when Pius XII changed the form of ballot to use unsigned ballots, first carried out in 1958), the sealing wax on the complex type ballots illustrated above had the effect of making the smoke from burning the ballots either black or white, depending on whether or not damp straw was added. Until the 20th century, sealing wax customarily had beeswax mixed into its composition. The use of wax made solely from animal fat does not give as much white colored smoke, as does wax that includes beeswax. In the 1939 conclave there was some confusion over the smoke color, which was even more apparent in the 1958 conclave. The lack of sealing wax on the ballots explains the confusion over the color of the smoke in the 1958 conclave. The Siri thesis was based on the confusion over the smoke color on the first day of that conclave.\n\nSince 1963, chemicals have been added to the burning process to augment the smoke's black or white color. Beginning in 2005, a successful election is also accentuated by bells ringing at the appearance of the white smoke.\n\nDuring the 2013 conclave, the Vatican disclosed the chemicals used to color the smoke—the black smoke comprises potassium perchlorate, anthracene, and sulfur; the white smoke is composed of potassium chlorate, lactose, and pine rosin.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 3.2029638290405273,
            "normalized": 0.7722753797258649
        },
        "potential_entropy": {
            "strategy_component": 0.47000000000000003,
            "strategy_details": {
                "raw": 0.9400000000000001,
                "normalized": 0.9400000000000001,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 179,
        "original_data": {
            "instruction": "What are three ways to build a pillow fort?",
            "input": "",
            "output": "You could use chairs or sofas in the room as stands, draping large sheets across them to form a canopy and walls. Alternatively, you could stick the corners of the sheets to a corner of the room so the walls are pre-built in, allowing for as tall a canopy as you want, but having a less cozy feel overall. As another option, you could use other large items besides blankets and sheets such as large umbrellas, decorating the space with lots of pillows and stuffed animals to cover any item that isn't already soft or fluffy.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.6421587467193604,
            "normalized": 0.8977596419198173
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 180,
        "original_data": {
            "instruction": "Classify each of the following as either a type of donut or a fruit candy brand: old-fashioned, Skittles, Mamba, maple bar, sprinkles, raised, Starburst, cruller, bear claw, apple fritter, Airheads",
            "input": "",
            "output": "Donut: old-fashioned, maple bar, sprinkles, raised, cruller, bear claw, apple fritter\nFruit candy brand: Skittles, Mamba, Starburst, Airheads",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.810940742492676,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.373,
            "strategy_details": {
                "raw": 0.746,
                "normalized": 0.746,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.186
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 181,
        "original_data": {
            "instruction": "Who was Svea Kristina Frisch",
            "input": "Svea Kristina Frisch (15 June 1898 in Ramsberg, Örebro county – 1991), known as Kristina Lindstrand, was a Swedish actor, journalist, author, and poet.\n\nHer first marriage was during 1922–1927 with the actor and director Ivar Kåge, and the second (1929–1946) was with the painter Vicke Lindstrand. She moved from Sweden to Italy in 1969.",
            "output": "Svea Kristina Frisch (15 June 1898 in Ramsberg, Oerebro county– 1991), known as Kristina Lindstrand, was a Swedish actor, journalist, author, and poet.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.80839204788208,
            "normalized": 0.9452548708234515
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 182,
        "original_data": {
            "instruction": "Tell me about plant life cycle",
            "input": "",
            "output": "Like any living being, a plant is programmed for its specie to last over generations.\nWith winter coming, the plant will generate seeds, what is called « fructification ». \nThese seeds are basically a strong capsule containing the plant’s genetic material and enough energy for the germ to grow the first leaf which will start photosynthesis.\nThe energy contained in the seed is stabilised under the form of starch. It is basically a long chain of sugar polymerised.\nDuring winter, the seed hibernate, but when spring comes, specific weather conditions (humidity, temperature) degrade the starch and release elementary sugar molecules for the new growing plant to create its first leaf.\nChlorophyll contained in leaves will enable the plant to produce its own energy; glucose sugar, from carbon dioxyde, water and light, and continue to grow, until the next cycle.\nThere is a bit of magic with plants: creating oxygen and energy from air and sun!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.776940107345581,
            "normalized": 0.9362686020987374
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 183,
        "original_data": {
            "instruction": "Q: Am I at risk for serious complications from COVID-19 if I smoke cigarettes?",
            "input": "",
            "output": "A: Yes. Data shows that when compared to never smokers, cigarette smoking increases the risk of more severe illness from COVID-19, which could result in hospitalization, the need for intensive care, or even death. Smoking cigarettes can cause inflammation and cell damage throughout the body, and can weaken your immune system, making it less able to fight off disease.\n\nThere’s never been a better time to quit smoking. If you need resources to help you quit smoking, the FDA has supportive tips and tools to help you get closer to quitting for good.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 2.6056735515594482,
            "normalized": 0.6016210147312709
        },
        "potential_entropy": {
            "strategy_component": 0.945,
            "strategy_details": {
                "raw": 1.89,
                "normalized": 1.89,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.5,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 184,
        "original_data": {
            "instruction": "Who invented cement that could be set underwater?",
            "input": "",
            "output": "The Romans were the first to make cement that could set in water",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.205105304718018,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1575000000000002,
            "strategy_details": {
                "raw": 2.3150000000000004,
                "normalized": 2.3150000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 185,
        "original_data": {
            "instruction": "Why Do We Need GIS?",
            "input": "",
            "output": "GIS use has become mandatory in many settings. GIS are used to fight crime, protect endangered species, reduce pollution, cope with natural disasters, treat epidemics, and improve public health; GIS are instrumental in addressing some of our most pressing societal problems.GIS tools in aggregate save billions of dollars annually in the delivery of goods and services. GIS regularly helps in the day-to-day management of many natural and man-made resources, including sewer, water,power, transportation networks, and package delivery. GIS are at the heart of one of the most important processes in U.S. democracy, decadal redrawing of U.S. congressional districts, and hence the distribution of tax dollars and other government resources.GIS are needed in part because human consumption has reached levels such that many resources, including air and land, replacing substantial limits on human action.The first 100,000 years of human existence caused scant impacts on the world's resources, but in the past 300 years humans have permanently altered most of the Earth's Surface. The atmosphere and oceans exhibit a decreasing ability to benignly absorb carbon dioxide and nitrogen, two of humanity's primary waste products. Silt chokes many rivers, and there are abundant examples of smoke, ozone, or other noxious pollutants substantially harming public health. By the end of the 20th century, most lands south of the boreal region had been farmed, grazed,cut, built over, drained, flooded, or otherwise altered by humans. GIS help us identify and address environmental problems by providing crucial information on where problems occur and who are affected by them. GIS helps us identify the source, location, and extent of adverse environmental impacts, and may help us devise practical plans for monitoring, managing, and mitigating environmental damage.Human impacts on the environment have spurred a strong societal push for the adoption of GIS. Conflicts in resource use,concerns about pollution, and precautions to protect public health have led to legislative mandates that explicitly or implicitly require the consideration of geography. The U.S. Endangered Species Act (ESA) is a good example. The ESA requires adequate protection of rare and threatened organisms. This entails mapping the available habitat and species range and migration patterns, relative to human land use. \nGIS use is mandated in other endeavors, including emergency services, flood protection, disaster assessment and management, and infrastructure development.Public organizations have also adopted GIS because it aids in governmental functions. For example, emergency service vehicles are regularly dispatched and route using GIS. E911 GIS matches the caller's address to the nearest emergency service station, a route is generated based on the streetnetwork and traffic, and emergency crews dispatched in a fraction of the pre-GIS times.Many businesses adopt GIS for increased efficiency in the delivery of goods and services. Retail businesses locate stores based on a number of spatially related factors. Where are the potential customers?What is the spatial distribution of competing businesses? Where are potential new store locations? What are traffic flows near current stores, and how easy is it to park near and access these stores? GIS are also used in hundreds of other business applications, toroute vehicles, guide advertising, design buildings, plan construction, and sell real estate.The societal push to adopt GIS has been complemented by a technological pull in the development and application of GIS. Thousands of lives and untold wealth have been lost because ship captains could not answer the simple question, “Where am I?”Remarkable positioning technologies, generically known as Global Navigation SatelliteSystems (GNSS), are now indispensable tools in commerce, planning, and safety.The technological pull has developed on several fronts. Spatial analysis in particular has been helped by faster computers with more storage, and by the increased interconnectedness via mobile networks. Most real-world spatial problems were beyond the scope of all but the largest organization until the 1990s. GIS computing expenses are becoming an afterthought as costs decrease and performance increases at dizzying rates. Powerful field computers are lighter, faster,more capable, and less expensive, so spatial data display and analysis capabilities may always be at hand.In addition to the computing improvements and the development of GNSS, current “cameras” deliver amazingly detailed aerial and satellite images. Initially,advances in image collection and interpretation were spurred by World War II and then the Cold War because accurate maps were required, but unavailable.Turned toward peacetime endeavors, imaging technologies now help us map food and fodder, houses and highways, and most other natural and human-built objects. Images may be rapidly converted to accurate spatial information over broad areas. Many techniques have been developed for extracting information from image data, and also for ensuring this information faithfully represents the location, shape, and characteristics of features on the ground. Visible light,laser, thermal, and radar scanners are currently being developed to further increase the speed and accuracy with which we map our world. Thus, advances in these three key technologies — imaging, GNSS, and computing — have substantially aided the development of GIS.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 3.5880980491638184,
            "normalized": 0.8823137283325195
        },
        "potential_entropy": {
            "strategy_component": 1.18,
            "strategy_details": {
                "raw": 2.36,
                "normalized": 2.36,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 186,
        "original_data": {
            "instruction": "Extract the types of communication methods that domestic cats use.",
            "input": "Domestic cats use many vocalizations for communication, including purring, trilling, hissing, growling/snarling, grunting, and several different forms of meowing.",
            "output": "Domestic cats communicate by purring, trilling, hissing, growling, snarling, grunting, and meowing.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.6980955600738525,
            "normalized": 0.6280273028782436
        },
        "potential_entropy": {
            "strategy_component": 0.47000000000000003,
            "strategy_details": {
                "raw": 0.9400000000000001,
                "normalized": 0.9400000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 187,
        "original_data": {
            "instruction": "Will AI replace all human beings?",
            "input": "",
            "output": "When I was writing down those words used to train an AI model, I feel funny. Some of my friends expressed concern that AI could soon replace humans and take away all of our jobs. However, I see things differently. I believe that AI has the potential to develop rapidly, and I'm excited to see how it will transform the world in my lifetime. If AI can create more resources, we may be able to live more comfortably and work fewer hours. This could allow us to pursue our passions and interests without worrying so much about survival. Additionally, in the future, humans may be able to live alongside AI, which mean live more longer and stronger.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.6,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.4,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.8112592697143555,
            "normalized": 0.6603597913469587
        },
        "potential_entropy": {
            "strategy_component": 1.4049999999999998,
            "strategy_details": {
                "raw": 2.8099999999999996,
                "normalized": 2.8099999999999996,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 2.0,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.4,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 188,
        "original_data": {
            "instruction": "What’s the difference between Spark Streaming and Spark Structured Streaming? What should I use?",
            "input": "",
            "output": "Spark Streaming is the previous generation of Spark’s streaming engine. There are no longer updates to Spark Streaming and it’s a legacy project. Spark Streaming provides a high-level abstraction called discretized stream or DStream, which represents a continuous stream of data. Internally, a DStream is represented as a sequence of RDDs.\n\nSpark Structured Streaming is the current generation of Spark’s streaming engine, which is richer in functionality, easier to use, and more scalable. Spark Structured Streaming is built on top of the Spark SQL engine and enables you to express streaming computation the same way you express a batch computation on static data.\n\nYou should use Spark Structured Streaming for building streaming applications and pipelines with Spark. If you have legacy applications and pipelines built on Spark Streaming, you should migrate them to Spark Structured Streaming.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.3897101879119873,
            "normalized": 0.5399171965462821
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 189,
        "original_data": {
            "instruction": "What is the capital of Brazil?",
            "input": "",
            "output": "Brasília is the capital of Brazil",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.27909779548645,
            "normalized": 0.5083136558532715
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 190,
        "original_data": {
            "instruction": "Name a style of hot yoga.",
            "input": "",
            "output": "Bikram.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.325229644775391,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 191,
        "original_data": {
            "instruction": "Is homekit's are necessary for a home?",
            "input": "",
            "output": "It's really not necessary now. But good to have.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.5,
            "s_out_div": 0.3,
            "s_out_dens": 0.4,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.5,
            "s_out_div": 0.7,
            "s_out_dens": 0.6,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.114807605743408,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.435,
            "strategy_details": {
                "raw": 2.87,
                "normalized": 2.87,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.7000000000000002,
                    "output": 0.8099999999999999
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.5,
                    "out_div": 0.7,
                    "out_dens": 0.6,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 192,
        "original_data": {
            "instruction": "Tell me whether these people are comedians or athletes: Chris Rock, Kevin Hart, Tom Segura, Joe Rogan, John Rahm, Zach Ertz, Kyler Murray",
            "input": "",
            "output": "Chris Rock: Comedian\nKevin Hart: Comedian\nTom Segura: Comedian\nJoe Rogan: Comedian\nJohn Rahm: Athlete\nZach Ertz: Athlete\nKyler Murray: Athlete",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 2.9692206382751465,
            "normalized": 0.7054916109357562
        },
        "potential_entropy": {
            "strategy_component": 0.615,
            "strategy_details": {
                "raw": 1.23,
                "normalized": 1.23,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.9,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 193,
        "original_data": {
            "instruction": "What makes an organization standout?",
            "input": "",
            "output": "When an organization puts its people ahead of the profits and cares about the society it  creates a unique value and culture that produces long term results. There are many companies that are examples which include the TATA group, Reliance etc.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.2833404541015625,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 194,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Ranat thum, Tumbi",
            "input": "",
            "output": "Tumbi is string, Ranat thum is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.093874454498291,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 195,
        "original_data": {
            "instruction": "What are some hobbies that I can try that have a relatively low cost to start?",
            "input": "",
            "output": "Rock collecting, coin collecting, bird watching, gardening, writing",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.9645915031433105,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.755,
            "strategy_details": {
                "raw": 1.51,
                "normalized": 1.51,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 196,
        "original_data": {
            "instruction": "Where is the village of Congosto",
            "input": "Congosto (Spanish pronunciation: [koŋˈɡosto]) is a village and municipality located in the region of El Bierzo (province of León, Castile and León, Spain) . It is located near to Ponferrada, the capital of the region. The village of Congosto has about 350 inhabitants.\n\nIts economy was traditionally based on agriculture, wine and coal mining. Nowadays, most of the inhabitants work on the surrounding area on activities such as wind turbine manufacturing or coal mining.\n\nCongosto also a large reservoir in its vicinity, the Barcena reservoir, to which many tourists visit during the summer.",
            "output": "Congosto is a village and municipality located in the region of El Bierzo (province of Leon, Castile and Leon, Spain). It is located near to Ponferrada, the capital of the region. The village of Congosto has about 350 inhabitants.\n\nIts economy was traditionally based on agriculture, wine and coal mining. Nowadays, most of the inhabitants work on the surrounding area on activities such as wind turbine manufacturing or coal mining.\n\nCongosto also a large reservoir in its vicinity, the Barcena reservoir, to which many tourists visit during the summer.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9747331142425537,
            "normalized": 0.7070666040693011
        },
        "potential_entropy": {
            "strategy_component": 0.5225,
            "strategy_details": {
                "raw": 1.045,
                "normalized": 1.045,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.31499999999999995
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 197,
        "original_data": {
            "instruction": "What are some different types of natural disasters?",
            "input": "",
            "output": "Types of natural disasters include: Forest Fires, Tsunamis, Earthquakes, Volcanic Eruptions, Hurricanes, Tornados, and Avalanches.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 2.193739652633667,
            "normalized": 0.4839256150381906
        },
        "potential_entropy": {
            "strategy_component": 1.0150000000000001,
            "strategy_details": {
                "raw": 2.0300000000000002,
                "normalized": 2.0300000000000002,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 198,
        "original_data": {
            "instruction": "Is toastmasters a good organization to join?",
            "input": "",
            "output": "Toastmasters is an organization that has been around for nearly 100 years.  It is an organization that helps its members to become competent communicators and competent leaders.  They achieve this by delivering prepared speeches that are evaluated during Toastmasters meetings.  You get evaluated on filler words used, grammatical construction of your speech, speech tonality, and a general evaluation of your speech.  This works so well because you get practical insight into where you can improve in your communication skills in delivering prepared speeches.  You can also fill different roles like treasurer, and president roles to help you build leadership skills.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.304980754852295,
            "normalized": 0.8014230728149414
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 199,
        "original_data": {
            "instruction": "Write the first paragraph of a blog post describing the appeal of Boston as a tourist destination",
            "input": "",
            "output": "Boston is a vibrant city with a rich history, incredible architecture, and diverse culture. Its cobblestone streets, charming neighborhoods, and vibrant nightlife make it an ideal destination for visitors. From the Freedom Trail to the Boston Common, Boston offers visitors a unique experience unlike any other. From historic sites to vibrant nightlife, Boston is a great place to visit and explore. The suburbs are great too.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.373481273651123,
            "normalized": 0.5352803639003209
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 200,
        "original_data": {
            "instruction": "How many strings are on a violin?",
            "input": "",
            "output": "Four",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.95,
            "s_out_div": 0.05,
            "s_out_dens": 0.9,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.95,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": NaN,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1575,
            "strategy_details": {
                "raw": 2.315,
                "normalized": 2.315,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.85,
                    "output": 0.34500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.95,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 201,
        "original_data": {
            "instruction": "Provide a list of body parts that are only three letters long. Here are some body parts that are three lettered words: toe, rib, eye, ear, leg and jaw.",
            "input": "",
            "output": "Here are some body parts that are three lettered words: toe, rib, eye, ear, leg and jaw.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.93337082862854,
            "normalized": 0.9809630938938686
        },
        "potential_entropy": {
            "strategy_component": 0.9574999999999999,
            "strategy_details": {
                "raw": 1.9149999999999998,
                "normalized": 1.9149999999999998,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 202,
        "original_data": {
            "instruction": "How do you play basketball",
            "input": "",
            "output": "Basketball is a game played on a court with five players on each team. There are two baskets that are 10 feet tall on each end of the court and the goal of the game is to get the ball into the basket at the opposing end of the court. Each basket is worth two points, unless the ball is shot from outside of an arc that is painted on the floor. If the shot it made outside of the arc, then it's worth three points. The games are 48 minutes long with 4 even quarters at 12 minutes each. The clock also stops every time a basket is made, the ball goes out of bounds, or a foul is committed. When running with the ball, a player must continuously bounce it off the ground or else it's considered traveling which is a not allowed. Additionally, aggressive contact is also considered a foul. Basketball is more about finesse than it is about physicality. Whichever team scores the most points by the end of the game wins",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.603940725326538,
            "normalized": 0.6011259215218681
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 203,
        "original_data": {
            "instruction": "What is the difference between a chihuaha and a golden retriever?",
            "input": "",
            "output": "Chihuahas are a breed of dog and they are generally smaller in size. They can be long or short haired. Their temperament is that they like to bark a lot and are easily agitated. Regardless, Chihuaha dogs make great companions and lap dogs. \n\nGolden retrievers are a breed of dog and they are generally larger in size. They are very intelligent dogs and can be easily trained. They are used as service dogs to help people, for example they can be a guide dog for the blind. Their temperament is that they are calm and focused. Golden retrievers are also great companion dogs.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.4938137531280518,
            "normalized": 0.5696610723223005
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 204,
        "original_data": {
            "instruction": "What are the four main linguistic and cultural regions of Switzerland?",
            "input": "Switzerland originates from the Old Swiss Confederacy established in the Late Middle Ages, following a series of military successes against Austria and Burgundy; the Federal Charter of 1291 is considered the country's founding document. Swiss independence from the Holy Roman Empire was formally recognised in the Peace of Westphalia in 1648. Switzerland has maintained a policy of armed neutrality since the 16th century and has not fought an international war since 1815. It joined the United Nations only in 2002, but pursues an active foreign policy that include frequent involvement in peace-building processes worldwide.\n\nSwitzerland is the birthplace of the Red Cross, one of the world's oldest and best-known humanitarian organisations, and hosts the headquarters or offices of most major international institutions, including the WTO, the WHO, the ILO, FIFA, and the United Nations. It is a founding member of the European Free Trade Association (EFTA), but not part of the European Union (EU), the European Economic Area, or the Eurozone; however, it participates in the European single market and the Schengen Area through bilateral treaties. Switzerland is a federal republic composed of 26 cantons, with federal authorities based in Bern.\n\nIt has four main linguistic and cultural regions: German, French, Italian and Romansh. Although most Swiss are German-speaking, national identity is fairly cohesive, being rooted in a common historical background, shared values such as federalism and direct democracy, and Alpine symbolism. Swiss identity transcends language, ethnicity, and religion, leading to Switzerland being described as a Willensnation (\"nation of volition\") rather than a nation state.",
            "output": "German, French, Italian and Romansh",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.111870288848877,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 205,
        "original_data": {
            "instruction": "Is a blackberry a fruit or a berry?",
            "input": "",
            "output": "While blackberry sounds like a berry, it's actually a fruit. To be more accurate, it's considered an \"aggregate fruit,\" a fruit that consists of multiple smaller fruits. It's made of multiple small drupelets that are arranged around a central core.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.618032217025757,
            "normalized": 0.605152062007359
        },
        "potential_entropy": {
            "strategy_component": 0.8899999999999999,
            "strategy_details": {
                "raw": 1.7799999999999998,
                "normalized": 1.7799999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 206,
        "original_data": {
            "instruction": "Give me a list of the best Romantic Comedy Movies",
            "input": "",
            "output": "My Big Fat Greek Wedding, About Time, P.S. I Love You, Somethings Gotta Give, How To Lose A Guy in Ten Days, Never Been Kissed, Annie Hall, The Big Sick, My Best Friend's Wedding, Sleepless in Seattle, Notting Hill, 10 Things I Hate About You, Bridget Jones's Diary, Clueless, You've Got Mail, When Harry Met Sally",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.3958675861358643,
            "normalized": 0.8273907388959613
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 207,
        "original_data": {
            "instruction": "What should you do when your rock climbing partner calls out \"On Belay?\"",
            "input": "",
            "output": "1. Check that their harness is properly buckled at the waist and doubled back.\n2. Ensure the leg straps are properly bucked and doubled back. Check that the leg loops are not twisted.\n3. Ensure a figure eight knot is tied at the end of the climbing rope, extending through both loops in the harness and has a fisherman's knot as a backup.\n4. Check that your own harness is properly buckled around the waist, legs and that leg straps are not twisted.\n5. Ensure the climbing rope is in your belay device, that a locking carabiner goes through both the belay device (and rope if an ATC device) and your belay loop on your harness. Ensure the carabiner is locked.\n6. Ensure that you and your partner are on the same rope. Your partner should be on the side of the rope closest to the climbing wall.\n7. After all safety checks are completed, then call out \"On Belay!\"",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            1,
            0
        ],
        "entropy": {
            "raw": 2.816119909286499,
            "normalized": 0.6617485455104283
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 208,
        "original_data": {
            "instruction": "When rock climbing, what is the difference between lead climbing and top rope climbing?",
            "input": "",
            "output": "The difference between lead climbing and top rope climbing has to do with the location of the rope when the climb is started. In lead climbing the rope begins at the bottom attached to the climber as well as the belayer. The climber must then bring the rope from the bottom of the route to the top by clipping into a series of quick draws that are generally placed 10-15 feet apart. If a person were to fall while lead climbing, they would be caught by the last quick draw they clipped into. When top rope climbing, the rope is already at the top of the route. This means the climber must tie into one side of the rope while their belayer must clip into the other side of the rope. As the climber ascends higher up the route the belayer must take out the slack of the rope as opposed to lead climbing where the belayer would be giving slack. When a person falls in top rope climbing they will only fall a short distance as the rope is already clipped in above them at the top of the route, so how far they fall will be determined by how much slack the belayer has not taken in.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.5514976978302,
            "normalized": 0.5861421993800572
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 209,
        "original_data": {
            "instruction": "Why is it so rare to see a unseeded tennis player win a grand slam?",
            "input": "",
            "output": "To raise a grand slam trophy, a player has to win 7 consecutive best of 5 set matches and in all likelihood against several top 10 players. Tennis is considered to be a demanding sport both physically and mentally. The players have to be in top shape and be ready to duel out many tough 5 set matches during the course of the slam.  During grand slam matches, players can run anywhere from 3 to 4 miles with most of the movement being lateral. The top players have the distinct advantage that they have large support staff including dedicated physios and massage therapists who can get the player ready for subsequent matches. Besides, the top players have coaches who may have themselves won slams, and thus carry a wealth of experience. Sometimes players also enlist the services of data analysts who can do opponent video/data analysis to determine patterns/preferences of opponents. Such massive support staff carry a lot of payroll that most unseeded players cannot afford. For this reason, though at times we see a qualifier make it through to later stages of a slam, they will rarely ever get past quarter finals let alone win a grand slam.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.891979694366455,
            "normalized": 0.9691370555332729
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 210,
        "original_data": {
            "instruction": "What is the name of the largest red-light district in Amsterdam?",
            "input": "",
            "output": "The largest red-light district in Amsterdam is De Wallen.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 5.055147171020508,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3449999999999998,
            "strategy_details": {
                "raw": 2.6899999999999995,
                "normalized": 2.6899999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 211,
        "original_data": {
            "instruction": "Who is the winner of Abel Prize 2022 ?",
            "input": "",
            "output": "Dennis Parness Sullivan(American Mathematician)",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.024269104003906,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 212,
        "original_data": {
            "instruction": "How do I fix the engine of my car?",
            "input": "",
            "output": "This would depend on how complex the repair job is. But essentially you will need \n1. A basic set of tools including spanners, screwdrivers and pliers.\n2. An automotive manual for the model of your car. For example a Haynes manual.\n3. Once you have diagnosed the problem, watch some videos on YouTube on how to fix the problem.\n4. Based on the knowledge that you have gathered, attempt to fix your car\n\nIf you are enable to fix it, go to an experienced mechanic.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.0844979286193848,
            "normalized": 0.7384279796055385
        },
        "potential_entropy": {
            "strategy_component": 1.4725000000000001,
            "strategy_details": {
                "raw": 2.9450000000000003,
                "normalized": 2.9450000000000003,
                "components": {
                    "instruction": 0.6,
                    "input": 2.0,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.5,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 213,
        "original_data": {
            "instruction": "How many cents do I have if I have 3 dimes?",
            "input": "",
            "output": "If you have 3 dimes, you have 30 cents.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.960749387741089,
            "normalized": 0.7030712536403111
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 214,
        "original_data": {
            "instruction": "Which NFL team has won every Super Bowl they played in?",
            "input": "",
            "output": "Tampa Bay Buccaneers, Baltimore Ravens, New Orleans Saints, New York Jets",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.7193329334259033,
            "normalized": 0.6340951238359723
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 215,
        "original_data": {
            "instruction": "Who was Cleopatra's first husband",
            "input": "",
            "output": "Ptolemy Dionysus – her brother",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 5.227388381958008,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3299999999999998,
            "strategy_details": {
                "raw": 2.6599999999999997,
                "normalized": 2.6599999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.42000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 216,
        "original_data": {
            "instruction": "Classify each of the following as either a real or a fictional characters: professor Dumbledore, Mahatma Gandhi, King Arthur, captain Kirk, Luke skywalker, Julius Caesar",
            "input": "",
            "output": "Dumbledore - fictional ,Mahatma Gandhi - real, King Arthur - fictional, captain Kirk - fictional, Luke skywalker - fictional, Julius Caesar - real",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.4,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.6,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.209470272064209,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.9949999999999999,
            "strategy_details": {
                "raw": 1.9899999999999998,
                "normalized": 1.9899999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.6,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 217,
        "original_data": {
            "instruction": "Why is cold water coming out of my water heater?",
            "input": "",
            "output": "If your gas water heater is not working, first check to see if:\n\n - Is the water heater leaking? If so, turn off the water and gas to the heater, and contact a plumber to replace the heater.\n - Do you smell gas around your heater? If so, you likely have a gas leak--turn off the gas line to the heater and contact a plumber to replace the heater.\n \nIf neither of these are issues, check to see if the pilot light for you heater is lit. This is likely hidden behind a panel or door on the bottom of the front of the heater.  If the pilot light is not lit, follow the instructions to try re-lighting it.\n\nIf the pilot light does not light or continues to go out after heating the water, the thermal coupling on the water heater likely needs to be replaced. Contact a plumber to have this replaced.\n\nGenerally speaking, traditional tank-based, gas water heaters have a lifetime of 10-15 years. If the age water heater is within this range, you may want to prepare for replacing the water heater soon.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.457611560821533,
            "normalized": 0.5593175888061523
        },
        "potential_entropy": {
            "strategy_component": 1.2774999999999999,
            "strategy_details": {
                "raw": 2.5549999999999997,
                "normalized": 2.5549999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 218,
        "original_data": {
            "instruction": "How many different Star Trek television series and movies were there in total?",
            "input": "",
            "output": "In total, there were 21 when you add both the eight television series and 13 movies.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.6703364849090576,
            "normalized": 0.9058104242597308
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 219,
        "original_data": {
            "instruction": "What is the population of Canada?",
            "input": "",
            "output": "Canada is a country in North America with a population of 38.25 million people. Its ten provinces and three territories extend from the Atlantic Ocean to the Pacific Ocean and northward into the Arctic Ocean, making it the world's second-largest country by total area, with the world's longest coastline. It is characterized by a wide range of both meteorologic and geological regions. The country is sparsely inhabited, with the vast majority residing south of the 55th parallel in urban areas. Canada's capital is Ottawa and its three largest metropolitan areas are Toronto, Montreal, and Vancouver.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.3,
            "s_out_div": 0.7,
            "s_out_dens": 0.6,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.7,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.4,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            0
        ],
        "entropy": {
            "raw": 2.328627347946167,
            "normalized": 0.5224649565560477
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.85,
                    "output": 0.4799999999999999
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.7,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.4,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 220,
        "original_data": {
            "instruction": "Tell me whether the following animals are reptiles or amphibians: salamander, alligator, caiman, snake, toad.",
            "input": "",
            "output": "Salamanders and toads are amphibians. Alligators, caimans, and snakes are reptiles.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.5529704093933105,
            "normalized": 0.5865629741123745
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 221,
        "original_data": {
            "instruction": "List the colors of the rainbow",
            "input": "",
            "output": "Red, orange, yellow, green, blue, indigo, and purple",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            2,
            0
        ],
        "entropy": {
            "raw": 1.990787386894226,
            "normalized": 0.4259392533983503
        },
        "potential_entropy": {
            "strategy_component": 0.985,
            "strategy_details": {
                "raw": 1.97,
                "normalized": 1.97,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 222,
        "original_data": {
            "instruction": "Give me 5 ways to use a ripe avocado",
            "input": "",
            "output": "1. Avocado toast (if you’re feeling adventurous, add garlic oil and pepper and use sourdough bread)\n2. Guacamole (possible additives include: garlic, salt, red onions, tomatoes and lime juice) \n3. Diced on your favorite taco \n4. Blended in a smoothie (you can also freeze avocado cubes and add to smoothies later)\n5. Cubed or sliced with a little salt and pepper on a pretzel cracker",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5275917053222656,
            "normalized": 0.8650262015206474
        },
        "potential_entropy": {
            "strategy_component": 0.5974999999999999,
            "strategy_details": {
                "raw": 1.1949999999999998,
                "normalized": 1.1949999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 223,
        "original_data": {
            "instruction": "Extract the gold medals championships and the year won by Ebikewenimo Welson from the text. List them using bullets in the format {Champioship} - {Year} in a comma separated list.",
            "input": "Ebikewenimo Welson is a Nigerian freestyle wrestler. He is a four-time medalist at the Commonwealth Games and a two-time medalist at the African Games. \n\nHe won four medals at the Commonwealth Games: in 2010, he won the silver medal in the men's 55 kg event and he repeated this in 2014 with the silver medal in the men's 57 kg event. In 2018, he won one of the bronze medals in the men's 57 kg event. He won the silver medal in the men's 57 kg event at the 2022 Commonwealth Games.\n\nHe represented Nigeria at the 2019 African Games held in Rabat, Morocco and he won the silver medal in the men's 57 kg event.\n\nHe won the gold medal in the men's 57 kg event at the 2014 African Wrestling Championships and the silver medal in that event at the 2016 African Wrestling Championships. At the 2018 African Wrestling Championships held in Port Harcourt, Nigeria, he also won the silver medal in the men's 57 kg event. He was unable to compete at the 2019 African Wrestling Championships due to injury.\n\nIn 2021, he won a gold medal at the Baraza Champion of Champions wrestling tournament held in Yenagoa, Bayelsa State, Nigeria. A few months later, he competed at the 2021 African & Oceania Wrestling Olympic Qualification Tournament hoping to qualify for the 2020 Summer Olympics in Tokyo, Japan.\n\nHe won the silver medal in his event at the 2022 African Wrestling Championships held in El Jadida, Morocco. He won the silver medal in the men's 57 kg event at the 2022 Commonwealth Games held in Birmingham, England.",
            "output": "African Wrestling Championships - 2014, Baraza Champion of Champions - 2021",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9197909832000732,
            "normalized": 0.9770831380571637
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 224,
        "original_data": {
            "instruction": "Given this paragraph about Harold Brainerd Hersey, what did he do and when were his observations published?",
            "input": "Harold Brainerd Hersey (April 11, 1893 – March 1956) was an American pulp editor and publisher, publishing several volumes of poetry. His pulp industry observations were published in hardback as Pulpwood Editor (1937).",
            "output": "Harold Brainerd Herse was an American pulp editor and publisher. His observations were published in 1937.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.469415664672852,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 225,
        "original_data": {
            "instruction": "What is kombucha made of?",
            "input": "",
            "output": "Kombucha is a fermented drink made from tea, yeast, bacteria and sugar. It has grown in popularity, and some believe it has many health benefits.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.63674259185791,
            "normalized": 0.6104978833879743
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 226,
        "original_data": {
            "instruction": "In which Harry Potter's book did we learn about Voldemort's childhood?",
            "input": "",
            "output": "Voldemort's childhood was depicted in Book 6 of Harry Potter and The Half-Bood's Price. Through the private lessons Harry had with Dumbledore, Harry learned about Voldemort's family and upbringing.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.8,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 4.521968841552734,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.15,
            "strategy_details": {
                "raw": 2.3,
                "normalized": 2.3,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 227,
        "original_data": {
            "instruction": "In the field of economics, what is considered a monopoly?",
            "input": "",
            "output": "A monopoly is when a single company or individual has a dominant position in a market and can freely raise prices without risk of losing customers to competing products. Monopolies are generally discouraged as they reduce competition.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0720374584198,
            "normalized": 0.7348678452627999
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 228,
        "original_data": {
            "instruction": "Jill, a 60-year-old personal training client, wants to improve her hip flexibility. Identify two exercises commonly done in a Pilates class that would address core strength and flexibility, thus improving Jill's lower back health.",
            "input": "",
            "output": "One exercise is the criss-cross, which helps target the participant’s core strength and flexibility. The participant alternates bringing one shoulder towards the opposing knee, while keeping their hands on the back of their head, lying face-up, and curling their head up. Another exercise would be plank leg lift, which starts as a regular high plank. The participant alternates lifting one leg off the floor, keeping their core engaged to stabilize the movement.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.4370687007904053,
            "normalized": 0.8391624859401158
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 229,
        "original_data": {
            "instruction": "What is the best football club in the world in 23 between Arsenal, Real Madrid, Napoli, Manchester City, and Bayern Munich?",
            "input": "",
            "output": "The answer is : \n1. Manchester City \n2. Bayern Munich \n3. Napoli\n4. Real Madrid\n5. Arsenal",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.5122008323669434,
            "normalized": 0.5749145235334124
        },
        "potential_entropy": {
            "strategy_component": 1.2550000000000001,
            "strategy_details": {
                "raw": 2.5100000000000002,
                "normalized": 2.5100000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.5700000000000001
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 230,
        "original_data": {
            "instruction": "Is a square classified as a rectangle, or is a rectangle classified as a square?",
            "input": "",
            "output": "All squares are rectangles, but not all rectangles are squares. A rectangle is a 4 sided form with 4 right angles. A square has an additional requirement that all 4 sides must be equal in length. So the only rectangle that is a square is one with 4 equal sides.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.108643054962158,
            "normalized": 0.4596123014177595
        },
        "potential_entropy": {
            "strategy_component": 0.5075000000000001,
            "strategy_details": {
                "raw": 1.0150000000000001,
                "normalized": 1.0150000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 231,
        "original_data": {
            "instruction": "What is the Sun?",
            "input": "",
            "output": "The sun is how we refer to the star in our solar system, around which each of the planets rotate.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.53057861328125,
            "normalized": 0.8658796037946429
        },
        "potential_entropy": {
            "strategy_component": 1.39,
            "strategy_details": {
                "raw": 2.78,
                "normalized": 2.78,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 232,
        "original_data": {
            "instruction": "What was the profession of Araber Rahaman?",
            "input": "Araber Rahaman (died 28 April 2013) was an Indian politician. He was the Tripura MLA for Boxanagar from 1978–1988.",
            "output": "He was a politician.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.399333477020264,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 233,
        "original_data": {
            "instruction": "When will George R.R. Martin release the sixth book in the series \"A Song of Ice and Fire\"?",
            "input": "",
            "output": "No release date has been announced yet for the sixth book titled \"The Winds of Winter.\" Author George R.R. Martin previously indicated on various occasions that it would be released in 2016, then 2017, then 2018. In October 2022, he appeared on \"The Late Show with Stephen Colbert\" and said that approximately three-quarters of the novel had been written.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.2932722568511963,
            "normalized": 0.5123635019574847
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 234,
        "original_data": {
            "instruction": "Which cities can I visit on the west coast of Australia?",
            "input": "",
            "output": "Fremantle and Perth are located on the west coast of the country.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.8559463024139404,
            "normalized": 0.6731275149754116
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 235,
        "original_data": {
            "instruction": "How to make Indian Lemonade?",
            "input": "In India and Pakistan, where it is commonly known as nimbu paani, and in Bangladesh, lemonades may also contain salt and/or ginger juice. Shikanjvi is a traditional lemonade from this region, and can also be flavored with saffron, cumin and other spices.",
            "output": "Lemonade is typically made with sugar, lemons and water. The Indian version, commonly known as nimbu paani, is made a bit differently.\n\nIngredients\n- 1 lime's juice\n- 2 teaspoons granulated sugar\n- 1 teaspoons black salt \n- 1 teaspoon cumin powder (optional) \n- Ice (optional and as needed)\n- 1 cup water or sparking water or soda water\n\nMix ingredient together and stir for 10-15 seconds in a glass\n\nSome tips\n- If you don't have granulated sugar, you can use simple syrup\n- You may add some crushed mint leaves",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.95211124420166,
            "normalized": 0.7006032126290458
        },
        "potential_entropy": {
            "strategy_component": 0.635,
            "strategy_details": {
                "raw": 1.27,
                "normalized": 1.27,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 236,
        "original_data": {
            "instruction": "Tell me whether these golfers are right-handed or left-handed: Phil Mickelson, Sergio Garcia, Bubba Watson, Kevin Na, Carlos Ortiz, Brooks Koepka",
            "input": "",
            "output": "Phil Mickelson: Left-handed\nSergio Garcia: Right-handed\nBubba Watson: Left-handed\nKevin Na: Right-handed\nCarlos Ortiz: Right-handed\nBrooks Koepka: Right-handed",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.6952807903289795,
            "normalized": 0.627223082951137
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 237,
        "original_data": {
            "instruction": "Which season was Roger Federer's most important in his career?",
            "input": "Federer played his first junior match in 1996 at the age of 14 at a grade 2 tournament in Switzerland. His main accomplishments as a junior player came at Wimbledon in 1998 when he won both the boys' singles final over Irakli Labadze, and in doubles teamed with Olivier Rochus defeating the team of Michaël Llodra and Andy Ram. In addition he reached the US Open Junior final in 1998, losing to David Nalbandian. Federer won four ITF junior singles tournaments in his career, including the prestigious Orange Bowl, where he defeated Guillermo Coria in the final. By the end of 1998 he attained the No. 1 junior world ranking and was awarded ITF junior World Champion. He ended his junior career at the end of 1998 with a high-ranking of No. 1 in singles and No. 7 in doubles (both attained on December 31, 1998) and a win–loss record of 78–20 in singles and 36–21 in doubles.\n\nJunior Grand Slam results – Singles:\n\nAustralian Open: SF (1998)\nFrench Open: 1R (1998)\nWimbledon: W (1998)\nUS Open: F (1998)\n\nJunior Grand Slam results – Doubles:\n\nAustralian Open: SF (1998)\nFrench Open: 1R (1998)\nWimbledon: W (1998)\nUS Open: 1R (1998)\n\n1998–2002: Early professional career\nMain article: Roger Federer's early career\nFederer made his ATP debut at the 1998 Swiss Open Gstaad in his home country of Switzerland losing to Lucas Arnold Ker in the first round. Later that year, he won his first ATP match in Toulouse against Guillaume Raoux. He got a wildcard into the 1998 Swiss Indoors and lost in the first round to 4th seed and former world number 1 Andre Agassi. Federer finished his career as a 10-time champion of the tournament.\n\nFederer entered the top 100 ranking for the first time on 20 September 1999 and started at the 1999 Marseille Open defeating the reigning champion of the 1998 French Open, Spaniard Carlos Moyá. His first final came at the Marseille Open in 2000, where he lost to fellow Swiss Marc Rosset. Federer won the 2001 Hopman Cup representing Switzerland, along with world No. 1 Martina Hingis. The duo defeated the American pair of Monica Seles and Jan-Michael Gambill in the finals. Federer later said that his experience with Hingis \"definitely helped me to become the player I am today.\"\n\nFederer's first singles win was at the 2001 Milan Indoor tournament, where he defeated Julien Boutter in the final. Although he won his first title already in 1999 on the Challenger tour, winning the doubles event in Segovia, Spain with Dutchman Sander Groen, the final was played on Federer's 18th birthday. In 2001, Federer made his first Grand Slam quarterfinal at the French Open, losing to former world No. 2 and eventual finalist Àlex Corretja. His run to the French quarterfinals launched him into the top 15 for the first time in his career.\n\nHis international breakthrough came at the 2001 Wimbledon Championships, when the 19-year-old Federer faced the four-time defending champion and all-time Grand Slam leader Pete Sampras. Federer beat the No. 1 seed in a five-set match to reach the quarterfinals. In the quarters he faced Englishman Tim Henman, eventually losing in a fourth-set tiebreaker.\n\nThe first final he reached at the Masters level came at the 2002 Miami Masters event, where he lost to former and future No. 1 Andre Agassi on hard court. Federer won his first Master Series event at the 2002 Hamburg Masters on clay, over Marat Safin; the victory put him in the top 10 for the first time. Federer made 10 singles finals between 1998 and 2002, of which he won four and lost six. He also made six finals in doubles. He finished 2001 with an ATP ranking of No. 13, and 2002 was the first year he was ranked within the top 10, finishing at No. 6.\n\n2003: Grand Slam breakthrough at Wimbledon\nMain article: 2003 Roger Federer tennis season\nIn 2003, Federer won his first Grand Slam singles title at Wimbledon, beating Andy Roddick in the semifinals and Mark Philippoussis in the final. In August he had a chance to take over the No. 1 ranking for the first time from Andre Agassi if he made it to the Montreal final. However, he fell in the semifinals to Roddick, in a final-set tiebreaker, leaving him 120 points behind Agassi. This, coupled with early losses to David Nalbandian at Cincinnati and the US Open, denied Federer the chance to become No. 1 for the duration of the season.\n\nFederer won his first and only doubles Masters event in Miami with Max Mirnyi and made it to one singles Masters event in Rome on clay, which he lost. Federer made it to nine finals on the ATP Tour and won seven of them, including the 500 series events at Dubai and Vienna. Lastly, Federer won the year-end championships over Andre Agassi, finishing the year as world No. 2, narrowly behind Andy Roddick by only 160 points.\n\n2004: Imposing dominance\nMain article: 2004 Roger Federer tennis season\nDuring 2004 Federer won three Grand Slam singles titles and became the first person to do in one season so since Mats Wilander in 1988. His first major hard-court title came at the Australian Open over Marat Safin, making him the world No. 1 for the first time. He then won his second Wimbledon crown over Andy Roddick. At the US Open, Federer defeated the 2001 champion, Lleyton Hewitt, for his first title there.\n\nFederer won three ATP Masters events, one on clay in Hamburg, and the other two on hard surfaces at Indian Wells and in Canada. Federer took the ATP 500 series event at Dubai and wrapped up the year by winning the year-end championships for the second time. He also won his first tournament on home soil by capturing the Swiss Open in Gstaad. His 11 singles titles were the most of any player in two decades, and his record of 74–6 was the best since Ivan Lendl in 1986. He reached the year-end No. 1 ranking for the first time.\n\n2005: Consolidating dominance\nMain article: 2005 Roger Federer tennis season\nIn 2005, Federer failed to reach the finals of the first two Grand Slam tournaments, losing the Australian Open semifinal to eventual champion Safin after holding match points, and the French Open semifinal to eventual champion Rafael Nadal. However, Federer reestablished his dominance on grass, winning Wimbledon for a third time by defeating Andy Roddick. At the US Open, Federer defeated Andre Agassi in the latter's last major final.\n\nFederer also took four Masters wins: Indian Wells, Miami and Cincinnati on hard court, and Hamburg on clay. The win in Miami was particularly noteworthy as it was the first final contested between Federer and Nadal. Federer recovered from two sets and a break down to take the final in five sets. Furthermore, Federer won two ATP 500 series events at Rotterdam and Dubai. Federer lost the year-end championships to David Nalbandian in five sets while playing through a foot injury that sidelined him for almost the rest of the season after September. He maintained his position as No. 1 for the entire season.\n\nFederer won 11 singles titles, which ties his 2004 season. Federer's 81 match victories were the most since Pete Sampras in 1993, and his record of 81–4 (95.2%) remains the third-best winning percentage in the Open Era behind John McEnroe's 1984 and Jimmy Connors's 1974.\n\n2006: Career-best season\nMain article: 2006 Roger Federer tennis season\nThe 2006 season was statistically the best season of Federer's career. In November 2011, Stephen Tignor, chief editorial writer for Tennis.com, ranked Federer's 2006 season as statistically the second-best season of all time during the Open Era, behind Rod Laver's Grand Slam year of 1969.\n\n\nFederer hits a forehand at the 2006 US Open, where he became the first man in history to achieve the Wimbledon-US Open double for three consecutive seasons.\nFederer won 12 singles titles (the most of any player since Thomas Muster in 1995 and John McEnroe in 1984) and had a match record of 92–5 (the most wins since Ivan Lendl in 1982). Federer reached the finals in an astounding 16 of the 17 tournaments he entered during the season.\n\nIn 2006, Federer won three Grand Slam singles titles and reached the final of the other, with the only loss coming against Nadal in the French Open. This was Federer and Nadal's first meeting in a Grand Slam final. He was the first man to reach all four finals in a calendar year since Rod Laver in 1969. Federer defeated Nadal in the Wimbledon Championships final. In the Australian Open, Federer defeated Marcos Baghdatis, and at the US Open, Federer defeated 2003 champion Roddick. In addition, Federer reached six Masters finals, winning four on hard surfaces and losing two on clay to Nadal. Federer, however, consistently pushed Nadal to the limit on clay throughout the season taking him to fourth-set tiebreakers in Monte-Carlo and Paris, and a thrilling match in Rome that went to a deciding fifth-set tiebreaker.\n\nFederer won one ATP 500 series event in Tokyo and captured the year-end championships for the third time in his career, again finishing the year as world No. 1. Federer only lost to two players during 2006, to Nadal four times in finals, and to 19-year-old Andy Murray in the second round of the 2006 Cincinnati Masters, in what was Federer's only defeat before the final of a tournament that year. Federer finished the season on a 29-match winning streak, as well as winning 48 of his last 49 matches after the French Open.\n\nNear the end of the season he won his hometown tournament, the Swiss Indoors in Basel, Switzerland for the first time, having finished runner up in 2000 and 2001, and missing the tournament in 2004 and 2005 due to injuries.\n\n2007: Holding off young rivals\nMain article: 2007 Roger Federer tennis season\nIn 2007, Federer reached all four Grand Slam singles finals, winning three of them again. He won the Australian Open without dropping a set, beating Fernando González in the final. This made him the first man in the 21st century to accomplish the feat, as Björn Borg at the 1980 French Open was the last to win a Grand Slam tournament without the loss of a set. Federer had entered the year on a huge winning streak and after capturing his fourth Dubai crown Federer's winning streak stood at 41 matches, the longest of his career and only five shy of the record. Federer entered Indian Wells as the three-time defending champion, but his streak ended in controversy. He was defeated by an Argentine, Guillermo Cañas, who had failed a drug test for illegal doping.\n\n\nFederer was called \"Darth Federer\" by fans and commentators at the 2007 US Open.\nThis surprising first-round loss marked the first time he was defeated since August 2006, a period spanning over 7 months.\n\nDuring the clay season Federer's victory in the Hamburg Masters final was particularly impressive, as it snapped Nadal's 81-match winning streak on clay, an Open-era record. Federer turned the match around from a set down to sweep 12 of the final 14 games, including a final set bagel. At the French Open, some anticipated that Federer could become the first man in almost 40 years to hold all four majors simultaneously, having just resoundingly defeated young rival Nadal on clay entering the tournament. However, in a repeat of the previous year Federer played a tough four-set final against Nadal, but was undone by going 1/17 on break-point chances.\n\nAt Wimbledon Federer entered the tournament not only as the four-time defending champion, but also riding a 48-match winning streak on grass. Once again, he defeated Rafael Nadal for a second consecutive year in the final, this time in a thrilling five-set encounter that many analysts hailed as the greatest Wimbledon final since 1980. Victory at Wimbledon equaled him with Björn Borg for the record of five consecutive championships at the All England Club.\n\nFederer reached the final in Montreal before playing a young and relatively unknown Serbian named Novak Djokovic. Djokovic proved his potential by beating the world No. 1 in a final-set tiebreaker upset. Federer rebounded in Cincinnati to capture his fifth title of the year. Federer entered the US Open as the three-time defending champion and faced Djokovic in the final. This time, Federer prevailed in a close straight-set match. Victory in New York moved him ahead of Laver and Borg for third on the all-time list of major championship victories. Throughout the tournament the American press nicknamed him Darth Federer for his all-black attire (which included tuxedo-striped shorts) and the tournament played \"The Imperial March\" from Star Wars when he was announced onto the court for each of his matches. He closed out the year with victories in Basel and the year-end championships in Shanghai.\n\nHe finished the season as the year-end No. 1 for the fourth year in a row, demonstrating his dominance, and during these four years he won 11 Grand Slam singles titles. After his phenomenal triple Grand Slam season yet again, Federer became the only player in history to win three majors in a year for three years (2004, 2006, 2007). It was the third consecutive season that Federer held the No. 1 ranking for all 52 weeks of the year.\n\n2008: Illness, Olympic Gold, and fifth US Open\nMain article: 2008 Roger Federer tennis season\nFederer's success in 2008 was severely hampered by a lingering bout of mononucleosis, which he suffered during the first half of the year. At the end of the year he suffered a back injury.\n\nIn 2008, Federer captured one Grand Slam, a singles title at the US Open over Andy Murray. Federer was defeated by Nadal in two Grand Slam finals, the French Open and Wimbledon, which was regarded as the best match of tennis history by many, when he was going for six straight wins to break Björn Borg's record. He came back from two sets down to force a fifth set, where he fell just two points from the title. At the Australian Open Federer lost in the semifinals to eventual winner Djokovic, which ended his record of 10 consecutive finals. He lost twice in Masters finals on clay to Nadal, at Monte Carlo and Hamburg. Federer captured three titles playing in 250-level events at Estoril, Halle, and Basel.\n\nAt the Olympic Games Federer and Stan Wawrinka won the gold medal in doubles, after beating the Bryan brothers American team in the semifinals and the Swedish duo of Simon Aspelin and Thomas Johansson in the final. However, Federer could reach only the quarterfinals in the singles draw, bowing out to then No. 8 James Blake, ceding his No. 1 ranking to Nadal after being at the top for a record 237 consecutive weeks. He ended the year ranked No. 2.\n\nFederer entered the 2009 season with 13 Grand Slams, only one behind Pete Sampras' all-time record. The season began with a loss to Nadal in the final of the Australian Open in a hotly contested five-set match. Federer struggled following the defeat in Melbourne and entered the clay season without a title.\n\n\nFederer winning the 2009 French Open, and completing the career Grand Slam\nFederer's season turned around in the final Masters event of the clay season when he defeated Nadal on clay for only the second time to capture the Madrid Masters. Federer entered the French Open with few predicting him to win the elusive Parisian title having lost to Nadal in the final weekend for the past four seasons. After Nadal's unexpected defeat to Robin Söderling, Federer became the overwhelming favorite. In his next match, he came from two sets and break point down in the third set to defeat Tommy Haas in five sets. He also fought back from a two-sets-to-one deficit against a young Juan Martín del Potro to win a five setter in the semifinals. In the final, he faced Söderling, and with straight sets victory, he finally captured the Coupe des Mousquetaires and career Grand Slam. This victory also tied him with Pete Sampras for the most Grand Slam singles titles.\n\nFederer turned his sights to the grass courts of Wimbledon, where he breezed his way up to the final. In the championship match he faced long-time rival Andy Roddick in what was their eighth and final meeting at a Grand Slam. Roddick pushed Federer into a record-setting fifth set, which Federer claimed 16–14 to win his 15th Grand Slam singles title, breaking the all-time record of Pete Sampras.\n\nFederer continued his summer run by winning his third title on the lightning-fast courts of the Cincinnati Masters, defeating Novak Djokovic in the final. At the US Open he defeated Söderling in the quarters and Djokovic, for the third consecutive year, in the semifinals. On the penultimate point of the Djokovic match he hit what many consider to be the greatest shot of his career, a tweener winner, to set up match points. Federer was defeated by del Potro in the final despite leading two sets to one and falling just two points from the title in the fourth set.\n\nThe 2009 season was perhaps the most historically relevant of Federer's career as he completed a career Grand Slam by winning his first French Open title and won a men's record fifteenth Grand Slam singles title at Wimbledon, surpassing Pete Sampras's mark of fourteen. The Wimbledon final was also historic for being the longest Grand Slam final in terms of games played with Federer prevailing 16–14 in the fifth set.\n\nFederer finished the season as the year-end No. 1 for the fifth time in his career.\n\n2010: Fourth Australian Open\nMain article: 2010 Roger Federer tennis season\n\nFederer won a record 16th major at the 2010 Australian Open.\nFederer started the year with a win at the Australian Open, where he defeated Andy Murray in the final, extending the Grand Slam singles record to sixteen titles and matching Andre Agassi's record of four Australian Open titles. Since Wimbledon 2005 Federer had made 18 out of 19 finals in Grand Slam tournaments, a period of sustained excellence unparalleled in the Open Era. This tournament, however, marked the end of his dominance at the majors.\n\nAt the French Open, Federer won his 700th tour match and 150th tour match on clay. However, he failed to reach a Grand Slam semifinal for the first time since the 2004 French Open, losing to Söderling in the last 8 and relinquishing his No. 1 ranking, having been just one week away from equaling Pete Sampras's record of 286 weeks as world No. 1. In a huge upset at Wimbledon, Federer lost in the last 8 again to Tomáš Berdych and fell to No. 3 in the rankings for the first time in 6 years and 8 months.\n\nTowards the middle of July, Federer hired Pete Sampras' old coach Paul Annacone on a trial basis to put his tennis game and career back on track. At the 2010 US Open Federer reached the semifinals, where he lost a five-set match to Novak Djokovic after holding two match points. Federer made it to four Masters finals prevailing against Mardy Fish at the Cincinnati Masters.\n\nFederer finished the year in strong form, winning indoor titles at the Stockholm Open, Swiss Indoors, and the ATP Finals in London bringing his tally to 66 career titles. Federer won the year-end championships in London by beating rival Rafael Nadal for his fifth title at the event. He beat all contenders except Nadal in straight sets. It remains the only tournament in his career where Federer defeated all fellow members of the Big Four. In 2010 Federer finished in the top two for the eighth consecutive season.\n\n2011: Tour Finals title record\nMain article: 2011 Roger Federer tennis season\nThe year 2011 was a lean year for Federer, although great by most player's standards. He was defeated in straight sets in the semifinals of the 2011 Australian Open by eventual champion Novak Djokovic, marking the first time since July 2003 that he did not hold any of the four major titles. In the French Open semifinals, Federer ended Djokovic's undefeated streak of 43 consecutive wins with a four-set victory. Federer then lost in the final to Rafael Nadal. At Wimbledon Federer advanced to his 29th consecutive Grand Slam quarterfinal, losing to Jo-Wilfried Tsonga. It marked the first time in his career that he had lost a Grand Slam tournament match after winning the first two sets.\n\nAt the US Open Federer lost in the semifinals to Novak Djokovic in five sets. In a repeat of previous year's semifinal event, Federer again squandered two match points on his own serve before losing after winning first two sets for second consecutive time in the year. The loss meant that it was the first time since 2002 that Federer had not won any of the four grand slam titles.\n\nIn September 2011, in a South African poll, Federer was voted the second most trusted and respected person in the world next to Nelson Mandela.\n\nFederer finished the season successfully in the indoor season, winning his last three tournaments of the year at the Swiss Indoors, Paris Masters, and ATP Finals, forming a 16 match winning streak. Federer finished the year ranked No. 3.",
            "output": "2009 was arguably the most important of his career since he won his first and only French Open title, completing the career grand slam. He was also able to surpass Pete Sampras' record of fourteen men's grand slam titles.\n\nUntil then, Federer had won the three other grand slams - Wimbledon, Australian Open and US Open but the French Open was elusive as he had to repeatedly compete on the clay court of Rolland Garros against Rafael Nadal, who was a clay court expert and he lost to him on multiple occasions.\n\n2004 was also an important year in Federer's career because he established dominance in the sport by winning three out of the four grand slams of the year and ending the year as the world number one for the first time in his long career.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0615415573120117,
            "normalized": 0.7318690163748605
        },
        "potential_entropy": {
            "strategy_component": 0.28550000000000003,
            "strategy_details": {
                "raw": 0.5710000000000001,
                "normalized": 0.5710000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.261
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 238,
        "original_data": {
            "instruction": "How did Washington become a state?",
            "input": "Washington, officially the State of Washington and often referred to as Washington state to distinguish it from the USA's capital, Washington, D.C., is a state in the Pacific Northwest region of the Western United States. Named for George Washington—the first U.S. president—the state was formed from the western part of the Washington Territory, which was ceded by the British Empire in 1846, by the Oregon Treaty in the settlement of the Oregon boundary dispute. The state is bordered on the west by the Pacific Ocean, Oregon to the south, Idaho to the east, and the Canadian province of British Columbia to the north. It was admitted to the Union as the 42nd state in 1889. Olympia is the state capital; the state's largest city is Seattle.",
            "output": "Washington became the 42nd state of the United States of America in 1889. The United States Congress passed an act enabling the territories of Washington, North Dakota, South Dakota, and Montana to seek statehood.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.550982713699341,
            "normalized": 0.5859950610569545
        },
        "potential_entropy": {
            "strategy_component": 0.5225,
            "strategy_details": {
                "raw": 1.045,
                "normalized": 1.045,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 239,
        "original_data": {
            "instruction": "Why do people have children?",
            "input": "",
            "output": "There are many reasons people have children.  A common reason is simply tradition and expectations.  Some people have grown up expecting to have children, their parents expect them to have children, and all of their friends have children, so they have children.  Another reason is economic.  In agricultural societies children can be an economic benefit, providing labor to help the family manage the farm.  Another common reason is that children can provide support when the parents become elderly.  Finally, the parents may simply be looking for the meaning, affection, and love that children bring to their lives.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.8036718368530273,
            "normalized": 0.6581919533865792
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 240,
        "original_data": {
            "instruction": "Can I copy my Delta Lake table to another location?",
            "input": "",
            "output": "Yes you can copy your Delta Lake table to another location. Remember to copy files without changing the timestamps to ensure that the time travel with timestamps will be consistent.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.2,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.8,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.612377643585205,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0699999999999998,
            "strategy_details": {
                "raw": 2.1399999999999997,
                "normalized": 2.1399999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.8,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 241,
        "original_data": {
            "instruction": "last 5 prime minister of India",
            "input": "",
            "output": "Narendra Modi\nManmohan Singh\nAtal Bihari Vajpayee\nDeve Gowda\nP.V Narsimha Rao",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.0819315910339355,
            "normalized": 0.7376947402954102
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 242,
        "original_data": {
            "instruction": "Given these paragraphs about the Tesla bot, where and when was it announced?",
            "input": "Optimus, also known as Tesla Bot, is a conceptual general-purpose robotic humanoid under development by Tesla, Inc. It was announced at the company's Artificial Intelligence (AI) Day event on August 19, 2021. CEO Elon Musk claimed during the event that Tesla would likely build a prototype by 2022. Musk is on record having said that he thinks Optimus \"has the potential to be more significant than the vehicle business over time.\"\n\nTesla Bot is planned to measure 5 ft 8 in (173 cm) tall and weigh 125 lb (57 kg). According to the presentation made during the first AI Day event, a Tesla Bot will be \"controlled by the same AI system Tesla is developing for the advanced driver-assistance system used in its cars\" and have a carrying capacity of 45 lb (20 kg). Proposed tasks for the product are ones that are \"dangerous, repetitive and boring\", such as providing manufacturing assistance.\n\nhttps://en.wikipedia.org/wiki/Optimus_(robot)",
            "output": "It was announced at the company's Artificial Intelligence (AI) Day event on August 19, 2021.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8708550930023193,
            "normalized": 0.677387169429234
        },
        "potential_entropy": {
            "strategy_component": 0.3975000000000001,
            "strategy_details": {
                "raw": 0.7950000000000002,
                "normalized": 0.7950000000000002,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 243,
        "original_data": {
            "instruction": "When did mountain biking start?",
            "input": "Late 1800s\nOne of the first examples of bicycles modified specifically for off-road use is the expedition of Buffalo Soldiers from Missoula, Montana, to Yellowstone in August 1896.[failed verification]\n\n1900s–1960s\nBicycles were ridden off-road by road racing cyclists who used cyclocross as a means of keeping fit during the winter. Cyclo-cross eventually became a sport in its own right in the 1940s, with the first world championship taking place in 1950.\n\nThe Rough Stuff Fellowship was established in 1955 by off-road cyclists in the United Kingdom.\n\nIn Oregon in 1966, one Chemeketan club member, D. Gwynn, built a rough terrain trail bicycle. He named it a \"mountain bicycle\" for its intended place of use. This may be the first use of that name.\n\nIn England in 1968, Geoff Apps, a motorbike trials rider, began experimenting with off-road bicycle designs. By 1979 he had developed a custom-built lightweight bicycle which was uniquely suited to the wet and muddy off-road conditions found in the south-east of England. They were designed around 2 inch x 650b Nokian snow tires though a 700x47c (28 in.) version was also produced. These were sold under the Cleland Cycles brand until late 1984. Bikes based on the Cleland design were also sold by English Cycles and Highpath Engineering until the early 1990s.\n\n1970s–1980s\nThere were several groups of riders in different areas of the U.S.A. who can make valid claims to playing a part in the birth of the sport. Riders in Crested Butte, Colorado, and Mill Valley, California, tinkered with bikes and adapted them to the rigors of off-road riding. Modified heavy cruiser bicycles, old 1930s and '40s Schwinn bicycles retrofitted with better brakes and fat tires, were used for freewheeling down mountain trails in Marin County, California, in the mid-to-late 1970s. At the time, there were no mountain bikes. The earliest ancestors of modern mountain bikes were based around frames from cruiser bicycles such as those made by Schwinn. The Schwinn Excelsior was the frame of choice due to its geometry. Riders used balloon-tired cruisers and modified them with gears and motocross or BMX-style handlebars, creating \"klunkers\". The term would also be used as a verb since the term \"mountain biking\" was not yet in use. The first person known to fit multiple speeds and drum brakes to a klunker is Russ Mahon of Cupertino, California, who used the resulting bike in cyclo-cross racing. Riders would race down mountain fire roads, causing the hub brake to burn the grease inside, requiring the riders to repack the bearings. These were called \"Repack Races\" and triggered the first innovations in mountain bike technology as well as the initial interest of the public (on Mt. Tamalpais in Marin CA, there is still a trail titled \"Repack\"—in reference to these early competitions). The sport originated in California on Marin County's Mount Tamalpais.\n\nIt was not until the late 1970s and early 1980s that road bicycle companies started to manufacture mountain bicycles using high-tech lightweight materials. Joe Breeze is normally credited with introducing the first purpose-built mountain bike in 1978. Tom Ritchey then went on to make frames for a company called MountainBikes, a partnership between Gary Fisher, Charlie Kelly and Tom Ritchey. Tom Ritchey, a welder with skills in frame building, also built the original bikes. The company's three partners eventually dissolved their partnership, and the company became Fisher Mountain Bikes, while Tom Ritchey started his own frame shop.\n\nThe first mountain bikes were basically road bicycle frames (with heavier tubing and different geometry) with a wider frame and fork to allow for a wider tire. The handlebars were also different in that they were a straight, transverse-mounted handlebar, rather than the dropped, curved handlebars that are typically installed on road racing bicycles. Also, some of the parts on early production mountain bicycles were taken from the BMX bicycle. Other contributors were Otis Guy and Keith Bontrager.\n\nTom Ritchey built the first regularly available mountain bike frame, which was accessorized by Gary Fisher and Charlie Kelly and sold by their company called MountainBikes (later changed to Fisher Mountain Bikes, then bought by Trek, still under the name Gary Fisher, currently sold as Trek's \"Gary Fisher Collection\"). The first two mass-produced mountain bikes were sold in the early 1980s: the Specialized Stumpjumper and Univega Alpina Pro. In 1988, The Great Mountain Biking Video was released, soon followed by others. In 2007, Klunkerz: A Film About Mountain Bikes was released, documenting mountain bike history during the formative period in Northern California. Additionally, a group of mountain bikers called the Laguna Rads formed a club during the mid eighties and began a weekly ride, exploring the uncharted coastal hillsides of Laguna Beach, California. Industry insiders suggest that this was the birth of the freeride movement, as they were cycling up and down hills and mountains where no cycling specific trail network prexisted. The Laguna Rads have also held the longest running downhill race once a year since 1986.\n\nAt the time, the bicycle industry was not impressed with the mountain bike, regarding mountain biking to be short-term fad. In particular, large manufacturers such as Schwinn and Fuji failed to see the significance of an all-terrain bicycle and the coming boom in 'adventure sports'. Instead, the first mass-produced mountain bikes were pioneered by new companies such as MountainBikes (later, Fisher Mountain Bikes), Ritchey, and Specialized. Specialized was an American startup company that arranged for production of mountain bike frames from factories in Japan and Taiwan. First marketed in 1981, Specialized's mountain bike largely followed Tom Ritchey's frame geometry, but used TiG welding to join the frame tubes instead of fillet-brazing, a process better suited to mass production, and which helped to reduce labor and manufacturing cost. The bikes were configured with 15 gears using derailleurs, a triple chainring, and a cogset with five sprockets.",
            "output": "Mountain biking as a sport came into existence in various areas in the USA in the late 1970s and early 1980s.  While many groups of riders in different US locations claim the birthright to mountain biking, it's clear that there wasn't a single person nor a single location that served as the starting point for the sport.  Riders in Crested Butte, CO, Marin County, CA, Mill Valley, CA, Cupertino, CA, and several other locations contributed significantly to the birth of the sport.\n\nIt wasn't until the late 1970s and early 1980s that bicycle companies began manufacturing mountain bikes with specific high-tech lightweight materials that popularized the sport.  Joe Breeze (Marin County) is often credited with building the first, purpose-built mountain bike in 1978.\n\nIt's important to note that there were mountain bike concepts and prototypes built well before this period, with the earliest concept belonging to the off-road bicycles used by the Buffalo Soldiers in 1896.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.6980972290039062,
            "normalized": 0.6280277797154018
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 244,
        "original_data": {
            "instruction": "Do you know how many episodes of Game of Thrones are there?",
            "input": "",
            "output": "Yes - there are 73 episodes for Game of Thrones that are spread over 8 seasons. The first season was released April 17, 2011 by HBO and season 8 concluded on May 19, 2019. The entire series is 70 hours and 14 minutes long and would take you just under 3 full days if you watched every episode back to back.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.5203857421875,
            "normalized": 0.5772530691964286
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 245,
        "original_data": {
            "instruction": "Who was Alexander Greek Thomson",
            "input": "Alexander \"Greek\" Thomson (9 April 1817 – 22 March 1875) was an eminent Scottish architect and architectural theorist who was a pioneer in sustainable building. Although his work was published in the architectural press of his day, it was little appreciated outside Glasgow during his lifetime. It has only been since the 1950s and 1960s that his critical reputation has revived—not least of all in connection with his probable influence on Frank Lloyd Wright.\n\nHenry-Russell Hitchcock wrote of Thomson in 1966: \"Glasgow in the last 150 years has had two of the greatest architects of the Western world. C. R. Mackintosh was not highly productive but his influence in central Europe was comparable to such American architects as Louis Sullivan and Frank Lloyd Wright. An even greater and happily more productive architect, though one whose influence can only occasionally be traced in America in Milwaukee and in New York City and not at all as far as I know in Europe, was Alexander Thomson\".",
            "output": "Alexander Greek Thomson (9 April 1817 – 22 March 1875) was an eminent Scottish architect and architectural theorist who was a pioneer in sustainable building. Although his work was published in the architectural press of his day, it was little appreciated outside Glasgow during his lifetime. It has only been since the 1950s and 1960s that his critical reputation has revived—not least of all in connection with his probable influence on Frank Lloyd Wright.\n\nHenry-Russell Hitchcock wrote of Thomson in 1966: \"Glasgow in the last 150 years has had two of the greatest architects of the Western world. C. R. Mackintosh was not highly productive but his influence in central Europe was comparable to such American architects as Louis Sullivan and Frank Lloyd Wright. An even greater and happily more productive architect, though one whose influence can only occasionally be traced in America in Milwaukee and in New York City and not at all as far as I know in Europe, was Alexander Thomson\".",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 1.0,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.0,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0800416469573975,
            "normalized": 0.7371547562735421
        },
        "potential_entropy": {
            "strategy_component": 0.38749999999999996,
            "strategy_details": {
                "raw": 0.7749999999999999,
                "normalized": 0.7749999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.135
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.0,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 246,
        "original_data": {
            "instruction": "Which of the following are chocolates: 5 Star, Dairy Milk, Bar One, Chips Ahoy, Oreo, Bounty, Mars, Milky Way, Lay's, Bananas.",
            "input": "",
            "output": "5 Star, Dairy Milk, Bar One, Bounty, Mars, Milky Way are chocolates.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.43656587600708,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.8125,
            "strategy_details": {
                "raw": 1.625,
                "normalized": 1.625,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.1,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.5,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 247,
        "original_data": {
            "instruction": "What is the wealthiest zip code in the United States?",
            "input": "",
            "output": "Atherton, California",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.2,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.8,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 6.457935810089111,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0399999999999998,
            "strategy_details": {
                "raw": 2.0799999999999996,
                "normalized": 2.0799999999999996,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.8,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 248,
        "original_data": {
            "instruction": "Qui était Clovis?",
            "input": "",
            "output": "Clovis Ier, en latin Chlodovechus, né vers 466 et mort à Paris le 27 novembre 511, est roi des Francs saliens, puis roi de tous les Francs de 481 à 511.\n\nIssu de la dynastie des Mérovingiens, il est le fils de Childéric Ier, roi des Francs saliens de Tournai (en actuelle Belgique), et de la reine Basine de Thuringe. Chef militaire, il accroît considérablement le territoire du petit royaume des Francs saliens, dont il hérite à la mort de son père, pour finir par unifier une grande partie des royaumes francs, repousser Alamans et Burgondes et annexer les territoires des Wisigoths dans le Sud de la Gaule.\n\nLe règne de Clovis est surtout connu par la description qu'en fit Grégoire de Tours, évêque gallo-romain dont l'Histoire des Francs est riche d'enseignements, mais dont la visée, essentiellement édifiante, s'accompagne d'un manque de précision et de cohérence historique. Les éléments de la vie de Clovis ne sont pas connus de manière certaine et leur « habillage » est le plus souvent suspect. Néanmoins, Clovis est considéré dans l'historiographie comme un des personnages les plus importants de l'histoire de France.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.8110995292663574,
            "normalized": 0.6603141512189593
        },
        "potential_entropy": {
            "strategy_component": 1.2025,
            "strategy_details": {
                "raw": 2.405,
                "normalized": 2.405,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 249,
        "original_data": {
            "instruction": "Given these paragraphs about Asperger Multiomics, what is the goal in combining all the \"omes\"?",
            "input": "Multiomics, multi-omics, integrative omics, \"panomics\" or \"pan-omics\" is a biological analysis approach in which the data sets are multiple \"omes\", such as the genome, proteome, transcriptome, epigenome, metabolome, and microbiome (i.e., a meta-genome and/or meta-transcriptome, depending upon how it is sequenced); in other words, the use of multiple omics technologies to study life in a concerted way. By combining these \"omes\", scientists can analyze complex biological big data to find novel associations between biological entities, pinpoint relevant biomarkers and build elaborate markers of disease and physiology. In doing so, multiomics integrates diverse omics data to find a coherently matching geno-pheno-envirotype relationship or association. The OmicTools service lists more than 99 softwares related to multiomic data analysis, as well as more than 99 databases on the topic.\n\nSystems biology approaches are often based upon the use of panomic analysis data. The American Society of Clinical Oncology (ASCO) defines panomics as referring to \"the interaction of all biological functions within a cell and with other body functions, combining data collected by targeted tests ... and global assays (such as genome sequencing) with other patient-specific information.\"\n\nSingle-cell multiomics\nA branch of the field of multiomics is the analysis of multilevel single-cell data, called single-cell multiomics. This approach gives us an unprecedent resolution to look at multilevel transitions in health and disease at the single cell level. An advantage in relation to bulk analysis is to mitigate confounding factors derived from cell to cell variation, allowing the uncovering of heterogeneous tissue architectures.\n\nMethods for parallel single-cell genomic and transcriptomic analysis can be based on simultaneous amplification or physical separation of RNA and genomic DNA. They allow insights that cannot be gathered solely from transcriptomic analysis, as RNA data do not contain non-coding genomic regions and information regarding copy-number variation, for example. An extension of this methodology is the integration of single-cell transcriptomes to single-cell methylomes, combining single-cell bisulfite sequencing to single cell RNA-Seq. Other techniques to query the epigenome, as single-cell ATAC-Seq and single-cell Hi-C also exist.\n\nA different, but related, challenge is the integration of proteomic and transcriptomic data. One approach to perform such measurement is to physically separate single-cell lysates in two, processing half for RNA, and half for proteins. The protein content of lysates can be measured by proximity extension assays (PEA), for example, which use DNA-barcoded antibodies. A different approach uses a combination of heavy-metal RNA probes and protein antibodies to adapt mass cytometry for multiomic analysis.\n\nMultiomics and machine learning\nIn parallel to the advances in highthroughput biology, machine learning applications to biomedical data analysis are flourishing. The integration of multi-omics data analysis and machine learning has led to the discovery of new biomarkers. For example, one of the methods of the mixOmics project implements a method based on sparse Partial Least Squares regression for selection of features (putative biomarkers).\n\n\nhttps://en.wikipedia.org/wiki/Multiomics",
            "output": "By combining these \"omes\", scientists can analyze complex biological big data to find novel associations between biological entities, pinpoint relevant biomarkers and build elaborate markers of disease and physiology. In doing so, multiomics integrates diverse omics data to find a coherently matching geno-pheno-envirotype relationship or association.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.92,
            "s_out_div": 0.88,
            "s_out_dens": 0.9,
            "s_out_bg": 0.87
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.07999999999999996,
            "s_out_div": 0.12,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.13
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 4.898468017578125,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.21950000000000003,
            "strategy_details": {
                "raw": 0.43900000000000006,
                "normalized": 0.43900000000000006,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.12899999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.07999999999999996,
                    "out_div": 0.12,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.13
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 250,
        "original_data": {
            "instruction": "Create an itinerary for 3 days in Park City Utah that is good for children and adults. Include hotels, food options, one day of skiing and activities.",
            "input": "",
            "output": "Ok, here are three days for a family trip to Park city.\n\nDay one:\n\n- Go grocery shopping at Whole Food Market to pick up snacks, and food for lunches.\n- Pick up rental skiing at Christy's for all adults and children whole are skiing.\n- Eat dinner at River Horse Tavern on Main street.\n\nDay two:\n- Go skiing at Park City Resort. Purchase day passes in person or pre-purchase Epic passes online.\n- Stop with the kids at Rocky Mountain Chocolate Company for a treat.\n- Eat dinner at High Saloon on Park Avenue.\n\nDay three:\n- Go to Park City Library which has a kids' section.\n- Shop at galleries, thrift stores, or gift shops on Main Street.\n- Eat dinner at Butcher House Chops and Grill.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.8709583282470703,
            "normalized": 0.9631309509277344
        },
        "potential_entropy": {
            "strategy_component": 0.5875,
            "strategy_details": {
                "raw": 1.175,
                "normalized": 1.175,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 251,
        "original_data": {
            "instruction": "Who won the 1974 Nobel Memorial Prize in Economic Sciences?",
            "input": "Friedrich August von Hayek CH FBA (/ˈhaɪək/ HY-ək, German: [ˈfʁiːdʁɪç ˈʔaʊɡʊst fɔn ˈhaɪɛk] (listen); 8 May 1899 – 23 March 1992), often referred to by his initials F. A. Hayek, was an Austrian-British intellectual who made contributions to economics, political science, psychology, intellectual history, philosophy and other fields. Hayek shared the 1974 Nobel Memorial Prize in Economic Sciences with Gunnar Myrdal for work on money and economic fluctuations, and the interdependence of economic, social and institutional phenomena. His account of how prices communicate information is widely regarded as an important contribution to economics that led to him receiving the prize",
            "output": "Friedrich August von Hayek and Gunnar Myrdal shared the 1974 Nobel Memorial Prize in Economic Sciences.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.579423427581787,
            "normalized": 0.594120979309082
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 252,
        "original_data": {
            "instruction": "Given this paragraph about Pittsford, New York, what year was the town settled?",
            "input": "The Town of Pittsford (formerly part of the town of Northfield) was settled in 1789 and incorporated in 1796. The Village of Pittsford was incorporated in 1827. It was named by Colonel Caleb Hopkins, War of 1812 hero and subsequently Pittsford Town Supervisor, for the town of his birth, Pittsford, Vermont.",
            "output": "The town of Pittsford was settled in 1789.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.4801580905914307,
            "normalized": 0.8514737401689801
        },
        "potential_entropy": {
            "strategy_component": 0.3975000000000001,
            "strategy_details": {
                "raw": 0.7950000000000002,
                "normalized": 0.7950000000000002,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 253,
        "original_data": {
            "instruction": "Give me a list of hobbies",
            "input": "",
            "output": "Pickleball, soccer, writing, reading, cooking, running, biking",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.295578479766846,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.105,
            "strategy_details": {
                "raw": 2.21,
                "normalized": 2.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 254,
        "original_data": {
            "instruction": "what is the indian subcontinent",
            "input": "The Indian subcontinent is a physiographical region in Southern Asia, situated on the Indian Plate, projecting southwards into the Indian Ocean from the Himalayas.",
            "output": "The Indian subcontinent is a physiographical region in Southern Asia, situated on the Indian Plate, projecting southwards into the Indian Ocean from the Himalayas. Geopolitically, it includes the countries of Bangladesh, Bhutan, India, Maldives, Nepal, Pakistan, and Sri Lanka. The terms \"Indian subcontinent\" and \"South Asia\" are often used interchangeably to denote the region, although the geopolitical term of South Asia frequently includes Afghanistan, which may otherwise be classified as Central Asian.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.1951634883880615,
            "normalized": 0.4843324252537319
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 255,
        "original_data": {
            "instruction": "Which of the following cast members have names that start with the letters A, B or P?",
            "input": "Balraj Sahni as Abdul Rehman Khan\nSonu as Mini\nUsha Kiran as Rama, Mini's mother\nPadma\nLaxmi\nSarita Devi\nAnwari Bai\nLeela Agha\nBaby Farida\nAsit Sen as Bhola",
            "output": "Balraj Sahni, Padma, Anwari Bai, Baby Farida, Asit Sen",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.997232437133789,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 256,
        "original_data": {
            "instruction": "What states make up New England?",
            "input": "",
            "output": "New England is comprised of the states Rhode Island, Connecticut, Massachusetts, Vermont, New Hampshire, and Maine",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.0215957164764404,
            "normalized": 0.7204559189932687
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 257,
        "original_data": {
            "instruction": "Where is New Hampshire?",
            "input": "",
            "output": "Location is a relative concept, although for the sake of this response, let's assume that you are looking at a standard map of the world built using the widely accepted Mercator projection. New Hampshire is a state located in the north east corner of the United States of America, which itself is located in the Northern and Western hemispheres. New Hampshire is bordered on 4 sides by Massachusetts to the south, Vermont to the west, Maine to the east, and Canada to the north.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.5094916820526123,
            "normalized": 0.5741404805864606
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 258,
        "original_data": {
            "instruction": "Which player has won the most Superbowls?",
            "input": "",
            "output": "Tom Brady",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 12.594620704650879,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1575000000000002,
            "strategy_details": {
                "raw": 2.3150000000000004,
                "normalized": 2.3150000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 259,
        "original_data": {
            "instruction": "Which Dutch artist painted “Girl with a Pearl Earring”?",
            "input": "",
            "output": "Vermeer",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.2,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.8,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 11.765624046325684,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3900000000000001,
            "strategy_details": {
                "raw": 2.7800000000000002,
                "normalized": 2.7800000000000002,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.8,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 260,
        "original_data": {
            "instruction": "Name some fruits that have green skin.",
            "input": "",
            "output": "Grapes, Green Apples and Avocados all have green skin.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.674328327178955,
            "normalized": 0.9069509506225586
        },
        "potential_entropy": {
            "strategy_component": 1.2699999999999998,
            "strategy_details": {
                "raw": 2.5399999999999996,
                "normalized": 2.5399999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 261,
        "original_data": {
            "instruction": "Give step by step instructions on how to make a Long Island Ice Tea.",
            "input": "",
            "output": "Ingredients:\n3/4 oz vodka\n3/4 oz rum\n3/4 oz gin\n3/4 oz tequila\n3/4 oz triple sec\n3/4 oz simple syrup\n3/4 oz lemon juice\nCola to fill\nLemon wedge to garnish\n\nSteps:\n1. Add the vodka, rum, gin, tequila, triple sec, simple syrup and lemon juice to a glass filled with ice. \n2. Fill the glass with cola and stir.\n3. Garnish with the lemon wedge and serve.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.9,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 1.4688576459884644,
            "normalized": 0.2768164702824184
        },
        "potential_entropy": {
            "strategy_component": 0.5974999999999999,
            "strategy_details": {
                "raw": 1.1949999999999998,
                "normalized": 1.1949999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 262,
        "original_data": {
            "instruction": "What is pleitropy?",
            "input": "",
            "output": "Pleiotropy occurs when one gene influences two or more seemingly unrelated phenotypic traits. Such a gene that exhibits multiple phenotypic expression is called a pleiotropic gene. Mutation in a pleiotropic gene may have an effect on several traits simultaneously, due to the gene coding for a product used by a myriad of cells or different targets that have the same signaling function.\n\nPleiotropy can arise from several distinct but potentially overlapping mechanisms, such as gene pleiotropy, developmental pleiotropy, and selectional pleiotropy. Gene pleiotropy occurs when a gene product interacts with multiple other proteins or catalyzes multiple reactions. Developmental pleiotropy occurs when mutations have multiple effects on the resulting phenotype. Selectional pleiotropy occurs when the resulting phenotype has many effects on fitness (depending on factors such as age and gender).\n\nAn example of pleiotropy is phenylketonuria, an inherited disorder that affects the level of phenylalanine, an amino acid that can be obtained from food, in the human body. Phenylketonuria causes this amino acid to increase in amount in the body, which can be very dangerous. The disease is caused by a defect in a single gene on chromosome 12 that codes for enzyme phenylalanine hydroxylase, that affects multiple systems, such as the nervous and integumentary system.\n\nPleiotropic gene action can limit the rate of multivariate evolution when natural selection, sexual selection or artificial selection on one trait favors one allele, while selection on other traits favors a different allele. Some gene evolution is harmful to an organism. Genetic correlations and responses to selection most often exemplify pleiotropy.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            1,
            0
        ],
        "entropy": {
            "raw": 2.4728758335113525,
            "normalized": 0.5636788095746722
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 263,
        "original_data": {
            "instruction": "How food insecurity can lead to obesity. How can teachers help alleviate food insecurity if they identify this problem in children’s families?",
            "input": "",
            "output": "Food insecurity can lead to less expensive, less healthy food choices that are limited in variety. In addition, children who do not have a consistent source of food may overeat when food is available. Low-income families often must choose high-fat, energy-dense, inexpensive food to maximize their calories per dollar spent. They also face additional risk factors for obesity, such as limited access to healthy and affordable foods and a lack of safe and accessible recreational facilities, parks, and walking trails that support physical activity.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.8,
            "s_out_div": 0.65,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.35,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.846830368041992,
            "normalized": 0.670522962297712
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.35,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 264,
        "original_data": {
            "instruction": "Give me a list of ten Mayan ruins to visit in Central America.",
            "input": "",
            "output": "- Chichen Itza in Mexico\n- Tikal in Guatemala\n- Caracol in Belize\n- Copan in Honduras\n- Palenque in Mexico\n- Tulum in Mexico\n- Uxmal in Mexico\n- Monte Alban in Mexico\n- Calakmul in Mexico\n- Ek Balam in Mexico",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 1.8397440910339355,
            "normalized": 0.38278402600969585
        },
        "potential_entropy": {
            "strategy_component": 1.1125,
            "strategy_details": {
                "raw": 2.225,
                "normalized": 2.225,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 265,
        "original_data": {
            "instruction": "What is the Mayan Civilization known for?",
            "input": "",
            "output": "The Maya civilization of the Mesoamerican people is known by its ancient temples and glyphs. Its Maya script is the most sophisticated and highly developed writing system in the pre-Columbian Americas. The first Maya cities developed around 750 BC, and by 500 BC.It is also noted for its art, architecture, mathematics, calendar, and astronomical system.\nThe Maya civilization developed in the Maya Region, an area that today comprises southeastern Mexico, all of Guatemala and Belize, and the western portions of Honduras and El Salvador.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.7596521377563477,
            "normalized": 0.6456148965018136
        },
        "potential_entropy": {
            "strategy_component": 0.47000000000000003,
            "strategy_details": {
                "raw": 0.9400000000000001,
                "normalized": 0.9400000000000001,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 266,
        "original_data": {
            "instruction": "Classify the following as either a painting or a sculpture: Mona Lisa, The Starry Night, Venus of Willendorf, Bust of Nefertiti, The Terracotta Army,  The Last Supper, The Burghers of Calais",
            "input": "",
            "output": "Painting, painting, statue, statue, statue, painting, statue",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.054520845413208,
            "normalized": 0.729863098689488
        },
        "potential_entropy": {
            "strategy_component": 0.49250000000000005,
            "strategy_details": {
                "raw": 0.9850000000000001,
                "normalized": 0.9850000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 267,
        "original_data": {
            "instruction": "What's the differences of the Wing Chun kongfu and the traditional Shaolin Kongfu?",
            "input": "",
            "output": "Wing chun was invented by a Buddhist nun who watched a snake and crane fighting. She adopted principles from both styles. She equated that to yin vs yang in terms of fighting style. \nIt's all about economy of movement, small quick strikes and close quarters combat. \nIt's meant to be effective for people of all ages and sizes and does not emphasize force. The crane stood there motionless and waded off the snakes quick strikes. It wasn't phased by the attacks. The posture and mindset of the crane and the quick strikes of the snake. \n\nShaolin was developed to ward off invaders. Usually enemies riding horses so they do a lot of high jumping kicks which were used to kick people off horses. \nIt's usually associated with tiger or dragon styles which can operate at more distance and use power. It's also very flashy and usedin demonstrations and many king fu movies .\nIt's actually counterintuitive to many martial arts where you strike hard or from a distance and move around a lot ducking blocking etc. Wing chun is just small subtle blocks to divert attacks off center and then you strike back concurrently",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.6,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.4,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.476842880249023,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.055,
            "strategy_details": {
                "raw": 2.11,
                "normalized": 2.11,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.2999999999999998,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.4,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 268,
        "original_data": {
            "instruction": "Where is Chiria",
            "input": "",
            "output": "Chiria is a census town in Pashchimi Singhbhum district in the state of Jharkhand, India. India's largest iron ore mine with reserves of 2 billion tonnes of iron ore is located here. The mine is operated by Steel Authority of India Limited.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.0323033332824707,
            "normalized": 0.723515238080706
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 269,
        "original_data": {
            "instruction": "What is C#?",
            "input": "",
            "output": "C# (pronounced C sharp)is a general-purpose high-level programming language supporting multiple paradigms. C# encompasses static typing, strong typing, lexically scoped, imperative, declarative, functional, generic, object-oriented (class-based), and component-oriented programming disciplines.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.1693694591522217,
            "normalized": 0.7626769883292062
        },
        "potential_entropy": {
            "strategy_component": 1.2025,
            "strategy_details": {
                "raw": 2.405,
                "normalized": 2.405,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 270,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Shime-daiko, Chilador",
            "input": "",
            "output": "Childador is string, Shime-daiko is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 8.09914493560791,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 271,
        "original_data": {
            "instruction": "Who built Sigiriya fortress in Sri Lanka?",
            "input": "It is likely that the area around Sigiriya may have been inhabited since prehistoric times. There is clear evidence that the many rock shelters and caves in the vicinity were occupied by Buddhist monks and ascetics from as early as the 3rd century BC. The earliest evidence of human habitation at Sigiriya is the Aligala rock shelter to the east of Sigiriya rock, indicating that the area was occupied nearly five thousand years ago during the Mesolithic Period.\n\nBuddhist monastic settlements were established during the 3rd century BC in the western and northern slopes of the boulder-strewn hills surrounding the Sigiriya rock. Several rock shelters or caves were created during this period. These shelters were made under large boulders, with carved drip ledges around the cave mouths. Rock inscriptions are carved near the drip ledges on many of the shelters, recording the donation of the shelters to the Buddhist monastic order as residences. These were made in the period between the 3rd century BCE and the 1st century AD.\n\nIn 477 AD, Kashyapa I, the king’s son by a non-royal consort, seized the throne from King Dhatusena, following a coup assisted by Migara, the King’s nephew and army commander. The rightful heir, Moggallana, fearing for his life, fled to South India. Afraid of an attack by Moggallana, Kashyapa moved the capital and his residence from the traditional capital of Anuradhapura to the more secure Sigiriya. During King Kashyapa’s reign (477 to 495 AD), Sigiriya was developed into a complex city and fortress. Most of the elaborate constructions on the rock summit and around it, including defensive structures, palaces, and gardens, date from this period.",
            "output": "King Kashyapa developed Sigiriya city and fortress.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 6.965728759765625,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.49249999999999994,
            "strategy_details": {
                "raw": 0.9849999999999999,
                "normalized": 0.9849999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 272,
        "original_data": {
            "instruction": "What are a few things to do when visiting Northern Vietnam?",
            "input": "",
            "output": "Here are some to do when visiting Northern Vietnam: \n1. Go hiking through the picturesque rice terraces in Sa Pa. This is a great way to experience the beauty and local culture of Northern Vietnam.\n2. Take a cruise on Ha Long Bay. This Unesco world heritage site is a great place to relax and take in the views as you sail through thousands of limestone islands.\n3. Visit Hanoi and go on a food tour. This is one of the best ways to experience the local culture of Vietnam through authentic Vietnamese cuisine. \n4. Spend a day exploring the stunning landscapes of Ninh Binh. Home to the ancient capital of Vietnam and the Unesco World Heritage site of Trang An, Ninh Binh is a popular destination in Northern Vietnam.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.5139029026031494,
            "normalized": 0.5754008293151855
        },
        "potential_entropy": {
            "strategy_component": 0.8899999999999999,
            "strategy_details": {
                "raw": 1.7799999999999998,
                "normalized": 1.7799999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 273,
        "original_data": {
            "instruction": "Think of a lot of different ways you could tell your roommate about something you need to buy at the store.",
            "input": "",
            "output": "You could send your roommate a text, leave them a post-it note, add it to the grocery list, tell your roommate in person or give them a phone call",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9001829624176025,
            "normalized": 0.9714808464050293
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 274,
        "original_data": {
            "instruction": "What countries have won the ODI Cricket World Cup?",
            "input": "",
            "output": "Australia, England, India, Sri Lanka, Pakistan",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.637600898742676,
            "normalized": 0.8964573996407645
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 275,
        "original_data": {
            "instruction": "What is a Cheque card?",
            "input": "",
            "output": "This type of card is issued by a bank to a customer. It guarantees that a cheque used with the card will be paid if the person issuing the cheque has kept to all the conditions.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.3,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.7,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.5699400901794434,
            "normalized": 0.8771257400512695
        },
        "potential_entropy": {
            "strategy_component": 1.2550000000000001,
            "strategy_details": {
                "raw": 2.5100000000000002,
                "normalized": 2.5100000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.57
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.7,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 276,
        "original_data": {
            "instruction": "What are the best places to eat and top Japanese sushi  restaurant in San Mateo?",
            "input": "",
            "output": "The best Japanese Restaurant in San Mateo are Sushi Sam and Sushi Maruyama",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.4,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.6,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.840579509735107,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.855,
            "strategy_details": {
                "raw": 1.71,
                "normalized": 1.71,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.6,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 277,
        "original_data": {
            "instruction": "What should I not do if I see a bear?",
            "input": "",
            "output": "Everyone knows that bears are not fluffy friends, and are unfortunately likely to maul humans. There is clear guidance on what to do depending on what type of bear you see: if it’s a black bear, make yourself loud and fight if necessary. If it’s a brown bear, lay down and cover your head. If it’s a polar bear, hope you updated your will recently. However, there is very little guidance on what not to do if you see a bear. Thankfully it is easy to provide guidance that applies to all bear types. First, do not attempt to hug the bear. While they do look fluffy and very much in need of a hug, bears highly value consent and will attempt to eat you if you ignore that. Next, do not invite a bear over for dinner. While some bears, black bears especially, may look like good dinner guests they are known for extremely rude eating habits and will likely dirty your house. Further, some types of bears might think that you are the menu. It is also not a good idea to challenge a bear to a game of basketball. Most bears are actually extremely adept at basketball, and this would likely hurt your ego considerably as they would dunk on you and then eat you as a celebration for winning. While this is not an exhaustive list of what not to do if you see a bear, it covers the most common questions.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.8,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.220583200454712,
            "normalized": 0.7773094858442035
        },
        "potential_entropy": {
            "strategy_component": 1.1800000000000002,
            "strategy_details": {
                "raw": 2.3600000000000003,
                "normalized": 2.3600000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.42
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 278,
        "original_data": {
            "instruction": "In the following initial summary of the plot of the series My Hero Academia, what is the secret that Izuku must keep and from whom?",
            "input": "Izuku Midoriya is a young man who dreams of becoming a Hero despite being bullied by his violent childhood friend Katsuki Bakugo for lacking a Quirk. After having a few encounters with his idol, All Might, who is Japan's greatest hero, Izuku is chosen by All Might to inherit his Quirk \"One For All\" (which gives him the ability to endlessly stockpile his raw power) and become his successor, as All Might was severely injured by his arch-nemesis All For One (whose Quirk allows him to steal other Quirks for his use and pass them over to others at his will) and does not have much time left to live. Izuku begins attending the U.A. High School for heroes in training alongside Bakugo and his friends from Class 1-A while keeping his connection with All Might a secret. During the course of the school year, Izuku and his friends work hard to improve themselves and have a few encounters with the League of Villains led by All For One's apprentice Tomura Shigaraki, who desires to kill All Might as part of their plan to take over the world. During one of these encounters, All Might and All For One have one last fight, which ends with All For One defeated and imprisoned, and All Might, having exhausted the last of One For All's power in himself, forced to retire.",
            "output": "Izuku keeps his secret that All Might had chosen Izuku to inherit All Might's Quirk \"One For All\" from Bakugo and Izuku's friends from Class 1-A.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.15377950668335,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 279,
        "original_data": {
            "instruction": "Can you summarize for me in no more than three paragraphs the book, \"The Little Engine That Could\", by Watty Piper?",
            "input": "The Little Engine That Could is an American folktale (existing in the form of several illustrated children's books and films) that became widely known in the United States after publication in 1930 by Platt & Munk. The story is used to teach children the value of optimism and hard work. Based on a 2007 online poll, the National Education Association listed the book as one of its \"Teachers' Top 100 Books for Children\"",
            "output": "The Little Engine That Could is a children's book written in 1930 by Watty Piper.  The main purpose of the book is to help bring out in children a positive, \"can-do attitude.\"  The key phrase in the book is, \"I think I can.\" \n\nThe main character is the \"Little Engine\".  The Little Engine is an undersized locomotive that is called upon to pull rail cars full of good things for the boys and girls that live on the other side of the mountain.  Several other locomotives, appearing newer, stronger or more mature than the Little Engine all make excuses for why they will not help take the train cars over the mountain.\n\nThe Little Engine gladly takes on the task when called upon.  When starting to pull the heavy rail cars, The Little Engine strains with all its might and begins to say, \"I think I can, I think I can, I think I can.\"  When climbing the mountain, it doesn't look like the Little Engine has the power to get over the peak.  The Little Engine puffs, \"I think I can, I think I can, I think I can.\"  Finally The Little Engine reaches the top, comes over the crest and on the way down says, \"I though I could, I thought I could, I thought I could.\"",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.627040147781372,
            "normalized": 0.6077257565089634
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 280,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Taiko, Vlier",
            "input": "",
            "output": "Vlier is string, Taiko is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.96175479888916,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 281,
        "original_data": {
            "instruction": "Based on the text below, please summarise in a few lines",
            "input": "Cricket is a bat-and-ball game played between two teams of eleven players on a field at the centre of which is a 22-yard (20-metre) pitch with a wicket at each end, each comprising two bails balanced on three stumps. The batting side scores runs by striking the ball bowled at one of the wickets with the bat and then running between the wickets, while the bowling and fielding side tries to prevent this (by preventing the ball from leaving the field, and getting the ball to either wicket) and dismiss each batter (so they are \"out\"). Means of dismissal include being bowled, when the ball hits the stumps and dislodges the bails, and by the fielding side either catching the ball after it is hit by the bat, but before it hits the ground, or hitting a wicket with the ball before a batter can cross the crease in front of the wicket. When ten batters have been dismissed, the innings ends and the teams swap roles. The game is adjudicated by two umpires, aided by a third umpire and match referee in international matches. They communicate with two off-field scorers who record the match's statistical information.\n\nForms of cricket range from Twenty20, with each team batting for a single innings of 20 overs (each \"over\" being a set of 6 fair opportunities for the batting team to score) and the game generally lasting three hours, to Test matches played over five days. Traditionally cricketers play in all-white kit, but in limited overs cricket they wear club or team colours. In addition to the basic kit, some players wear protective gear to prevent injury caused by the ball, which is a hard, solid spheroid made of compressed leather with a slightly raised sewn seam enclosing a cork core layered with tightly wound string.",
            "output": "Cricket is a bat-and-ball game played between two teams with eleven players each. The game is played in a field with a pitch at the center with wickets at either end. There is a batting team and a bowling team. The game is adjudicated by two on-field umpires, a third umpire, and a referee. There are various forms of the game, starting from Twenty20, which generally lasts up to 3 hours to test cricket, played over 5 days.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.877223491668701,
            "normalized": 0.6792067119053432
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 282,
        "original_data": {
            "instruction": "Make me a list of all the different ways I could tell my sister about a recent vacation",
            "input": "",
            "output": "You could tell her about your vacation by writing a letter, mailing a postcard, sending an email, giving her a call, sending a text or visiting her house",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.85,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.206643581390381,
            "normalized": 0.7733267375401088
        },
        "potential_entropy": {
            "strategy_component": 0.75,
            "strategy_details": {
                "raw": 1.5,
                "normalized": 1.5,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 283,
        "original_data": {
            "instruction": "Which of the following movies is not from the Harry Potter Franchise? \n-  Order of the Phoenix.\n- Goblet of Fire.\n- Sorcerer's Stone.\n- Chamber of Secrets.\n- The Fellowship of the rings.",
            "input": "",
            "output": "\"The Fellowship of the rings\" is not part of the Harry Potter Franchise, it is in-fact a part of the Lord of the Rings Franchise. The canonical Harry Potter movies in chronological order are:\n- Harry Potter and the Philosopher's Stone.\n- Harry Potter and the Chamber of Secrets.\n- Harry Potter and the Prisoner of Azkaban.\n- Harry Potter and the Goblet of Fire.\n- Harry Potter and the Order of the Phoenix.\n- Harry Potter and the Half-Blood Prince.\n- Harry Potter and the Deathly Hallows – Part 1.\n- Harry Potter and the Deathly Hallows – Part 2.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 1.2610396146774292,
            "normalized": 0.21743988990783691
        },
        "potential_entropy": {
            "strategy_component": 0.45500000000000007,
            "strategy_details": {
                "raw": 0.9100000000000001,
                "normalized": 0.9100000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.7000000000000001,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 284,
        "original_data": {
            "instruction": "What is the ranking of poker hands from strongest to weakest that you can make with a regular deck of 52 cards and five cards total?",
            "input": "",
            "output": "The ranking of poker hands with 52 regular playing cards is:\n\n1) A Royal Flush - this hand consists of ten, jack, queen, king and ace of a single suit (e.g. ten, jack, queen, king, ace, all of spades)\n2) A Straight Flush - this hand consists of five consecutive cards in a single suit. The ace counts as either the highest or lowest, but not both at the same time. (e.g. five, six, seven, eight, nine, all of hearts)\n3) Four-of-a-kind - this hand contains four of the same card (e.g. four tens)\n4) Full house - Three-of-a-kind of the same card, and a pair of a different card. (e.g. three sevens and two jacks)\n5) Flush - Five cards of the same suits (e.g. five spades)\n6) Straight - Five consecutive cards, where the ace can count as either the highest or lowest card, but not both at the same time (e.g. ace, two, three, four, five). \n7) Three-of-a-kind - this hand contains three of the same card (e.g. three sevens)\n8) Two pair - this hand contains two sets of two cards that are the same (e.g. two fives and two queens)\n9) One pair - this hand contains one set of two cards that are the same (e.g. two tens)\n10) High card - this hand contains none of the other options, and just has a high card.\n\nFor all poker hands, if they are of the same ranking, then one has to look at the height of the cards themselves to determine the strongest hand. A full house of three aces and two tens is stronger than a full house of three kings and two tens. At the same time, three kings and two tens beats three tens and two aces, because the kings are stronger.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 1.5010218620300293,
            "normalized": 0.2860062462942941
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 285,
        "original_data": {
            "instruction": "From the text, give me a short list of people the musician Bruce Gilbert collaborated with.",
            "input": "Bruce Clifford Gilbert (born 18 May 1946) is an English musician. One of the founding members of the influential and experimental art punk band Wire, he branched out into electronic music, performance art, music production, and DJing during the band's extended periods of inactivity. He left Wire in 2004, and has since been focusing on solo work and collaborations with visual artists and fellow experimental musicians.\n\nWire released three albums between 1976 and 1979, Pink Flag, Chairs Missing and 154, before temporarily disbanding after a show at London's Electric Ballroom at the start of 1980. At this time, Gilbert formed a series of bands/projects with Wire's bassist, Graham Lewis, including Cupol, Dome, P'o, and Duet Emmo. Gilbert's collaborations with Lewis were experimental, featuring ambient music and found sounds. Dome performed at art galleries with visual displays that allowed audience interactivity. Gilbert and Lewis performed with tubes made of paper over their heads, thus restricting their vision. Artist Russell Mills frequently collaborated with Dome. In 1980, Gilbert and Lewis produced The The's debut single \"Black & White/Controversial Subject\" for 4AD, as well as the single \"Drop/So\" by A.C. Marias for their own Dome label. Between 8 and 31 August 1981, Gilbert, Lewis, and Mills took over London's Waterloo Gallery and produced MZUI, an interactive audio-visual installation where visitors were encouraged to play a number of instruments created by the artists from objects found on the site. The MZUI album, released by Cherry Red in May 1982, contains two untitled pieces based on recordings from the venue, finishing with the looped and distorted voice of Marcel Duchamp, whom Gilbert considers a key influence. Gilbert's experimental piece \"Children\", released in 1983 by Touch, features his parents talking about significant events from their childhood.",
            "output": "Bruce Gilbert collaborated with a number of people, including Wire's bassist Graham Lewis, Marcel Duchamp, and Russel Mills. Bruce Gilbert also collaborated with other members of the band Wire.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.28212308883667,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 286,
        "original_data": {
            "instruction": "give me a bulleted list of ways to get to the other side of a lake.",
            "input": "",
            "output": "- walk across the suspension bridge hovering over the lake \n- swim across the lake\n- if you are a scorpion, hitch a ride on a frogs back and behave\n- canoe across the lake\n- drive around the lake and reach the other side\n- zip line across the lake\n- skate across the lake in the winter when the lake is frozen\n- water ski across the lake\n- lay on a float and hope you end up on the other side of the lake\n- drain the lake and walk across on the lake bed\n- take a ferry across the lake\n- scuba dive across the lake",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.6653711795806885,
            "normalized": 0.6186774798801967
        },
        "potential_entropy": {
            "strategy_component": 1.1800000000000002,
            "strategy_details": {
                "raw": 2.3600000000000003,
                "normalized": 2.3600000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.42
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 287,
        "original_data": {
            "instruction": "From the passage provided, extract the two forms of energy transfer and list them in bullets.",
            "input": "The First Law of thermodynamics is a formulation of the law of conservation of energy, adapted for thermodynamic processes. A simple formulation is: \"The total energy in a system remains constant, although it may be converted from one form to another.\" Another common phrasing is that \"energy can neither be created nor destroyed\" (in a \"closed system\"). While there are many subtleties and implications that may be more precisely captured in more complex formulations, this is the essential principle of the First Law.\n\nIt distinguishes in principle two forms of energy transfer: heat, and thermodynamic work, for a system of a constant amount of matter. The law also defines the internal energy of a system, an extensive property for taking account of the balance of energies in the system.",
            "output": "The two forms of energy transfer are:\n- Heat\n- Thermodynamic work",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.88,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.12,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.6955320835113525,
            "normalized": 0.9130091667175293
        },
        "potential_entropy": {
            "strategy_component": 0.3205,
            "strategy_details": {
                "raw": 0.641,
                "normalized": 0.641,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.23099999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.12,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 288,
        "original_data": {
            "instruction": "Why is film photography popular again?",
            "input": "",
            "output": "Film photography had seen a long decline in popularity ever since the invention of digital cameras with their remarkable leaps in technological advances. As of today, a decent professional digital camera significantly outperforms the 35mm film cameras in terms of resolution, burst-shooting capabilities, nearly unlimited storage, and etc... In the recent years, the \"film look\" has made its comeback with lower resolution, grainy texture and unique color rendition. On top of that, half-decent retro-looking film cameras can be acquired for much less than a potent digital camera, lowering the entry barrier for those on the budget and looking for that look. An abundant pool of film stocks also adds to the fun, where each can drastically differ in low-light sensitivity, color rendition, graininess, post-processing chemistry, and of course, price. However, many major photographic film producers have either discontinued or significantly reduced the volume of their production. This, coupled with the surge in the demand, has led to skyrocketing prices on most of the still-available film stocks. On top of this, many popular and sought-after film cameras like the Leica M6, Nikon FM2 and Contax T2 also saw steep increase in their used-market prices. Given these price hikes on films and decades-old equipments, film photography is quickly becoming an expensive hobby, where each click of a shutter could cost more than 50 cents. How ironic is this today, when digital storage is virtually free? The authentic \"film look\" carries a hefty price tag today, and the future popularity of film photograhy is uncertain.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.420631170272827,
            "normalized": 0.8344660486493792
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 289,
        "original_data": {
            "instruction": "What caused the millenium tower to sink in san francisco?",
            "input": "",
            "output": "The critics blame the city for allowing Millennium partners to anchor the building into packed sand rather than bedrock. The problem with Millennium tower arose because the 60 to 90-foot-long friction piles supporting it were driven into sandy soil rather than the bedrock, 200 feet down.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.375916004180908,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 290,
        "original_data": {
            "instruction": "How do you push start a car with a manual transmission?",
            "input": "",
            "output": "1. Get the car to the top of a stretch of clear downhill road, or get people who will push you there.\n\n2. Fully depress the clutch and put the car in second gear (first gear if you’ll be rolling slowly; choose the gear that you would normally use at that speed, and faster is better).\n\n3. Turn the ignition to on.\n\n4. Leaving the clutch fully depressed, take your foot off the brake and start rolling down the hill (or get people to start pushing you).\n\n5. Coast down the hill until you reach 5+ miles per hour (faster is better).\n\n6. Release the clutch quickly while giving some gas; the engine should turn and start. You may need to feather the clutch/gas to keep the engine running at first.\n\n7. If it doesn’t start the first time, depress the clutch and release it again.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.222076654434204,
            "normalized": 0.7777361869812012
        },
        "potential_entropy": {
            "strategy_component": 1.2025000000000001,
            "strategy_details": {
                "raw": 2.4050000000000002,
                "normalized": 2.4050000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.4649999999999999
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 291,
        "original_data": {
            "instruction": "Why do leaves on houseplants turn yellow and brown?",
            "input": "",
            "output": "Often houseplant leaves turn yellow and brown for a few common reasons. First is overwatering. Houseplants are kept in containers that can retain water which can lead to root rot if not watered carefully. Some houseplants prefer the soil to dry out between waterings and many home gardeners end up watering before the soil has dried up sufficiently. Too much water in a container can lead to root rot which first can show symptoms in browning, dropping, and yellowing of leaves. Another reason for leaf discoloration is under watering as some plants prefer the soil to have a level of moisture at all times. Other reasons include too little sunlight which limits the plants ability to photosynthesize. Also, even too much sunlight can cause burned leaves.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.768632650375366,
            "normalized": 0.6481807572501046
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 292,
        "original_data": {
            "instruction": "How do I cook an egg over easy?",
            "input": "",
            "output": "To cook an egg over easy you will need a pan, stove, oil (or butter), an egg, and a spatula. Begin by placing the pan on the stove with a minimal amount of oil or butter. Heat the pan on the stove until it is hot. What’s the pan is hot crack the egg in the middle of the pan and let it cook until the egg white becomes solid. At which point flip the egg and let it cook until the desired consistency of the yoke. Finally remove from the pan and enjoy!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.8293001651763916,
            "normalized": 0.6655143329075405
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 293,
        "original_data": {
            "instruction": "Football or soccer?",
            "input": "",
            "output": "Most of the world prefers to use the term \"football\". Exceptions include the United States of America, and to some extent, Australia, New Zealand, and even Ireland. These countries all play other sports called football, such as Gaelic football and American football, and so they use the term \"soccer\" to distinguish it from their other sport.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.8836746215820312,
            "normalized": 0.6810498918805804
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 294,
        "original_data": {
            "instruction": "What did Bobby Moore die from?",
            "input": "Moore's first cancer was in 1964, two years before England's first World Cup win — a diagnosis of testicular cancer, treated by orchidectomy of one; it had not spread. In April 1991, Moore underwent an operation for suspected colorectal cancer. At the time it was reported as an \"emergency stomach operation\".\n\nOn 14 February 1993, he announced he was suffering from colorectal cancer and liver cancer; by this time it had spread. Three days later he commentated on an England match against San Marino at Wembley, alongside his friend Jonathan Pearce. Moore attended a dinner after the match and made a presentation. That was to be his final public act; seven days later on 24 February, at 6:36 am, he died at the age of 51.",
            "output": "Booby Moore died aged 51 from colorectal and liver cancer.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.95,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 5.101058483123779,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.44749999999999995,
            "strategy_details": {
                "raw": 0.8949999999999999,
                "normalized": 0.8949999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 295,
        "original_data": {
            "instruction": "classify each of the following as either a city or a county: usa, mexico, cananda, syracuse, austin, mumbai, japan, tokyo",
            "input": "",
            "output": "Of the list your presented, following are countries: usa, mexico, canada and japan. The following are cities: syracuse, austin, mumbai, tokyo.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.768106460571289,
            "normalized": 0.9337447030203683
        },
        "potential_entropy": {
            "strategy_component": 0.9124999999999999,
            "strategy_details": {
                "raw": 1.8249999999999997,
                "normalized": 1.8249999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 296,
        "original_data": {
            "instruction": "Provide an example of using modern materials science to study archaeological artifacts",
            "input": "Ursula Martius Franklin was a German-Canadian metallurgist, research physicist, author, and educator who taught at the University of Toronto for more than 40 years. Franklin was a pioneer in the field of archaeometry, which applies modern materials analysis to archaeology. She worked for example, on the dating of prehistoric bronze, copper and ceramic artifacts. One small example of her work in this field regards what was a standing question on the nature of shiny black Chinese mirrors found in high quantities in ancient tombs. Franklin's use of microscopic etching, electron microprobe and x-ray fluorescence analyses produced evidence that what was thought by some to be a corrosive effect was in fact present in these ancient mirrors (and weapons) at their inception, in dark iron oxides intentionally added near the objects' surfaces. Franklin additionally pulled from historic and literary accounts of black mirrors in Chinese literature to support these findings. Franklin's expertise was also instrumental to dating glass; she guided a study on the remains of blue glass beads in North America remaining from early trade relationships between American Indian tribes and Europe",
            "output": "Archaeometry employs modern technology (such as x-ray fluorescence) to study the origins and environments of historical artifacts. Ursula Franklin was well known in this field; she found evidence of global trade in the remains of glass beads and proved that black mirrors present in large quantities in ancient Chinese tombs were purposely crafted, and not the result of corrosion.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.111508369445801,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 297,
        "original_data": {
            "instruction": "What is the National Payments Corporation of India?",
            "input": "",
            "output": "The National Payments Corporation of India is an umbrella organization for operating retail payments and settlement systems in India, is an initiative of the Reserve Bank of India (RBI) and Indian Banks’ Association (IBA) under the provisions of the Payment and Settlement Systems Act, 2007, for creating a robust Payment & Settlement Infrastructure in India. It was created by RBI for operating retail payments and settlement systems in India.\n\nFounded in December 2008, the NPCI is a not-for-profit organisation registered under Section 8 of the Companies Act 2013, established by the Reserve Bank of India and Indian Banks' Association. The organisation is owned by a consortium of major banks, and has been promoted by the country's central bank, the Reserve Bank of India. The NPCI was incorporated in December 2008 and the Certificate of Commencement of Business was issued in April 2009. The authorised capital has been pegged at ₹3 billion (US$38 million) and paid-up capital is ₹1 billion (US$13 million).",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 2.0365946292877197,
            "normalized": 0.43902703693934847
        },
        "potential_entropy": {
            "strategy_component": 1.18,
            "strategy_details": {
                "raw": 2.36,
                "normalized": 2.36,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 298,
        "original_data": {
            "instruction": "What are some movies starring Chevy Chase",
            "input": "",
            "output": "Fletch, Caddyshack, Three Amigos, National Lampoon's Vacation, Funny Farm, Spies Like Us, Foul Play, European Vacation",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.8134541511535645,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.105,
            "strategy_details": {
                "raw": 2.21,
                "normalized": 2.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 299,
        "original_data": {
            "instruction": "Give me a list of the 3 best activities to do with your kids when the weather is bad outside",
            "input": "",
            "output": "Build a fort in the living room with blankets, pillows and chairs. Watch nostalgic movies like The Goonies, Sandlot and Honey, I Shrunk the kids. Build a lego castle.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.008194446563721,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7775,
            "strategy_details": {
                "raw": 1.555,
                "normalized": 1.555,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 300,
        "original_data": {
            "instruction": "List five titles by Kevin Kwan.",
            "input": "",
            "output": "Sex and Vanity, Crazy Rich Asians, China Rich Girlfriend, Rich People Problems, Luck: The Essential Guide",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 6.111686706542969,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 301,
        "original_data": {
            "instruction": "Why do you love Biriyani?",
            "input": "",
            "output": "Because it's flavorful",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.2,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.15,
            "s_out_div": 0.1,
            "s_out_dens": 0.1,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.9,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 9.898392677307129,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.9449999999999998,
            "strategy_details": {
                "raw": 3.8899999999999997,
                "normalized": 3.8899999999999997,
                "components": {
                    "instruction": 0.96,
                    "input": 1.85,
                    "output": 1.0799999999999998
                },
                "gaps": {
                    "ins_tone": 0.8,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.85,
                    "out_div": 0.9,
                    "out_dens": 0.9,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 302,
        "original_data": {
            "instruction": "How do you grill a steak?",
            "input": "",
            "output": "To grill a great steak, it all starts with the seasoning.  A simple yet timeless seasoning includes a light coat with olive oil and salt and pepper to taste on all sides.   While seasoning, the steak should be fully and evenly covered.   From there, make sure your grill is hot (about 450* or higher) and grill for 4 minutes per side.  Make sure to turn the steak 90* after your second turn.  From there, remove once the steak hits 125* internal and let rest for 10 minutes.  Serve and enjoy!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 3.04752779006958,
            "normalized": 0.7278650828770229
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 303,
        "original_data": {
            "instruction": "How do you make a good cappuccino?",
            "input": "",
            "output": "There are multiple steps involved in making a good cappuccino:\n\nStep 1: Find fresh coffee beans and grind them finely\n\nFor optimal taste, the coffee beans should be fresh and only slightly oxidized. It is important to use a high quality grinder that grinds the beans uniformly and finely. You should fill a portafilter with 18-21g of grounds and tamp the grounds.\n\nStep 2: Pull the shot\n\nTurn on the espresso machine, wait for it to heat up, and press the button to start the shot. The goal is to make ~40g of espresso in 25-30 seconds starting from when you initiated the pull.\n\nStep 3: Steam the milk and pour\n\nPour milk into a carafe and use the steam wand to steam the milk. You want to aerate the milk, but not too much or too little. You want to avoid making macrobubbles and also heat up the milk so that the carafe is hot to the touch. Now pour the milk into the espresso, and you're done!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.596991777420044,
            "normalized": 0.5991405078342983
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 304,
        "original_data": {
            "instruction": "Write a story about an ice fishing trip in Minnesota. Make sure to include what you need for a successful trip.",
            "input": "",
            "output": "Ah, winter! It’s time to put on your long underwear. Slip in some wool socks and a sweater. Climb into your snowpants. Pull down your stocking cap. Grab your fi shing rod and bait bucket.\nThere’s ice on the lake, and it’s the season to enjoy a true Minnesota adventure, —ice fishing. Don’t worry about getting cold or bored on a frozen lake. Ice fishing is both easy and exciting. It’s fun to hike across the ice imagining hungry sunnies or walleyes lurking below. It’s an adventure to hang out around an ice hole with friends and family, telling stories and holding a funny looking fishing rod as you wait for a bite. And it’s thrilling when your bobber suddenly  vanishes down the hole, and you pull a slippery fish from the water with a splash. So grab a grown-up, a Thermos of hot cocoa, and get ready for an ice fishing adventure.\n\nStart with a visit to your local bait store or DNR Fisheries office. Folks there can tell you in which lakes the fish are biting and where you can get onto the lake. They can also tell you where the ice is most likely to be OK. Wind, warm weather, underwater springs, and currents can all make ice unsafe. Ice must be at least 4 inches thick before you walk on it. (See Be Safe, page 45.) Once you know the ice is thick enough, you can go find a fishing spot. Here are three ways to look: § If you know where fish hang out in summer, go there. Fish usually go to the same places in winter. § Pick up a lake map at the bait shop or DNR and look for shallow areas or drop-offs (where the bottom gets deep quickly). Fish are more likely to be there. § Look for anglers congregated at the best fishing holes. Ask if you can fish near them. (It’s not polite to drill holes too close to other anglers.) If the fish aren’t biting in one spot, try another.\n\nYou can use a regular reel, or some anglers use a device called a tip-up instead. A tipup has two sticks. One lies across the hole. The other points down into the hole and has a spool with line. When a fish takes your bait, a flag springs up from the stick across the hole. Then you pull the fish up with the line. Tip-ups are fun because you can watch them while reading a book or tossing a Frisbee.\n\n1.\tCut a Hole.\na.\tTo catch fish through the ice, you must first drill a hole. To drill a hole, ice anglers use a tool called an auger, which looks like a giant screw with a sharp blade on the end. Another handy tool is a spud, a long-handled chisel with a sharp blade for checking ice thickness and chipping extra ice from the hole. Anglers use a scoop, a big spoon with holes, to clean out ice shavings.\n\n2.\t Find the Depth\na.\tIf you know the depth of the water, you have a better idea what fish to fish for. Bluegills and northerns like shallow water. Look for walleyes in deeper water. Some anglers use an electronic fish locator or a lead weight on a hook. You can also tie a string to a weight, hold the loose end in one hand, and drop the weigh to the bottom. Then measure the length of string that’s down the hole. The simplest solution is to use a weighted hook, which drops to the bottom. Then you reel it up to fish as far off the bottom as you like. \n\n3.\tLand your Catch\na.\tWhen you feel a fish on the line, reel it up steadily but not too fast because you might yank out the hook. When you see the fish, ask your adult fishing buddy to lift it out. After landing a fish, remove it from the hook. It’s easier to get a hook out with pliers than with your hands in cold weather. If the fish is too big or too small to keep, return it to the water. Or put your catch in a bucket or a snowbank. Watch out for dogs! A dog might try to run away with your catch! It happens. \n\n4.\tUse Good Manners\na.\tBe polite and don’t disturb your fellow anglers with loud talk or goofing around. Always pick up your trash and anything else you brought. Remember: Anything you leave on the lake will go into the water when the ice melts.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.88,
            "s_out_cot": 0.92,
            "s_out_div": 0.87,
            "s_out_dens": 0.89,
            "s_out_bg": 0.91
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.12,
            "s_out_cot": 0.07999999999999996,
            "s_out_div": 0.13,
            "s_out_dens": 0.10999999999999999,
            "s_out_bg": 0.08999999999999997
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 3.1889352798461914,
            "normalized": 0.7682672228131976
        },
        "potential_entropy": {
            "strategy_component": 0.2615,
            "strategy_details": {
                "raw": 0.523,
                "normalized": 0.523,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.21999999999999997,
                    "output": 0.12299999999999997
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.12,
                    "out_cot": 0.07999999999999996,
                    "out_div": 0.13,
                    "out_dens": 0.10999999999999999,
                    "out_bg": 0.08999999999999997
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 305,
        "original_data": {
            "instruction": "From the passage note down the valuation of NFT trade for the year 2020 and 2021. List the results in format ‘Year: Value‘.",
            "input": "A non-fungible token (NFT) is a unique digital identifier that cannot be copied, substituted, or subdivided, that is recorded in a blockchain, and that is used to certify ownership and authenticity. The ownership of an NFT is recorded in the blockchain and can be transferred by the owner, allowing NFTs to be sold and traded. NFTs can be created by anybody, and require few or no coding skills to create. NFTs typically contain references to digital files such as photos, videos, and audio. Because NFTs are uniquely identifiable assets, they differ from cryptocurrencies, which are fungible.\n\nProponents of NFTs claim that NFTs provide a public certificate of authenticity or proof of ownership, but the legal rights conveyed by an NFT can be uncertain. The ownership of an NFT as defined by the blockchain has no inherent legal meaning and does not necessarily grant copyright, intellectual property rights, or other legal rights over its associated digital file. An NFT does not restrict the sharing or copying of its associated digital file and does not prevent the creation of NFTs that reference identical files.\n\nThe NFT market grew dramatically from 2020 to 2021: the trading of NFTs in 2021 increased to more than $17 billion, up by 21,000% over 2020's total of $82 million. NFTs have been used as speculative investments and they have drawn increasing criticism for the energy cost and carbon footprint associated with validating blockchain transactions as well as their frequent use in art scams. The NFT market has also been compared to an economic bubble or a Ponzi scheme. In 2022, the NFT market suffered a major collapse, with prices sharply falling; a May 2022 estimate was that the number of sales was down over 90% compared to its 2021 peak.",
            "output": "2020: $82million\n2021: $17billion",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.88,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.12,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8545725345611572,
            "normalized": 0.6727350098746163
        },
        "potential_entropy": {
            "strategy_component": 0.3205,
            "strategy_details": {
                "raw": 0.641,
                "normalized": 0.641,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.23099999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.12,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 306,
        "original_data": {
            "instruction": "Name famous French singers (all time).",
            "input": "",
            "output": "Johnny Hallyday, Michel Sardou, Orelsan, Kendji, Lara Fabian, Jennifer, Charles Aznavour, Edith Piaf.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.0110764503479,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0425,
            "strategy_details": {
                "raw": 2.085,
                "normalized": 2.085,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.65,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 307,
        "original_data": {
            "instruction": "Identify which instrument is string or woodwind: Tenor violin, Bass flute",
            "input": "",
            "output": "Bass flute is woodwind, Tenor violin is string.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.097821235656738,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.9624999999999999,
            "strategy_details": {
                "raw": 1.9249999999999998,
                "normalized": 1.9249999999999998,
                "components": {
                    "instruction": 0.3,
                    "input": 1.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.6,
                    "inp_complex": 0.8,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 308,
        "original_data": {
            "instruction": "What is Augusta National Golf Club?",
            "input": "Augusta National Golf Club was founded by Bobby Jones and Clifford Roberts, the course was designed by Jones and Alister MacKenzie and opened for play in 1932. Since 1934, the club has played host to the annual Masters Tournament, one of the four men's major championships in professional golf, and the only major played each year at the same course. It was the top-ranked course in Golf Digest's 2009 list of America's 100 greatest courses and was the number ten-ranked course based on course architecture on Golfweek Magazine's 2011 list of best classic courses in the United States",
            "output": "Augusta National Golf Club, sometimes referred to as Augusta is a private Golf Club in Augusta Georgia that was opened in 1932.  Augusta is famously known for hosting the Masters Tournament which is one of the four men's major championships in professional golf, and the only major played each year at the same course.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.069981575012207,
            "normalized": 0.7342804500034877
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 309,
        "original_data": {
            "instruction": "What makes the world go around?",
            "input": "",
            "output": "Some say money makes the world go around, others say it is love. It has even been suggested that music is the fuel that makes the world go around. These explanations are however based on human spirituality and needs. As far as other living creatures are concerned, the drive to survive is what makes the world go around. In reality the world physically started going around itself, rotating around its axis, because of the way it was formed.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.6,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.4,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.314455986022949,
            "normalized": 0.8041302817208427
        },
        "potential_entropy": {
            "strategy_component": 1.2699999999999998,
            "strategy_details": {
                "raw": 2.5399999999999996,
                "normalized": 2.5399999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.4,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 310,
        "original_data": {
            "instruction": "Why should you use multi factor authentication to secure your computer accounts?",
            "input": "",
            "output": "A username and password is commonly used to access computer systems or services (eg email). However, should an attacker gain knowledge of your username and password they can very easily impersonate you and compromise your account. \n\nMulti Factor authentication helps protect your account by requiring you to enter a username and password but also provide another factor (such as something you have) to confirm your identity. This might be a physical security token such as a Yubikey or perhaps a biometric identifier such as a fingerprint. You can only access your account when you have both something you know (your password) and something you have (your token).",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.4528541564941406,
            "normalized": 0.5579583304268974
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 311,
        "original_data": {
            "instruction": "From the passage note down the name and population of largest and smallest US state categorized by population. List the results in format ‘name of state: Population count‘.",
            "input": "The United States of America is a federal republic consisting of 50 states, a federal district (Washington, D.C., the capital city of the United States), five major territories, and various minor islands. Both the states and the United States as a whole are each sovereign jurisdictions. The Tenth Amendment to the United States Constitution allows states to exercise all powers of government not delegated to the federal government. Each state has its own constitution and government, and all states and their residents are represented in the federal Congress, a bicameral legislature consisting of the Senate and the House of Representatives. Each state is represented by two senators, while representatives are distributed among the states in proportion to the most recent constitutionally mandated decennial census. Additionally, each state is entitled to select a number of electors to vote in the Electoral College, the body that elects the president of the United States, equal to the total of representatives and senators in Congress from that state. The federal district does not have representatives in the Senate, but has a non-voting delegate in the House, and it is also entitled to electors in the Electoral College. Congress can admit more states, but it cannot create a new state from territory of an existing state or merge of two or more states into one without the consent of all states involved, and each new state is admitted on an equal footing with the existing states.\n\nThe United States has control over fourteen territories. Five of them (American Samoa, Guam, the Northern Mariana Islands, Puerto Rico, and the U.S. Virgin Islands) have a permanent, nonmilitary population, while nine of them (the United States Minor Outlying Islands) do not. With the exception of Navassa Island, Puerto Rico, and the U.S. Virgin Islands, which are located in the Caribbean, all territories are located in the Pacific Ocean. One territory, Palmyra Atoll, is considered to be incorporated, meaning the full body of the Constitution has been applied to it; the other territories are unincorporated, meaning the Constitution does not fully apply to them. Ten territories (the Minor Outlying Islands and American Samoa) are considered to be unorganized, meaning they have not had an organic act enacted by Congress; the four other territories are organized, meaning an organic act has been enacted by Congress. The five inhabited territories each have limited autonomy in addition to having territorial legislatures and governors, but residents cannot vote in federal elections, although all are represented by non-voting delegates in the House.\n\nThe largest state by population is California, with a population of 39,538,223 people, while the smallest is Wyoming, with a population of 576,851 people; the federal district has a larger population (689,545) than both Wyoming and Vermont. The largest state by area is Alaska, encompassing 665,384 square miles (1,723,337 square kilometers), while the smallest is Rhode Island, encompassing 1,545 square miles (4,001 square kilometers). The most recent states to be admitted, Alaska and Hawaii, were admitted in 1959. The largest territory by population is Puerto Rico, with a population of 3,285,874 people (larger than 21 states), while the smallest is the Northern Mariana Islands, with a population of 47,329 people. Puerto Rico is the largest territory by area, encompassing 5,325 square miles (13,791 square kilometers); the smallest territory, Kingman Reef, encompasses only 0.005 square miles (0.01 square kilometers).",
            "output": "Largest state by population - California: 39,538,223\nSmallest state by population - Wyoming: 576,851",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.9145100116729736,
            "normalized": 0.6898600033351353
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 312,
        "original_data": {
            "instruction": "Will immunity increase when you have pets like Dogs/Cats?",
            "input": "",
            "output": "Mostly yes! immunity increases of humans with pets excluding the people with pet dander allergy. Thoughts to ponder for growing up kids who are exposed to pets build a higher level of immunity and resilience. Added to the unconditional LOVE the pet offers.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.5,
            "s_out_div": 0.4,
            "s_out_dens": 0.6,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.5,
            "s_out_div": 0.6,
            "s_out_dens": 0.4,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.199400424957275,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.335,
            "strategy_details": {
                "raw": 2.67,
                "normalized": 2.67,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.65,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.5,
                    "out_div": 0.6,
                    "out_dens": 0.4,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 313,
        "original_data": {
            "instruction": "What percentage of Manewra's population work for the government?",
            "input": "Manewra (Maneora) is a small village in Phalodi tehsil, Jodhpur district, Rajasthan, India. Its population is one thousand.\n\nMost of the people in the village are Bishnoi, subcaste Bhanwal, but in ten houses there are Meghwal, and in one Lohar.\n\nManewra is situated near Champasar (4 km.), Ajasar (5 km.), Kharia (4 km0, choutin/laxmannagar (11 km.) and Chadi 15 km . Its postal pin code is 342312.\n\nIn this village two hundred person are in government service. Most are in the Indian army and some are teachers. Other are farmers, but agriculture depends on the weather, as there are no facilities for irrigation water, so villagers depend on the rainy season.",
            "output": "20 percent. 200 / 1000 = 20%",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.5,
            "s_out_dens": 0.85,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.5,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.3853659629821777,
            "normalized": 0.5386759894234794
        },
        "potential_entropy": {
            "strategy_component": 0.605,
            "strategy_details": {
                "raw": 1.21,
                "normalized": 1.21,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.5,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 314,
        "original_data": {
            "instruction": "Where is the Black Strait Lake",
            "input": "The Garabogazköl (also spelt Kara-Bogaz-Gol; \"Black Strait Lake\"), or Garabogazköl Aylagy (\"Black Strait Lake Bay\"), is a shallow, water-filled, highly-saline depression in the northwestern corner of Turkmenistan. It forms a lagoon of the Caspian Sea and has a variable surface area, about 18,000 km2 (6,900 sq mi). The Caspian Sea proper sits immediately to the west, mostly divorced by a narrow, rocky ridge having a very narrow opening through which the Caspian waters flow, overwhelmingly, into it. There is likely to be a subterranean highly saline flow, only when there is less evaporation in winter, as in the case of the Turkish Straits year-round. The lagoon's volume fluctuates seasonally, accentuated by its salt evaporation ponds and natural counterpart surrounding, seasonally dry salt pans.\n\nThe city of Garabogaz (formerly Bekdaş) lies on the ridge, about 50 km (31 mi) north of the channel between the main Caspian basin and the Garabogazköl lagoon. It has a population of about 10,000 people.",
            "output": "The Garabogazkoel (also spelt Kara-Bogaz-Gol; \"Black Strait Lake\"), or Garabogazkoel Aylagy (\"Black Strait Lake Bay\"), is a shallow, water-filled, highly-saline depression in the northwestern corner of Turkmenistan. \n\nIt forms a lagoon of the Caspian Sea and has a variable surface area, about 18,000 km2 (6,900 sq mi).",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.492156982421875,
            "normalized": 0.8549019949776786
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 315,
        "original_data": {
            "instruction": "What are good foods for keto diet",
            "input": "",
            "output": "Keto is a high fat and low carbohydrates diet. This diet helps the body to get into a state called ketosis where the body uses fat as an energy source instead of carbohydrates.\nThe ideal food items should have high fat content and low or no carbs and sugar. Some meal suggestions are:\n- cauliflower cheese \n- broccoli cheese\n- Avocado chicken salad\n- cauliflower rice and chicken\n- Lamb curry \n- lamb roast with Brussels sprouts \n- beef roast \n- beef steak with halloumi fries\n- salads including cucumber, onion, broccoli \nKeto food should not have potatoes, rice, pasta, wheat, sugar etc.\nDiet and sugar free soda can be good alternatives to curb sugar cravings",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.8,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 3.1746907234191895,
            "normalized": 0.7641973495483398
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.23999999999999996
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 316,
        "original_data": {
            "instruction": "Extract the locations where Nachum lived from the text below and list them in alphabetical order and separated by a semicolon.",
            "input": "Nachum Gutman was born in Teleneşti, Bessarabia Governorate, then a part of the Russian Empire (now in the Republic of Moldova). He was the fourth child of Simha Alter and Rivka Gutman. His father was a Hebrew writer and educator who wrote under the pen name S. Ben Zion. In 1903, the family moved to Odessa, and two years later, to Ottoman Palestine. In 1908, Gutman attended the Herzliya Gymnasium in what would later become Tel Aviv. In 1912, he studied at the Bezalel School in Jerusalem. In 1920–26, he studied art in Vienna, Berlin and Paris.\n\nGutman was married to Dora, with whom he had a son. After Gutman's death in 1980, Dora asked two Tel Aviv gallery owners, Meir Stern of Stern Gallery and Miriam Tawin of Shulamit Gallery, to appraise the value all of the works left in his estate.",
            "output": "Berlin;Jerusalem;Odessa;Palestine;Paris;Tel Aviv;Vienna",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.8961687088012695,
            "normalized": 0.684619631086077
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 317,
        "original_data": {
            "instruction": "How can you open a can without using your hands?",
            "input": "",
            "output": "1) You could try to use both feet to open the can if you have very strong feet.\n2) You could ask a friend to open the can with their hands.\n3) You could use your elbow to push the can off of a very high surface.\n4) You could drive over the can with a car, though you would have to use only your arms to drive.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.727877616882324,
            "normalized": 0.6365364619663784
        },
        "potential_entropy": {
            "strategy_component": 1.1725,
            "strategy_details": {
                "raw": 2.345,
                "normalized": 2.345,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 318,
        "original_data": {
            "instruction": "What's the area of Fuldera village?",
            "input": "Fuldera is a village in the Val Müstair municipality in the district of Inn in the Swiss canton of Graubünden. In 2009 Fuldera merged with Lü, Müstair, Santa Maria Val Müstair, Tschierv and Valchava to form the municipality of Val Müstair.\n\nHistory\nFuldera is first mentioned in 1322 as Faldiera.\n\nGeography\n\nFuldera village\nFuldera had an area, as of 2006, of 13.2 km2 (5.1 sq mi). Of this area, 24.9% is used for agricultural purposes, while 37.2% is forested. Of the rest of the land, 1.4% is settled (buildings or roads) and the remainder (36.4%) is non-productive (rivers, glaciers or mountains).\n\nThe village is located in the Val Müstair sub-district (now Val Müstair municipality) of the Inn district. It is located on the right side of the Rombach. It consists of the larger Fuldera-Dora and the smaller Fuldera-Daint.",
            "output": "The Fuldera village encompasses an area of 13.2 km2 or 5.1 sq mi.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.85,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.570758104324341,
            "normalized": 0.8773594583783831
        },
        "potential_entropy": {
            "strategy_component": 0.5675,
            "strategy_details": {
                "raw": 1.135,
                "normalized": 1.135,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 319,
        "original_data": {
            "instruction": "Write a letter to my landlord, Dave, asking them to hire a plumber to repair a leaking faucet.",
            "input": "",
            "output": "Hi Dave, I'm reaching out about our leaking sink faucet. The sink has been leaking for about a week now, and I've tried tightening the faucet and making sure that the faucet is fully closed, but it still seems to be leaking. It is a pretty constant drip, and is filling a cup up every hour. I think it probably needs to be seen by a plumber who can decide if it is fixable or needs to be replaced. Could you call a plumber and ask them to step by?",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.7849185466766357,
            "normalized": 0.6528338704790387
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 320,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Handbell, Dobro",
            "input": "",
            "output": "Dobro is string, Handbell is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 6.917991638183594,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.705,
            "strategy_details": {
                "raw": 1.41,
                "normalized": 1.41,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 321,
        "original_data": {
            "instruction": "What are the top 10 Hollywood movies liked by audiences",
            "input": "",
            "output": "Gone with the Wind (1939) was voted the favorite film of Americans in a poll of 2,279 adults taken by Harris Interactive in 2008, and again in a follow-up poll of 2,276 adults in 2014.\n\nRoman Holiday (1953) was voted the best foreign (i.e. non-Japanese) film of all time in a 1990 poll of about a million people organized by Japanese public broadcaster NHK.\n\nThe Godfather (1972) was voted number 1 by Entertainment Weekly's readers in 1999 and voted as number 1 in a Time Out readers' poll in 1998. The film was voted the \"Greatest Movie of All Time\" in September 2008 by 10,000 readers of Empire magazine, 150 people from the movie business, and 50 film critics. It also topped Empire's June 2017 poll of 20,000 readers.\nThe Empire Strikes Back (1980) was voted the best film of all time by over 250,000 readers of the Empire film magazine in 2015.\n\nThe Shawshank Redemption (1994) was voted the greatest film of all time by Empire readers in \"The 201 Greatest Movies of All Time\" poll taken in March 2006.\n\nTitanic (1997) was voted the greatest hit of all time in a poll of 6,000 movie fans conducted by English-language newspaper China Daily in March 2008.\n\nShiri (1999) was voted the favorite film of Koreans with 11,918 votes in a 2002 online poll of 54,013 people conducted by Korean movie channel Orion Cinema Network.\n\nThe Lord of the Rings trilogy (2001–2003) was voted the favorite film of Australians in an audience poll for the Australian television special My Favourite Film in 2005. It was also voted the best film in a poll of 120,000 German voters for the TV special Die besten Filme aller Zeiten (\"The best films of all time\") in 2004.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.6,
            "s_out_div": 0.75,
            "s_out_dens": 0.55,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.4,
            "s_out_div": 0.25,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.1817874908447266,
            "normalized": 0.4805107116699219
        },
        "potential_entropy": {
            "strategy_component": 0.625,
            "strategy_details": {
                "raw": 1.25,
                "normalized": 1.25,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.65,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.4,
                    "out_div": 0.25,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 322,
        "original_data": {
            "instruction": "https://en.wikipedia.org/wiki/Narendra_Modi#Public_perception_and_image",
            "input": "A vegetarian and teetotaler, Modi has a frugal lifestyle and is a workaholic and introvert. A person named Badri Meena has been his cook since 2002. Modi used to maintain a close and publicised relationship with his centenarian mother, Hiraben. Modi's 31 August 2012 post on Google Hangouts made him the first Indian politician to interact with citizens on a live chat. Modi has also been called a fashion-icon for his signature crisply ironed, half-sleeved kurta, as well as for a suit with his name embroidered repeatedly in the pinstripes that he wore during a state visit by US President Barack Obama, which drew public and media attention and criticism. Modi's personality has been variously described by scholars and biographers as energetic, arrogant, and charismatic.",
            "output": "Among his followers, Narendra Modi is regarded as a charismatic leader with excellent communication abilities. He has, however, also come under fire for how he has handled intercommunal violence and for pushing a Hindu nationalist agenda. Modi's reputation has been strengthened by the success of his economic policies in Gujarat, and he has sought to build a solid international reputation through travel and social media. Critics and protests have been sparked by his government's execution of contentious measures including demonetization and the Citizenship Amendment Act. Despite this, Modi is still a well-liked figure in Indian politics and has been elected prime minister more than once.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.6,
            "s_out_div": 0.65,
            "s_out_dens": 0.55,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.4,
            "s_out_div": 0.35,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.0604958534240723,
            "normalized": 0.7315702438354492
        },
        "potential_entropy": {
            "strategy_component": 0.6200000000000001,
            "strategy_details": {
                "raw": 1.2400000000000002,
                "normalized": 1.2400000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.4,
                    "out_div": 0.35,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 323,
        "original_data": {
            "instruction": "What is the plot of Kuroko's Basketball?",
            "input": "The Teiko Middle School basketball team dominated basketball teams within Japan, winning the middle school Nationals for three consecutive years. The star players of the team became known as the \"Generation of Miracles\". After graduating from middle school, the five star players went to different high schools with top basketball teams. However, there was a rumor that there was another player in the \"Generation of Miracles\": a phantom sixth man. This mysterious player is now a freshman at Seirin High, a new school with a powerful, if little-known, team. Now, Tetsuya Kuroko – the sixth member of the \"Generation of Miracles\", and Taiga Kagami – a naturally talented player who spent most of his youth in the US, aim to bring Seirin to the top of Japan by taking on Kuroko's former teammates one by one. The series chronicles Seirin's rise to become Japan's number one high school team. The Generation of Miracles include Ryota Kise, Shintaro Midorima, Daiki Aomine, Atsushi Murasakibara, and Seijuro Akashi.",
            "output": "Kuroko's Basketball is an anime about basketball. The story starts with Teiko Middle School where 6 talented players are known as the Generation of Miracles and they went undefeated for 3 years. However, all of the players split off for high school. The protagonist, Tetsuya, is the mysterious sixth man who comes to Seirin high school and meets Taiga Kagami, who is a naturally talented American player. The show focuses on this duo beating the rest of the Generation of Miracles to become the top Japanese high school basketball team.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.45981764793396,
            "normalized": 0.8456621851239886
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 324,
        "original_data": {
            "instruction": "Given these paragraphs about Large language models, what is hallucination?",
            "input": "A large language model (LLM) is a language model consisting of a neural network with many parameters (typically billions of weights or more), trained on large quantities of unlabelled text using self-supervised learning. LLMs emerged around 2018 and perform well at a wide variety of tasks. This has shifted the focus of natural language processing research away from the previous paradigm of training specialized supervised models for specific tasks.\n\nProperties\nThough the term large language model has no formal definition, it often refers to deep learning models having a parameter count on the order of billions or more. LLMs are general purpose models which excel at a wide range of tasks, as opposed to being trained for one specific task (such as sentiment analysis, named entity recognition, or mathematical reasoning). The skill with which they accomplish tasks, and the range of tasks at which they are capable, seems to be a function of the amount of resources (data, parameter-size, computing power) devoted to them, in a way that is not dependent on additional breakthroughs in design.\n\nThough trained on simple tasks along the lines of predicting the next word in a sentence, neural language models with sufficient training and parameter counts are found to capture much of the syntax and semantics of human language. In addition, large language models demonstrate considerable general knowledge about the world, and are able to \"memorize\" a great quantity of facts during training.\n\nHallucinations\nMain article: Hallucination (artificial intelligence)\nIn artificial intelligence in general, and in large language models in particular, a \"hallucination\" is a confident response that does not seem to be justified by the model's training data.\n\nEmergent abilities\n\nOn a number of natural language benchmarks involving tasks such as question answering, models perform no better than random chance until they reach a certain scale (in this case, measured by training computation), at which point their performance sharply increases. These are examples of emergent abilities.\nUnpredictable abilities that have been observed in large language models but that were not present in simpler models (and that were not explicitly designed into the model) are usually called \"emergent abilities\". Researchers note that such abilities \"cannot be predicted simply by extrapolating the performance of smaller models\". These abilities are discovered rather than programmed-in or designed, in some cases only after the LLM has been publicly deployed. Hundreds of emergent abilities have been described. Examples include multi-step arithmetic, taking college-level exams, identifying the intended meaning of a word, chain-of-thought prompting, decoding the International Phonetic Alphabet, unscrambling a word’s letters, identifying offensive content in paragraphs of Hinglish (a combination of Hindi and English), and generating a similar English equivalent of Kiswahili proverbs.\n\nArchitecture and training\nLarge language models have most commonly used the transformer architecture, which, since 2018, has become the standard deep learning technique for sequential data (previously, recurrent architectures such as the LSTM were most common). LLMs are trained in an unsupervised manner on unannotated text. A left-to-right transformer is trained to maximize the probability assigned to the next word in the training data, given the previous context. Alternatively, an LLM may use a bidirectional transformer (as in the example of BERT), which assigns a probability distribution over words given access to both preceding and following context. In addition to the task of predicting the next word or \"filling in the blanks\", LLMs may be trained on auxiliary tasks which test their understanding of the data distribution such as Next Sentence Prediction (NSP), in which pairs of sentences are presented and the model must predict whether they appear side-by-side in the training corpus.\n\nThe earliest LLMs were trained on corpora having on the order of billions of words. The first model in OpenAI's GPT series was trained in 2018 on BookCorpus, consisting of 985 million words. In the same year, BERT was trained on a combination of BookCorpus and English Wikipedia, totalling 3.3 billion words. In the years since then, training corpora for LLMs have increased by orders of magnitude, reaching up to hundreds of billions or trillions of tokens.\n\nLLMs are computationally expensive to train. A 2020 study estimated the cost of training a 1.5 billion parameter model (1-2 orders of magnitude smaller than the state of the art at the time) at $1.6 million.\n\nA 2020 analysis found that neural language models' capability (as measured by training loss) increased smoothly in a power law relationship with number of parameters, quantity of training data, and computation used for training. These relationships were tested over a wide range of values (up to seven orders of magnitude) and no attenuation of the relationship was observed at the highest end of the range (including for network sizes up to trillions of parameters).\n\nApplication to downstream tasks\nBetween 2018 and 2020, the standard method for harnessing an LLM for a specific natural language processing (NLP) task was to fine tune the model with additional task-specific training. It has subsequently been found that more powerful LLMs such as GPT-3 can solve tasks without additional training via \"prompting\" techniques, in which the problem to be solved is presented to the model as a text prompt, possibly with some textual examples of similar problems and their solutions.\n\nFine-tuning\nMain article: Fine-tuning (machine learning)\nFine-tuning is the practice of modifying an existing pretrained language model by training it (in a supervised fashion) on a specific task (e.g. sentiment analysis, named entity recognition, or part-of-speech tagging). It is a form of transfer learning. It generally involves the introduction of a new set of weights connecting the final layer of the language model to the output of the downstream task. The original weights of the language model may be \"frozen\", such that only the new layer of weights connecting them to the output are learned during training. Alternatively, the original weights may receive small updates (possibly with earlier layers frozen).\n\nPrompting\nSee also: Prompt engineering and Few-shot learning (natural language processing)\nIn the prompting paradigm, popularized by GPT-3, the problem to be solved is formulated via a text prompt, which the model must solve by providing a completion (via inference). In \"few-shot prompting\", the prompt includes a small number of examples of similar (problem, solution) pairs. For example, a sentiment analysis task of labelling the sentiment of a movie review could be prompted as follows:\n\nReview: This movie stinks.\nSentiment: negative\n\nReview: This movie is fantastic!\nSentiment:\n\nIf the model outputs \"positive\", then it has correctly solved the task. In zero-shot prompting, no solve examples are provided. An example of a zero-shot prompt for the same sentiment analysis task would be \"The sentiment associated with the movie review 'This movie is fantastic!' is\".\n\nFew-shot performance of LLMs has been shown to achieve competitive results on NLP tasks, sometimes surpassing prior state-of-the-art fine-tuning approaches. Examples of such NLP tasks are translation, question answering, cloze tasks, unscrambling words, and using a novel word in a sentence. The creation and optimisation of such prompts is called prompt engineering.\n\nInstruction tuning\nInstruction tuning is a form of fine-tuning designed to facilitate more natural and accurate zero-shot prompting interactions. Given a text input, a pretrained language model will generate a completion which matches the distribution of text on which it was trained. A naive language model given the prompt \"Write an essay about the main themes of Hamlet.\" might provide a completion such as \"A late penalty of 10% per day will be applied to submissions received after March 17.\" In instruction tuning, the language model is trained on many examples of tasks formulated as natural language instructions, along with appropriate responses. Various techniques for instruction tuning have been applied in practice. OpenAI's InstructGPT protocol involves supervised fine-tuning on a dataset of human-generated (prompt, response) pairs, followed by reinforcement learning from human feedback (RLHF), in which a reward function was learned based on a dataset of human preferences. Another technique, \"self-instruct\", fine-tunes the language model on a training set of examples which are themselves generated by an LLM (bootstrapped from a small initial set of human-generated examples).\n\nhttps://en.wikipedia.org/wiki/Large_language_model",
            "output": "In artificial intelligence in general, and in large language models in particular, a \"hallucination\" is a confident response that does not seem to be justified by the model's training data.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.6734588146209717,
            "normalized": 0.6209882327488491
        },
        "potential_entropy": {
            "strategy_component": 0.32000000000000006,
            "strategy_details": {
                "raw": 0.6400000000000001,
                "normalized": 0.6400000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 325,
        "original_data": {
            "instruction": "Give me a comma separated list of stone fruit I could buy at the market",
            "input": "",
            "output": "Plums, Apricots, Peaches, Nectarines, Pluots",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.7012157440185547,
            "normalized": 0.6289187840053013
        },
        "potential_entropy": {
            "strategy_component": 1.03,
            "strategy_details": {
                "raw": 2.06,
                "normalized": 2.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.7000000000000002,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 326,
        "original_data": {
            "instruction": "Based on the text, please give me a brief summary of the history of the DTES neighbourhood in Vancouver",
            "input": "Around the beginning of the 20th century, the DTES was Vancouver's political, cultural and retail centre. Over several decades, the city centre gradually shifted westwards, and the DTES became a poor neighbourhood, although relatively stable. In the 1980s, the area began a rapid decline due to several factors, including an influx of hard drugs, policies that pushed sex work and drug-related activity out of nearby areas, and the cessation of federal funding for social housing. By 1997, an epidemic of HIV infection and drug overdoses in the DTES led to the declaration of a public health emergency. As of 2018, critical issues include opioid overdoses, especially those involving the drug fentanyl; decrepit and squalid housing; a shortage of low-cost rental housing; and mental illness, which often co-occurs with addiction.",
            "output": "The DTES has transformed from the focal point of Vancouver’s cultural and commercial life at the start of the 20th Century to an impoverished neighbourhood with many socio-economic problems by the 1980s. Since the 1990s, the DTES has been home to a public health emergency due to drug overdoses, HIV infection, a housing crisis, and mental health problems.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.7417328357696533,
            "normalized": 0.6404950959341866
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 327,
        "original_data": {
            "instruction": "What is the difference between tennis and pickleball?",
            "input": "",
            "output": "Pickleball is a slower-paced game. The court is smaller than a tennis court so there is less ground to cover. Pickleball paddles are lighter than tennis rackets; even the ball itself is lighter. Pickleball is typically easier on your joints compared to tennis. Some believe pickleball is an easier sport to learn.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.8120546340942383,
            "normalized": 0.6605870383126395
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 328,
        "original_data": {
            "instruction": "Is crocodile a reptile or an amphibian?",
            "input": "",
            "output": "Crocodiles are reptiles. While they live both on land and in water, that in itself doesn't make them an amphibian. It lacks other amphibian characteristics, like a moist, glandular skin that lacks scales and feathers. \n\nWhile a crocodile, like a reptile, has dry, scaly skin that helps prevent water loss and provides them protection. They lay their eggs on ground.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            1,
            3
        ],
        "entropy": {
            "raw": 3.254483938217163,
            "normalized": 0.7869954109191895
        },
        "potential_entropy": {
            "strategy_component": 1.0075,
            "strategy_details": {
                "raw": 2.015,
                "normalized": 2.015,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.9,
                    "inp_complex": 0.8,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 329,
        "original_data": {
            "instruction": "How do I create knit stitches?",
            "input": "",
            "output": "In patterns a knit stitch is often abbreviated as k. A knit stitch is the first basic stitch to learn. Hold the needle with the cast-on stitches in your left hand and the second empty needle in your right hand. The yarn that is coming from your skein or ball of yarn should be held toward the back of your work and this yard strand is considered your working yard. Push the empty needle into the first loop between the first and second stitch from left to right. Grab the working yarn with your right hand and loop it right to left behind the needle that you just pushed into the first stitch. Keeping that loop on the right needle, slide the needle tips toward each other so the stitch on the left needle can fall off of the needle , keeping the loop on the right needle that you created. This is your first knit stitch. Continue the pattern to the end of the row.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.2247018814086914,
            "normalized": 0.7784862518310547
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 330,
        "original_data": {
            "instruction": "Who are the most commonly mentioned best rappers of all time?",
            "input": "",
            "output": "In no particular order, the most common top 5 rappers in \"top 5 dead or alive\" lists are as follows: Jay-Z, Nas, Tupac, The Notorious BIG, Eminem. Obviously everyone has their own preferences and styles they prefer, but more often than not, these are the 5 rappers mentioned in most lists. These rappers all had great commercial success and critical acclaim from the industry and their peers.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.5,
            "s_out_dens": 0.6,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.5,
            "s_out_dens": 0.4,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.0396242141723633,
            "normalized": 0.7256069183349609
        },
        "potential_entropy": {
            "strategy_component": 1.0025,
            "strategy_details": {
                "raw": 2.005,
                "normalized": 2.005,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.4650000000000001
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.5,
                    "out_dens": 0.4,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 331,
        "original_data": {
            "instruction": "Tell me whether these characters are from Harry Potter or Game of Thrones",
            "input": "",
            "output": "Tyrion Lannister, Luna Lovegood, Sirius Black, Draco Malfoy, Jon Snow, Sansa Stark",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.842391014099121,
            "normalized": 0.6692545754568917
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 332,
        "original_data": {
            "instruction": "What are some ideas for businesses that cater to millennials?",
            "input": "",
            "output": "Millennials like coffee shops, boba tea shops and cafes that have healthy foods like salads and smoothies. Restaurants that sell organic, vegetarian and gluten-free foods are good ideas. A pet store would also be a good idea because millennials spend a lot of money on their pets. Millennials also value travel and experiences, so a tour company that takes customers to unusual or exciting places could work well.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2042860984802246,
            "normalized": 0.7726531709943499
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 333,
        "original_data": {
            "instruction": "What are some good things to do while spending a day in the Tamalpais Valley area of Mill Valley, CA?",
            "input": "",
            "output": "Tamalpais Valley is a small neighborhood located in Marin County, California. A number of enjoyable activities can be had there during a short visit. Surrounded by lush hills, you will find a number of well maintained hiking trails. You can also enjoy delicious food at Cafe del Soul or Good Earth. If you need a quick pick-me-up, head over to Equator Coffees for a latte or espresso. Finally, you can cap your night off socializing with friends at The Junction over a pint of beer served from one of their 30 rotating taps.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.6662700176239014,
            "normalized": 0.9046485764639718
        },
        "potential_entropy": {
            "strategy_component": 0.9124999999999999,
            "strategy_details": {
                "raw": 1.8249999999999997,
                "normalized": 1.8249999999999997,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 334,
        "original_data": {
            "instruction": "From the passage list down the materials used to prepare boomerang. List the results in comma separated format.",
            "input": "A boomerang is a throwing stick with aerodynamic properties, traditionally made of wood, but also of bone, horn, tusks and even iron. Modern boomerangs used for sport may be made from plywood or plastics such as ABS, polypropylene, phenolic paper, or carbon fibre-reinforced plastics.\n\nBoomerangs come in many shapes and sizes depending on their geographic or tribal origins and intended function. Many people think only of the traditional Australian type, although today there are many types of more easily usable boomerangs, such as the cross-stick, the pinwheel, the tumble-stick, the Boomabird, and many other less common types.\n\nAn important distinction should be made between returning boomerangs and non-returning boomerangs:\n\nReturning boomerangs fly, and are examples of the earliest heavier-than-air human-made flight. A returning boomerang has two or more aerofoil section wings arranged so that when spinning they create unbalanced aerodynamic forces that curve its path into an ellipse, returning to its point of origin when thrown correctly. Their typical L-shape makes them the most recognisable form of boomerang. Although used primarily for leisure or recreation, returning boomerangs are also used to decoy birds of prey, thrown above the long grass to frighten game birds into flight and into waiting nets. Non-traditional, modern, competition boomerangs come in many shapes, sizes and materials.\nNon-returning boomerangs, throwing sticks, valari, or kylies, are primarily used as weapons. They lack the aerofoil sections, are generally heavier and designed to travel as straight and forcefully as possible to the target to bring down game. The Tamil valari variant, of ancient origin and mentioned in the Tamil Sangam literature \"Purananuru\", was one of these. The usual form of the Valari is two limbs set at an angle; one thin and tapering, the other rounded as a handle. Although valaris come in many shapes and sizes they are usually made of cast iron cast from moulds. However, some may have wooden limbs tipped with iron or with lethally sharpened edges or with special double-edged and razor-sharp daggers known as kattari.",
            "output": "wood, bone, horn, tusks, iron, plywood, plastics such as ABS, polypropylene, phenolic paper, carbon fibre-reinforced plastics",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.8469715118408203,
            "normalized": 0.956277574811663
        },
        "potential_entropy": {
            "strategy_component": 0.265,
            "strategy_details": {
                "raw": 0.53,
                "normalized": 0.53,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 335,
        "original_data": {
            "instruction": "Classify the below based on whether they are tree or animal.\nDeer, Goat, Acacia, Banyan",
            "input": "",
            "output": "Animal - Deer, Goat\nTree - Acacia, Banyan",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.087341785430908,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 336,
        "original_data": {
            "instruction": "Which of the following are continents: Asia, Africa, China, Australia, United States of America, North America, Sri Lanka, Thailand.",
            "input": "",
            "output": "The continents are: Asia, Africa, Australia, and North America.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 2.511563301086426,
            "normalized": 0.5747323717389788
        },
        "potential_entropy": {
            "strategy_component": 0.645,
            "strategy_details": {
                "raw": 1.29,
                "normalized": 1.29,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.9,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 337,
        "original_data": {
            "instruction": "From the passage provided, extract the difference in caffeine between a light roast and a dark roast.",
            "input": "Sucrose is rapidly lost during the roasting process, and may disappear entirely in darker roasts. During roasting, aromatic oils and acids weaken, changing the flavor; at 205 °C (401 °F), other oils start to develop. One of these oils, caffeol, is created at about 200 °C (392 °F), which is largely responsible for coffee's aroma and flavor. The difference of caffeine content between a light roast and a dark roast is only about 0.1%.",
            "output": "The difference of caffeine content between a light roast and a dark roast is about 0.1%.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.1854100227355957,
            "normalized": 0.7672600064958844
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 338,
        "original_data": {
            "instruction": "classify the following as outfits vs appliances: dress, shoes, toaster, jackets, fridge, air fryer, instant pot, vest, gloves, pants, jeans",
            "input": "",
            "output": "outfits: dress, shoes, jackets, vest, gloves, pants, jeans\nappliances: toaster, fridge, air fryer, instant pot",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.734898090362549,
            "normalized": 0.9242565972464425
        },
        "potential_entropy": {
            "strategy_component": 0.36250000000000004,
            "strategy_details": {
                "raw": 0.7250000000000001,
                "normalized": 0.7250000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 339,
        "original_data": {
            "instruction": "Given a reference text about Kostas Georgakis, when did he go to Italy and what did he do there?",
            "input": "Kostas Georgakis (Greek: Κώστας Γεωργάκης) (23 August 1948 – 19 September 1970) was a Greek student of geology, who in the early hours of 19 September 1970, set himself ablaze in Matteotti square in Genoa in a fatal protest against the dictatorial regime of Georgios Papadopoulos.\n\nEarly life\nGeorgakis grew up in Corfu in a family of five. His father was a self-employed tailor of modest means. Both his father and grandfather distinguished themselves in the major wars that Greece fought in the 20th century. He attended the second lyceum in Corfu where he excelled in his studies. In August 1967, a few months after the 21 April coup in Greece, Georgakis went to Italy to study as a geologist in Genoa. He received 5,000 drachmas per month from his father and this, according to friends' testimony, made him feel guilty for the financial burden his family endured so that he could attend a university. In Italy he met Rosanna, an Italian girl of the same age and they got engaged. In 1968 Georgakis became a member of the Center Union party of Georgios Papandreou.",
            "output": "Kostas Georgakis went to Genoa, Italy in 1967 to study geology.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.44285249710083,
            "normalized": 0.8408149991716657
        },
        "potential_entropy": {
            "strategy_component": 0.4025,
            "strategy_details": {
                "raw": 0.805,
                "normalized": 0.805,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.4,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 340,
        "original_data": {
            "instruction": "What are some ways you can cross the grand canyon?",
            "input": "",
            "output": "1. Use a powerful grappling hook to launch a long rope over and after security the rope climb across\n\n2. Use small aircraft to take off from one side of the canyon to land on the other.\n\n3. Take liberties with the term \"cross the Grand Canyon\" and use a car to drive around it\n\n4. Climb down to the bottom of the canyon and hike back up the other side\n\n5. Wait for a particularly heavy rain storm to fill the Grand Canyon and take a boat across.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.5027353763580322,
            "normalized": 0.857924393245152
        },
        "potential_entropy": {
            "strategy_component": 1.2625,
            "strategy_details": {
                "raw": 2.525,
                "normalized": 2.525,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 341,
        "original_data": {
            "instruction": "What is the list of top hybrid cars in the US market in 2023",
            "input": "",
            "output": "Toyota RAV4 Hybrid.\nKia Sportage Hybrid.\nHonda CR-V Hybrid.\nToyota RAV4 Prime.\nHyundai Tucson Plug-in Hybrid.\nMitsubishi Outlander Hybrid.\nHyundai Santa Fe Plug-in Hybrid.\nJeep Grand Cherokee 4xe.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.776991605758667,
            "normalized": 0.650569030216762
        },
        "potential_entropy": {
            "strategy_component": 0.6050000000000001,
            "strategy_details": {
                "raw": 1.2100000000000002,
                "normalized": 1.2100000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 342,
        "original_data": {
            "instruction": "What is a credit card?",
            "input": "",
            "output": "A credit card is a payment card issued to users (cardholders) to enable the cardholder to pay a merchant for goods and services based on the cardholder's accrued debt (i.e., promise to the card issuer to pay them for the amounts plus the other agreed charges).The card issuer (usually a bank or credit union) creates a revolving account and grants a line of credit to the cardholder, from which the cardholder can borrow money for payment to a merchant or as a cash advance. There are two credit card groups: consumer credit cards and business credit cards. Most cards are plastic, but some are metal cards (stainless steel, gold, palladium, titanium), and a few gemstone-encrusted metal cards.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            1,
            0
        ],
        "entropy": {
            "raw": 2.673280954360962,
            "normalized": 0.6209374155317035
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 343,
        "original_data": {
            "instruction": "What do I use a pressure washer for?",
            "input": "",
            "output": "A pressure washer forces water out at high speeds which allows you to remove grime or dirt from large surfaces, often of a home or building.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.4,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.6,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 4.08036470413208,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3299999999999998,
            "strategy_details": {
                "raw": 2.6599999999999997,
                "normalized": 2.6599999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.42
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.6,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 344,
        "original_data": {
            "instruction": "What causes an airplane wing to fly?",
            "input": "",
            "output": "As Bernoulli's principal explains, because of the shape, air moving across the curved upper surface on top of the wing will travel faster and thus produce less pressure than the air moving more slowly along the flatter bottom of the wing's surface. This imbalance of high and low pressures results in a force called lift which is applied in the direction of the lower pressure.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9325051307678223,
            "normalized": 0.6950014659336635
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 345,
        "original_data": {
            "instruction": "Why do companies play dividends?",
            "input": "",
            "output": "Companies will pay dividends out of their after tax earnings. It's generally done to signal to shareholders that the balance sheet is strong and that growth is expected without the need to reinvest all free cashflow back into the business.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2603142261505127,
            "normalized": 0.7886612074715751
        },
        "potential_entropy": {
            "strategy_component": 0.605,
            "strategy_details": {
                "raw": 1.21,
                "normalized": 1.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 346,
        "original_data": {
            "instruction": "Which of the following are names typically given to girls, typically given to boys, or names that could easily be given to either a boy or a girl: Sam, Bill, Liz, Tracy, Mary, Joe, Taylor, Missy, Ralph, Scott, Michelle, Michael, Frank, Cheryl, Ryan, Stephenie.",
            "input": "",
            "output": "Of these, Bill, Joe, Ralph, Scott, Michael, Frank and Ryan are names that would typically be for boys; Liz, Mary, Missy, Michelle, Cheryl and Stephenie are names that would typically be for girls; and Sam, Tracy, and Taylor are names that could apply to either a girl or a boy.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.6787397861480713,
            "normalized": 0.9082113674708775
        },
        "potential_entropy": {
            "strategy_component": 0.9124999999999999,
            "strategy_details": {
                "raw": 1.8249999999999997,
                "normalized": 1.8249999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 347,
        "original_data": {
            "instruction": "Tell me the genre of music that these artist play: Miles Davis, Ice Cube, John Mayer, Waylon Jennings",
            "input": "",
            "output": "Miles Davis: jazz\nIce Cube: rap\nJohn Mayer: rock\nWaylon Jennings: country",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 4.1681928634643555,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0825,
            "strategy_details": {
                "raw": 2.165,
                "normalized": 2.165,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 348,
        "original_data": {
            "instruction": "What is the most popular sport?",
            "input": "",
            "output": "This depends on where you live, but football is one of the most popular sports in the world. In some countries, it is called soccer. Every 4 years countries assemble teams to see which is the best, and most countries have several leagues. Popular teams, such as real madrid, can garner fans from across the world. There are people who would even argue that football is more than just a sport.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.0231199264526367,
            "normalized": 0.7208914075578962
        },
        "potential_entropy": {
            "strategy_component": 1.1800000000000002,
            "strategy_details": {
                "raw": 2.3600000000000003,
                "normalized": 2.3600000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.42000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 349,
        "original_data": {
            "instruction": "Which is an imperial or a metric measurement: mile, kilometre",
            "input": "",
            "output": "mile is imperial, kilometre is metric",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.753250598907471,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 350,
        "original_data": {
            "instruction": "What are the five smallest countries in Europe?",
            "input": "",
            "output": "The five smallest countries in Europe are Andorra, Monaco, San Marino, Vatican City, and Liechtenstein.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.5636789798736572,
            "normalized": 0.5896225656781878
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 351,
        "original_data": {
            "instruction": "Give me a list of good psychedelic rock bands that I'd like",
            "input": "",
            "output": "- Tame Impala \n- Pond\n- Kikagaku Moyo \n- Unknown Mortal Orchestra \n- King Gizzard & the Lizard Wizard\n- Pink Floyd \n- The Doors \n- Grateful Dead \n- The Flaming Lips \n- The Beatles\n- Post Animal\n- The Who",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.3096983432769775,
            "normalized": 0.8027709552219936
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 352,
        "original_data": {
            "instruction": "Please list down some key points  regarding governance system of Portland from the given text",
            "input": "The government of Portland, Oregon is based on a city commission government system. Elected officials include the mayor, commissioners, and a city auditor. The mayor and commissioners (members of City Council) are responsible for legislative policy and oversee the various bureaus that oversee the day-to-day operation of the city. Portland began using a commission form of government in 1913 following a public vote on May 3 of that year. Each elected official serves a four-year term, without term limits. Each city council member is elected at-large. In 2022, Portland residents approved a ballot measure to replace the commission form of government with a 12-member council elected in four districts using single transferable vote, with a professional city manager appointed by a directly-elected mayor, with the first elections to be held in 2024.",
            "output": "1. Portland, Oregon's governance is built on a city commission system.\n2. Elected representatives include a city auditor, commissioners, and the mayor.\n3. The mayor and commissioners, who make up the City Council, are in charge of setting legislative policy and managing the different bureaus in charge of running the city on a daily basis.\n4. After a popular vote on May 3 of that year, Portland adopted a commission system of governance.\n5. There are no term limitations on the terms of each elected official's four-year terms.\n6. All members of the municipal council are chosen at-large.\n7. Portland voters approved a ballot measure in 2022 to replace the commission form of government with a 12-member council elected in four districts using a single transferable vote, with a professional city manager appointed by a directly-elected mayor. The first elections under the new system would take place in 2024.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.000473737716675,
            "normalized": 0.7144210679190499
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 353,
        "original_data": {
            "instruction": "Who are the last three french presidents ?",
            "input": "",
            "output": "Nicolas Sarkozy was the president from May 2007 to May 2012 then Francois Hollande from May 2012 to May 2017 and finally Emmanuel Macron from May 2017 to now",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 1.8226666450500488,
            "normalized": 0.3779047557285854
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 354,
        "original_data": {
            "instruction": "What are main vegetables in V8 juice?",
            "input": "The original V8 is a savory juice. It is made mainly from water and tomato concentrate, and reconstituted vegetable juice blend: water and concentrate of eight vegetables, specifically: beets, celery, carrots, lettuce, parsley, watercress, spinach, and tomato. Campbell's has produced several varieties of the drink, such as Original, Spicy Hot, Hint of Black Pepper, Lemon, Picante, Low-Sodium, and Organic.\n\nThree spin-offs of the V8 brand, \"V8 Energy\", \"V8 Splash\" and \"V8 V•Fusion\", are blends of fruit and vegetable (specifically carrot) juices. V8 Splash is a juice cocktail partially sweetened with high-fructose corn syrup and sucralose; diet versions of V8 Splash omit the high-fructose corn syrup. V8 Energy also classifies as a cocktail and is partially sweetened with sucralose. V•Fusion is made from 100% juice.",
            "output": "beets, celery, carrots, lettuce, parsley, watercress, spinach, and tomato",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.1742804050445557,
            "normalized": 0.764080115727016
        },
        "potential_entropy": {
            "strategy_component": 0.335,
            "strategy_details": {
                "raw": 0.67,
                "normalized": 0.67,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.4,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 355,
        "original_data": {
            "instruction": "How many total seats are there?",
            "input": "Parliamentary elections were held in Norway on 7 October 1957. The result was a victory for the Labour Party, which won 78 of the 150 seats in the Storting. As a result, the Gerhardsen government continued in office.\n\nThis was the last time a single party won a majority of seats on its own in a Norwegian election.",
            "output": "150 seats",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.4,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.6,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 6.8868327140808105,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.595,
            "strategy_details": {
                "raw": 1.19,
                "normalized": 1.19,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.36
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.6,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 356,
        "original_data": {
            "instruction": "What is 'karma' according to Buddhism?",
            "input": "",
            "output": "Karma is a central and fundamental aspect of Buddhist doctrine common to all traditions of Buddhism. The Buddhist concept of Karma is a natural process that operates with all our volitional actions and which does not need an external agency or power that sits in judgement.\n\nKarma in Buddhism should not be considered as moral justice, or a method of reward and punishment, that needs an external body or authority to decide whom or what should be punished and rewarded. It is believed that Gautama Buddha attained complete insight into how the process of Karma works during the night of his full enlightenment when, through the divine eye (dibbacakku), Buddha saw how other beings die and are reborn in happy and unhappy states according to their Karma.\n\nAccording to the Cula-kammavibhanga sutta of the Majima Nikava (collection of the middle length discourses of the Buddha), a young brahmin by the name of Subha had approached the Buddha and inquired as to why there was so much inequality among human beings in terms of being short-lived or long-lived, sickly or healthy, ugly or beautiful, powerful or powerless, poor or rich, low-born or high-born, ignorant or intelligent, etc. Buddha had then explained that:\n\"all living beings are the owners of their actions, heirs to their actions, they originate from their actions, they are related through their actions, they have their actions as their refuge and that it is action or Karma that distinguishes beings as inferior and superior\n\nThe word Karma in Pali, and karma in Sanskrit, means action but encompasses only actions that are intentional, volitional and willful and which will lead to consequences sooner or later. The mind (citta) by itself cannot perform or direct any mental, verbal or physical action and it is the intention, will, mental effort or volition (cetana) that directs any mental, verbal or physical action.\n\nThus the Buddha declared:\n\"Cetanaham bhikkhave kammam vadami\nCetayitva kammam karoti, kayena, vacaya, manasa\"\n\n\"It is volition that I call Karma, by volition one performs Karma through body, word or mind.\"",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.4625871181488037,
            "normalized": 0.8464534623282296
        },
        "potential_entropy": {
            "strategy_component": 1.2025,
            "strategy_details": {
                "raw": 2.405,
                "normalized": 2.405,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 357,
        "original_data": {
            "instruction": "What are the most common meals?",
            "input": "",
            "output": "The most common meals are breakfast, lunch and dinner.  Breakfast is eat when a person wakes up, lunch is eaten in the afternoon after 4 - 5 hours of being awake, and dinner is eaten in the evening before bed time.  Each of the meals is important to give the body energy to be able to function during the day.  Skipping meals is a form of fasting, and this is very common in certain cultures and certain religions too.  Eating in between meals is not a meal but a form of snacking.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.4899699687957764,
            "normalized": 0.5685628482273647
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 358,
        "original_data": {
            "instruction": "On which day did Theodore Roosevelt lose both his first wife and his mother?",
            "input": "",
            "output": "February 14, 1884.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.85,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 2.3253097534179688,
            "normalized": 0.5215170724051339
        },
        "potential_entropy": {
            "strategy_component": 1.0825,
            "strategy_details": {
                "raw": 2.165,
                "normalized": 2.165,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 1.7000000000000002,
                    "output": 0.40499999999999997
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 359,
        "original_data": {
            "instruction": "How can I be healthy ?",
            "input": "",
            "output": "Being healthy is not a difficult task. In fact, by supporting your body and mind, making some simple tweaks to your routine, and setting small goals for yourself, you can be on the path toward living a healthier, happier life. \nIn order to be healthy, focus on the following things: \n- Having a healthy diet: you should drink more water, eat your breakfast and respect your meals time. \n- Having a healthy exercice plan.\n- Being emotionally healthy. \n- Building a healthy routine.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.699730396270752,
            "normalized": 0.6284943989345005
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 360,
        "original_data": {
            "instruction": "List three US states whose name begins with C. Then tell me the capital of the first one.",
            "input": "",
            "output": "Three US states that begin with C are:\n\n1. California\n2. Colorado\n3. Connecticut\n\nThe state capital of California is Sacramento.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.368131637573242,
            "normalized": 0.5337518964494977
        },
        "potential_entropy": {
            "strategy_component": 0.8125,
            "strategy_details": {
                "raw": 1.625,
                "normalized": 1.625,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.1,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.5,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 361,
        "original_data": {
            "instruction": "I'm planning a trip to India. How do I decide which places to visit?",
            "input": "",
            "output": "India has 28 states and many states have their own distinct culture, traditions, history, and cuisine. It takes many months to properly sample all that India has to offer, and most people who visit India (or even Indians themselves) have probably visited only a fraction of the interesting places. So, pick a particular culture that's interesting to you (for example Rajasthani), and based on the available budget and time pick a few nearby places to visit. While visiting the places, don't forget to enjoy the local cuisine, though be careful to do so in restaurants that are well reviewed and to drink only bottled water to prevent water-borne stomach diseases.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.18912672996521,
            "normalized": 0.7683219228472028
        },
        "potential_entropy": {
            "strategy_component": 0.8650000000000001,
            "strategy_details": {
                "raw": 1.7300000000000002,
                "normalized": 1.7300000000000002,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.1,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.5,
                    "inp_complex": 0.6,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 362,
        "original_data": {
            "instruction": "Given the reference text below about Larry Bird, tell me where he was born, what he did for a living, and who his greatest professional rival was.",
            "input": "Larry Joe Bird (born December 7, 1956) is an American former professional basketball player, coach, and executive in the National Basketball Association (NBA). Nicknamed \"the Hick from French Lick\" and \"Larry Legend\", Bird is widely regarded as one of the greatest basketball players of all time. He is the only person in NBA history to be named Rookie of the Year, Most Valuable Player, Finals MVP, All-Star MVP, Coach of the Year, and Executive of the Year. Growing up in French Lick, Indiana, he was a local basketball star. Highly recruited, he initially signed to play college basketball for coach Bob Knight of the Indiana Hoosiers, but Bird dropped out after one month and returned to French Lick to attend a local community college. The next year he attended the smaller Indiana State University, ultimately playing three years for the Sycamores. Drafted by the Boston Celtics with the sixth overall pick in the 1978 NBA draft after his second year at Indiana State, Bird elected to stay in college and returned for the 1978–1979 season. He then led his team to an undefeated regular season. The season finished with a national championship game match-up of Indiana State against Michigan State and featured a highly anticipated match-up of Bird against Michigan State great Magic Johnson, thus beginning a career-long rivalry that the two shared for over a decade.\nhttps://en.wikipedia.org/wiki/Larry_Bird",
            "output": "Larry Bird was born in French Lick. He played and coached basketball for a living, and his main rival was Magic Johnson.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.2635602951049805,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 363,
        "original_data": {
            "instruction": "Should differential privacy be used in the U.S. census?",
            "input": "",
            "output": "I disagree with the US Census Bureau in its use of differential privacy for reasons as follow. The main purpose of the US Census is enumeration – to correctly count the population such that resource distribution can be performed smoothly and properly in the future. The use of differential privacy essentially undermines this goal. Although populations at the state level would remain true to the raw data, those at the town level could be subject to much higher variances. For example, the New York Times article “Changes to the Census Could Make Small Towns Disappear” exemplifies how differential privacy could compromise the enumerative purpose of the US Census. Native American towns that are much less populated than the urban districts could see their population numbers being halved by the DP algorithm. In addition, these incorrect numbers for Native Americans mean more than a mere calculation mistake – it could bring real-world consequences like reduced fundings or inadequate political representation. Yet, isn’t the US Census counting the population such that everyone can receive federal resources and form political constituency equally and equitably? Therefore, the use of differential privacy essentially negates the Census’s purpose.\n\nBesides the more material concern illustrated above, I argue that the use of differential privacy could also lead to a much more social and cultural issue – representation. The New York Times article has revealed that our society has adopted algorithms that always attend to the majority at the expense of the minority, and such logic behind the algorithmic design would push the representation issue into a vicious cycle. Native American people, whose land was brutally taken away and colonized, have been historically rendered as a minority group in the society. All talks on land reparations aside, the US Census Bureau has designed and passed after checks an algorithm that diminishes their very existence on the spreadsheet. What does this imply about the logic behind the implementation of differential privacy? If the Census Bureau is fully aware that most of the Native American towns are rather sparsely populated, and the algorithm to preserve differential privacy would lead to high variances for estimation of small populations, what does the implementation of this algorithm imply? That this group of people are fine with not being represented in the final headcount, even if they have spent time and money in the process? This show of negligence again invalidates the use of differential privacy in the US Census.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.418224811553955,
            "normalized": 0.8337785175868443
        },
        "potential_entropy": {
            "strategy_component": 0.37,
            "strategy_details": {
                "raw": 0.74,
                "normalized": 0.74,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 364,
        "original_data": {
            "instruction": "How do I cook basmati rice?",
            "input": "",
            "output": "To cook basmati rice perfectly, first rinse the desired amount of rice until the water runs clear. Then, in a saucepan with a tight fitting lid and over medium heat, add rice and water in a 1C rice to 1.5C water or other liquid (like broth) ratio. Bring the pot to a simmer, reduce heat to low, cover and let cook for 15 minutes. Let the rice sit for another 10 minutes off of the heat with the lid on and the fluff with a fork and enjoy! To make the rice more flavorful, you can add cumin or other spices and salt.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.5599405765533447,
            "normalized": 0.5885544504438128
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 365,
        "original_data": {
            "instruction": "Tell me whether these are animals or bands names: Eagles, Camels, Penguins, The Turtles, The Beatles, The Animals, Arctic Monkeys, Spain, France, Gorillas, Gorillaz, Crazy Horse, Lamb Of God, Beluga sturgeon",
            "input": "",
            "output": "Interesting! Some don't fit in any of the categories, some of those are animals, some are bands names and some are actually both!\n\nLet's start by saying that Spain and France aren't animals or bands names. They are European countries.\n\nEagles, The Turtles, The Beatles could be considered both. The Turtles, The Beatles in this case, are probably referring to the band due to the use of 'The'.\n\nCamels, Penguins, Gorillas and Beluga sturgeon are definitely animals and not bands.\n\nThe Animals, Arctic Monkeys, Gorillaz, Crazy Horse, Lamb Of God are names of bands and they aren't animals.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.488278388977051,
            "normalized": 0.8537938254220145
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 366,
        "original_data": {
            "instruction": "How can I go from my couch to running a 5K in 3 months?",
            "input": "",
            "output": "Basically, you are asking, how to go from not running at all to completing your first 5K.  It is best to first consult your doctor before starting any exercise program.  Make sure you invest in an excellent pair of running shoes, try a local running shoe store for advice.  Start off by measuring the time you are running.  Alternate days running and gradually increase the length of time being careful not to increase more than 10% per run.   Once you get up to 30 minutes of running every other day, start increasing the length of time running every fourth day by 10% again until you hit 45 minutes.  It is ok to walk if you feel you need to.  Then you will be 5K running race ready.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.8408360481262207,
            "normalized": 0.6688102994646344
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 367,
        "original_data": {
            "instruction": "Tell me if the following activities are healthy or harmful: sleeping, running, eating fruit, smoking, listening to music, drinking alcohol, occasional cocaine usage, hiking by yourself in the desert with no water, eating McDonald’s",
            "input": "",
            "output": "There are a variety of activities here, some are healthy while others will result in doing harm to your body.  The clear healthy activities include: sleeping, running, eating fruit and listening to music.  The activities that are clearly harmful include: smoking, occasional cocaine usage, hiking by yourself in the desert with no water and eating McDonald’s.  There is one activity that is widely thought to be healthy when done in moderation: drinking alcohol.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5872819423675537,
            "normalized": 0.8820805549621582
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 368,
        "original_data": {
            "instruction": "What are the names of some great New England Patriots football players?",
            "input": "",
            "output": "Great New England Patriots football players include: Tom Brady, Rob Gronkowski, Ron Burton, Ty Law, Teddy Bruschi, and John Hannah.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.6736555099487305,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0975000000000001,
            "strategy_details": {
                "raw": 2.1950000000000003,
                "normalized": 2.1950000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 369,
        "original_data": {
            "instruction": "Give me a list of golf clubs you should carry in your golf bag.",
            "input": "",
            "output": "Here is a list of golf clubs you should have in your bag.   - Driver\n- 3 wood\n- Hybrid\n- 4-9 iron\n- pitching wedge\n- sand wedge\n- lob wedge\n- putter",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.9658195972442627,
            "normalized": 0.7045198849269322
        },
        "potential_entropy": {
            "strategy_component": 0.8899999999999999,
            "strategy_details": {
                "raw": 1.7799999999999998,
                "normalized": 1.7799999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 370,
        "original_data": {
            "instruction": "What are the qualities of a good manager?",
            "input": "",
            "output": "Qualities that make a good manager could vary. Different managers bring different things to the table in different situations to help the team and company. Here are a few qualities that are generally useful:\n1. Hiring: Primary job for a manager is to build a good team which is not possible without hiring the team.\n2. Building trust: It's important for a manager to build trust with each individual on their team but also within the team members themselves to create a culture for collaboration\n3. Problem solving: Managers must be able to recognize problems and help the team find resolutions either by directly helping them or finding the right help for the team\n4. Prioritization: Teams often have more work than they have people and managers play a critical role in prioritizing tasks to ensure the higher impact tasks are done first.\n5. Flexibility: Managers must be willing to play different roles as the situation demands it ranging from hands on domain expertise to conflict resolution to culture builder.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.897955894470215,
            "normalized": 0.6851302555629185
        },
        "potential_entropy": {
            "strategy_component": 1.065,
            "strategy_details": {
                "raw": 2.13,
                "normalized": 2.13,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 371,
        "original_data": {
            "instruction": "What is the capital of Maine?",
            "input": "",
            "output": "Augusta is the capital of Maine",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.208271503448486,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 372,
        "original_data": {
            "instruction": "Who was the guitarist and drummer of The Verve?",
            "input": "The Verve were an English rock band formed in Wigan, Greater Manchester in 1990 by lead vocalist Richard Ashcroft, guitarist Nick McCabe, bass guitarist Simon Jones and drummer Peter Salisbury. Guitarist and keyboard player Simon Tong later became a member in their first reunion only.",
            "output": "Nick McCabe and Peter Salisbury",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.5,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.5,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 9.821721076965332,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.62,
            "strategy_details": {
                "raw": 1.24,
                "normalized": 1.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.5,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 373,
        "original_data": {
            "instruction": "Which Philip K. Dick novel was the basis for the movie \"Blade Runner\"?",
            "input": "",
            "output": "Do Androids Dream of Electric Sheep?",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.524170398712158,
            "normalized": 0.864048685346331
        },
        "potential_entropy": {
            "strategy_component": 1.3449999999999998,
            "strategy_details": {
                "raw": 2.6899999999999995,
                "normalized": 2.6899999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 374,
        "original_data": {
            "instruction": "Extract all of the dates mentioned in this paragraph and list them using bullets in the format {Date} - {Description}",
            "input": "Daniel Leavitt was born November 16, 1813, at Rye, New Hampshire, the son of Benning Leavitt, influential businessman, state senator, county commissioner and later Chicopee Selectman, and his wife Olive (Jenness) Leavitt. Daniel Leavitt married in 1838 at West Springfield, Massachusetts, Ruth Jeannette Ball. They had three children.",
            "output": "• November 16, 1813 - This is when Daniel Leavitt was born\n• 1838 - This is when Daniel Leavitt married his wife Ruth Jeannette Ball.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.8564324378967285,
            "normalized": 0.6732664108276367
        },
        "potential_entropy": {
            "strategy_component": 0.39,
            "strategy_details": {
                "raw": 0.78,
                "normalized": 0.78,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 375,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Bongo drum, Kingri",
            "input": "",
            "output": "Kingri is string, Bongo drum is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.617046356201172,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 376,
        "original_data": {
            "instruction": "What is the tallest building in the San Francisco Bay Area?",
            "input": "",
            "output": "The tallest building in the SF Bay Area is the Salesforce Tower.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.1064376831054688,
            "normalized": 0.7446964808872768
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 377,
        "original_data": {
            "instruction": "What is the name of the largest train station in Amsterdam",
            "input": "",
            "output": "The largest train station in Amsterdam is called Centraal Station.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.464802265167236,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 378,
        "original_data": {
            "instruction": "Tracey and Hepburn first film in 1942 was what",
            "input": "",
            "output": "Woman of the Year",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.2,
            "s_out_div": 0.0,
            "s_out_dens": 0.1,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.8,
            "s_out_div": 1.0,
            "s_out_dens": 0.9,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 5.677276611328125,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.645,
            "strategy_details": {
                "raw": 3.29,
                "normalized": 3.29,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 2.0,
                    "output": 0.9299999999999999
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.8,
                    "out_div": 1.0,
                    "out_dens": 0.9,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 379,
        "original_data": {
            "instruction": "How many vowels are in the word dinosaur?",
            "input": "",
            "output": "The letters a, e, I, o and u are vowels. As such there are 4 vowels in dinosaur",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.8638978004455566,
            "normalized": 0.9611136572701591
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 380,
        "original_data": {
            "instruction": "Where was city musician Thomas Russell born?",
            "input": "",
            "output": "Los Angeles",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.3,
            "s_out_div": 0.1,
            "s_out_dens": 0.2,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 10.037908554077148,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.4050000000000002,
            "strategy_details": {
                "raw": 2.8100000000000005,
                "normalized": 2.8100000000000005,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.8700000000000001
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.7,
                    "out_div": 0.9,
                    "out_dens": 0.8,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 381,
        "original_data": {
            "instruction": "Which characters belong to DC or Marvel Universe? Red Hood, Bucky Barnes",
            "input": "",
            "output": "Red Hood is DC, Bucky Barnes is Marvel",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.2,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.8,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 6.9378557205200195,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0699999999999998,
            "strategy_details": {
                "raw": 2.1399999999999997,
                "normalized": 2.1399999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.8,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 382,
        "original_data": {
            "instruction": "Classify the below aircraft models based on the companies which created them.\nNext-Generation 737, A220, 737 MAX, A350, A330, 747-8, 767, A320, 777, 777X, A380, 787",
            "input": "",
            "output": "Boeing- Next-Generation 737, 737 MAX, 747-8, 767, 777, 777X, 787\nAirbus- A220, A320, A330, A350, A380",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 1.516842007637024,
            "normalized": 0.29052628789629253
        },
        "potential_entropy": {
            "strategy_component": 0.265,
            "strategy_details": {
                "raw": 0.53,
                "normalized": 0.53,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 383,
        "original_data": {
            "instruction": "What is the colloquial term for LSD?",
            "input": "Lysergic acid diethylamide, commonly known as LSD (from German Lysergsäure-diethylamid), also known colloquially as acid, is a potent psychedelic drug. Effects typically include intensified thoughts, emotions, and sensory perception. At sufficiently high dosages LSD manifests primarily mental, visual, as well as auditory, hallucinations. Dilated pupils, increased blood pressure, and increased body temperature are typical. Effects typically begin within half an hour and can last for up to 20 hours. LSD is also capable of causing mystical experiences and ego dissolution. It is used mainly as a recreational drug or for spiritual reasons. LSD is both the prototypical psychedelic and one of the \"classical\" psychedelics, being the psychedelics with the greatest scientific and cultural significance. LSD is typically either swallowed or held under the tongue.",
            "output": "Lysergic acid diethylamide is commonly known as LSD or acid",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.836597442626953,
            "normalized": 0.9533135550362724
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 384,
        "original_data": {
            "instruction": "Please list the key points about Bangladesh economy from the paragraph below:",
            "input": "Bangladesh is the second largest economy in South Asia after India. The country has outpaced India (of which it was a part until 1947) and Pakistan (of which it was a part until 1971) in terms of per capita income. According to the World Bank, \"When the newly independent country of Bangladesh was born on December 16, 1971, it was the second poorest country in the world—making the country's transformation over the next 50 years one of the great development stories. Since then, poverty has been cut in half at record speed. Enrolment in primary school is now nearly universal. Hundreds of thousands of women have entered the workforce. Steady progress has been made on maternal and child health. And the country is better buttressed against the destructive forces posed by climate change and natural disasters. Bangladesh's success comprises many moving parts—from investing in human capital to establishing macroeconomic stability. Building on this success, the country is now setting the stage for further economic growth and job creation by ramping up investments in energy, inland connectivity, urban projects, and transport infrastructure, as well as prioritizing climate change adaptation and disaster preparedness on its path toward sustainable growth\".\n\nAfter the partition of India, the region underwent a change in economic geography. In East Pakistan, free market principles were generally accepted. The government promoted industrialization to produce consumer goods as quickly as possible in order to avoid dependence on imports. Certain sectors, like public utilities, fell under state ownership. Demand for jute during the Korean War led to the creation of the Adamjee Jute Mills, which replaced jute mills in Dundee and Calcutta as the largest jute mill in the world. However, by the 1960s, East Pakistan's share of exports fell from 70% to 50% as West Pakistan received the major portion of investments. Economic grievances played a key role in the pro-independence aspirations of East Pakistanis. During the initial five years of independence (1971-1975), newly created Bangladesh followed a socialist economy. In the late 1970s, socialist policies were largely reversed, industrial plants were returned to private owners, and private industry was increasingly promoted. The government set up export processing zones to stimulate the export economy. Between 1991 and 1993, finance minister Saifur Rahman launched further reforms with support from the IMF which liberalized the economy and boosted industrial growth, services, and exports. By the late 1990s and early 2000s, the reform momentum lost steam due to chronic political instability, but the economy continued to grow.\n\nIn 2022, Bangladesh had the second largest foreign-exchange reserves in South Asia. The reserves have boosted the government's spending capacity in spite of tax revenues forming only 7.7% of government revenue. A big chunk of investments have gone into the power sector. In 2009, Bangladesh was experiencing daily blackouts several times a day. In 2022, the country achieved 100% electrification. One of the major anti-poverty schemes of the Bangladeshi government is the Ashrayan Project which aims to eradicate homelessness by providing free housing. The poverty rate has gone down from 80% in 1971, to 44.2% in 1991, to 12.9% in 2021. The literacy rate stood at 74.66% in 2022. Bangladesh has a labor force of roughly 70 million, which is the world's seventh-largest; with an unemployment rate of 5.2% as of 2021. The government is setting up 100 special economic zones to attract foreign direct investment (FDI) and generate 10 million jobs. The Bangladesh Investment Development Authority (BIDA) and the Bangladesh Economic Zones Authority (BEZA) have been established to help investors in setting up factories; and to complement the longstanding Bangladesh Export Processing Zone Authority (BEPZA). The Bangladeshi taka is the national currency. The service sector accounts for about 51.3% of total GDP and employs 39% of the workforce. The industrial sector accounts for 35.1% of GDP and employs 20.4% of the workforce. The agriculture sector makes up 13.6% of the economy but is the biggest employment sector, with 40.6% of the workforce. In agriculture, the country is a major producer of rice, fish, tea, fruits, vegetables, flowers, and jute. Lobsters and shrimps are some of Bangladesh's well known exports.",
            "output": "1. Bangladesh is the second-largest economy in South Asia\n\n2. In 2022, Bangladesh had the second largest foreign-exchange reserves in South Asia\n\n3. The poverty rate of Bangladesh is currently at 12.9%. Down from 80% in 1971 and 44.2% in 1991.\n\n4. Bangladesh has the world's seventh-largest labor force.\n\n5. The service sector of Bangladesh makes up 51.3% of the GDP, the industrial sector makes up 35.1% and the agriculture sector 35.1%.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 1.9872318506240845,
            "normalized": 0.42492338589259554
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 385,
        "original_data": {
            "instruction": "List some of the television gameshows that Bob Stewart created, co-created, and/or produced.",
            "input": "",
            "output": "The Price is Right, Password, To Tell the Truth, Pyramid shows (such as the $10,000 Pyramid, the $25,000 Pyramid, and so on), Jackpot!, Winning Streak",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.4211199283599854,
            "normalized": 0.8346056938171387
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 386,
        "original_data": {
            "instruction": "write a short poem about gratitude to my sister friends who lead Obon dancing with me every year",
            "input": "",
            "output": "A small group of women gather with me in the small country church in California to keep Japanese Obon (folk dancing) tradition alive in the community.  The festival is open to the public and includes brightly colored kimonos, lanterns and japanese food. We carry the tradition for the next generation - children are dressed in traditional dress.   These 8 - 10 sister-women keep the tradition of honoring our ancestors, keeping the family tradition and provide dance leadership to hundreds of people every year at the annual Obon festival.  They sacrifice their time to teach and lead these dances every year.  They are my sisters",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.6559205055236816,
            "normalized": 0.9016915730067662
        },
        "potential_entropy": {
            "strategy_component": 0.41500000000000004,
            "strategy_details": {
                "raw": 0.8300000000000001,
                "normalized": 0.8300000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 387,
        "original_data": {
            "instruction": "From the given passage, find out why companies would announce their future products much in advance.",
            "input": "The Osborne effect is a social phenomenon of customers canceling or deferring orders for the current, soon-to-be-obsolete product as an unexpected drawback of a company's announcing a future product prematurely. It is an example of cannibalization.\nThe term alludes to the Osborne Computer Corporation, whose second product did not become available until more than a year after it was announced. The company's subsequent bankruptcy was widely blamed on reduced sales after the announcement.\nThe Osborne Effect states that prematurely discussing future, unavailable products damages sales of existing products. The name comes from the planned replacement of the Osborne 1, an early personal computer first sold by the Osborne Computer Corporation in 1981. In 1983, founder Adam Osborne pre-announced several next-generation computer models (the Osborne Executive and Osborne Vixen), which were only prototypes, highlighting the fact that they would outperform the existing model as the prototypes dramatically cut down assembly time. A widely held belief was that sales of the Osborne 1 fell sharply as customers anticipated those more advanced systems, leading to a sales decline from which Osborne Computer was unable to recover. This belief appeared in the media almost immediately after the company's September 1983 bankruptcy:\n\nTo give the jazzy $2,495 Osborne Executive a running start, Adam began orchestrating publicity early in 1983. We, along with many other magazines, were shown the machine in locked hotel rooms. We were required not to have anything in print about it until the planned release date in mid-April. As far as we know, nothing did appear in print, but dealers heard about the plans and cancelled orders for the Osborne 1 in droves. In early April, Osborne told dealers he would be showing them the machine on a one-week tour the week of 17 April, and emphasized that the new machine was not a competitor for the Osborne 1. But dealers didn't react the way Osborne expected; said Osborne, \"All of them just cancelled their orders for the Osborne 1.\"\n\nOsborne reacted by drastically cutting prices on the Osborne 1 in an effort to stimulate cash flow. But nothing seemed to work, and for several months sales were practically non-existent.\n\nPre-announcement is done for several reasons: to reassure current customers that there is improvement or lower cost coming, to increase the interest of the media and investors in the company's future prospects, and to intimidate or confuse competitors. When done correctly, the sales or cash flow impact to the company is minimal, with the revenue drop for the current product being offset by orders or completed sales of the new product as it becomes available. However, when the Osborne effect occurs, the quantity of unsold goods increases and the company must react by discounting and/or lowering production of the current product, both of which depress cash flow.",
            "output": "Companies pre-announce their future products because of the following reasons:\n1. to reassure current customers that their product is improving\n2. to comfort current customers that a lower cost model of their is coming\n2. to increase the interest of the media and investors in the company's future prospects\n3. to intimidate or confuse competitors",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1293272972106934,
            "normalized": 0.7512363706316266
        },
        "potential_entropy": {
            "strategy_component": 0.32000000000000006,
            "strategy_details": {
                "raw": 0.6400000000000001,
                "normalized": 0.6400000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 388,
        "original_data": {
            "instruction": "How many golf clubs can a player carry in their bag?",
            "input": "",
            "output": "Per the PGA (Professional Golfers' Association), a player can carry up to 14 clubs in their bag.  There is no minimum number of clubs a player must carry.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.6738052368164062,
            "normalized": 0.6210872105189732
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 389,
        "original_data": {
            "instruction": "Classify the numbers based on their divisibility by 2,3,7\n4, 9, 20, 28, 35, 8",
            "input": "",
            "output": "Divisible by 2: 4, 20, 8, 28\nDivisible by 3: 21\nDivisible by 7: 21, 28",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.5,
            "s_out_div": 0.4,
            "s_out_dens": 0.8,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.5,
            "s_out_div": 0.6,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 1.4927759170532227,
            "normalized": 0.2836502620152065
        },
        "potential_entropy": {
            "strategy_component": 0.71,
            "strategy_details": {
                "raw": 1.42,
                "normalized": 1.42,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.5,
                    "out_div": 0.6,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 390,
        "original_data": {
            "instruction": "Who played Billy the Kid in The Left Handed Gun",
            "input": "",
            "output": "Paul Newman",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 13.101675987243652,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1725,
            "strategy_details": {
                "raw": 2.345,
                "normalized": 2.345,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 391,
        "original_data": {
            "instruction": "Give me a one line summary of the story below: \n\nA long long time ago the Queen of Odisha heard that there was a master stone sculptor who made such stunning statues of the gods that people would weep from happiness at the beauty of the statues if they saw them. One summer day, while resting indolently with the King in the main hall of the royal palace, the Queen suddenly had a thought. \"How nice it would be, King, if we could build some beautiful statutes of Lord Jagannath so that the people can come worship Jagannath, Balabhadra, and Subadhra. What do you think?\" \n\nThe King thought for a few minutes, and then nodded in agreement. \"What do you have in mind, my Queen?\" And so, soon the stone sculptor was summoned to appear before the Court for an appointment to build in his way the statutes of the gods. Soon the monsoon season started and the sculptor appeared in front of the court, awaiting his appointment. He was a humble man from the mountainous regions of Odisha. Even at his best-dressed, one couldn't help but notice his rough hands, and the small sprinkling of dust that seemed ever present in his hair. \n\nThe King and Queen graciously waited as the sculptor bowed and then asked \"Your majesties, what can I do for you?\" The Queen leaned forward in her throne and her voice rang out in the court \"We have heard, o Master Sculptor, of your skills from people far and wide across the Kingdom. We dream of having you put your skills to use to build beautiful statues that will represent Lord Jagannath, Subhadra, and Balabhadra for our people to worship.\"\n\nThe sculptor was silent. As his silence extended, ministers of the court began to feel restless. \"Speak up! How can you be silent when the Queen graces you so!\" But still, the sculptor was silent. \n\nThe King and Queen sat patiently in their thrones as they waited for the sculptor to respond. Perhaps the enormity of the task was too much. Perhaps, the sculptor was thinking of what he should ask for in return. Or even more, perhaps, the sculptor was awed and honored by the request. \n\nAt last, as the shadows from the late afternoon sun began to stretch across the floors of the hall, the sculptor shifted his weight, and opened his mouth to speak. \"Your majesties, you grace me with such a request, I cannot help but oblige. It is an honor.\" At this, the Queen sat as straight as a rod, listening for the words that would come next. \"But, I can do this on one condition only. I will work in a closed hall, where none may enter - regardless of whether they can hear me working or otherwise. None may enter until I open the doors from within and proclaim to all that the statues are completed.\"\n\nThe ministers gasped - what a strange request from this man. Was he a fool? What if he just slept and ate and didn't do any work? How could the King and Queen trust that he would do the work he was promising to do? The Queen looked on thoughtfully. She glanced at the King, and it seemed to her that he agreed on this condition without reservation. She looked at the sculptor and then nodded her assent. \"We decree that you shall build these statues in a hall set aside, into which no one shall enter until your work is complete and you proclaim that to us after opening the doors of the hall from within.\"",
            "input": "",
            "output": "An Odia sculptor successfully bargains with the King and Queen to build statutes for Jagannath, Balabhadra, and Subhadra, without any disturbances or interruptions, until the sculptor is ready to state that the statues are complete.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9673802852630615,
            "normalized": 0.9906800815037319
        },
        "potential_entropy": {
            "strategy_component": 0.41,
            "strategy_details": {
                "raw": 0.82,
                "normalized": 0.82,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 392,
        "original_data": {
            "instruction": "How big is the Atlas Moth?",
            "input": "The Atlas moth is one of the largest lepidopterans, with a wingspan measuring up to 24 cm (9.4 in) and a wing surface area of about 160 cm2 (≈25 in2). It is only surpassed in wingspan by the white witch (Thysania agrippina) and Attacus caesar, and in wing surface area by the Hercules moth (Coscinocera hercules). As in most silk moths, females are noticeably larger and heavier than males, while males have broader antennae.",
            "output": "The Atlas Moth has a wingspan measuring up to 24 cm.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.129551649093628,
            "normalized": 0.751300471169608
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 393,
        "original_data": {
            "instruction": "What are all Marvel movies with Thanos in it?",
            "input": "",
            "output": "Here is a list of Marvel Cinematic Universe films in which the character Thanos makes an appearance:\n1. The Avengers (2012)\n2. Guardians of the Galaxy (2014)\n3. Avengers: Age of Ultron (2015)\n4. Avengers: Infinity War (2018)\n5. Avengers: Endgame (2019)",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 1.3303309679031372,
            "normalized": 0.23723741940089635
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 394,
        "original_data": {
            "instruction": "Choose the odd one out: Roger Federer, Roger Moore, Roger Binny",
            "input": "",
            "output": "Roger Moore as he is an actor, whereas the other two people are former sportsmen",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 5.245209693908691,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 395,
        "original_data": {
            "instruction": "Which of the following are breeds of dogs:\nDachshund, Tabby, Russian Blue, Airedale Terrier, Papillon, Bengal, Greyhound",
            "input": "",
            "output": "Dachshund, Airedale Terrier, Papillon and Greyhound are all breeds of dogs.\nTabby, Russian Blue and Bengal are breeds of cats.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.8744442462921143,
            "normalized": 0.678412641797747
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 396,
        "original_data": {
            "instruction": "From the given text, Provide me step by step instructions on how to prepare tea.",
            "input": "Steeping, or brewing, is the process of making tea from leaves and hot water, generally using 2 grams (0.071 oz) of tea per 100 millilitres (3.5 imp fl oz; 3.4 US fl oz) of water (H2O) or about 1 teaspoon of green tea per 150 ml cup. Steeping temperatures range from 61 °C (142 °F) to 87 °C (189 °F) and steeping times from 30 seconds to three minutes.",
            "output": "1. To brew/steep tea, use 2gm tea leaves per 100ml of water and let it brew between 61 °C to 87 °C for 30 seconds to 3 minutes.\n2. For green tea, use 1 teaspoon of green tea per 150 ml of water and  let it brew between 61 °C to 87 °C for 30 seconds to 3 minutes.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.0918610095977783,
            "normalized": 0.45481743131365093
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 397,
        "original_data": {
            "instruction": "Categorize each of the following by the amount of noise they would make, either \"a lot of noise\", \"some noise\" or \"no noise\". A baby crying, a glass vase being dropped, a door being closed, a rabbit hopping across carpeted floors, a running microwave, reading a book",
            "input": "",
            "output": "A baby crying: a lot of noise, a glass vase being dropped: a lot of noise, a door being closed: some noise, a rabbit hopping across carpeted floors: no noise, a running microwave: some noise, reading a book: no noise, slamming a door: a lot of noise",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.284965753555298,
            "normalized": 0.7957045010157994
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 398,
        "original_data": {
            "instruction": "How many grand slams did Boris Becker win?",
            "input": "",
            "output": "Boris Becker won a total of six Grand Slam titles during his career. He won Wimbledon in 1985, 1986 and 1989, the US Open in 1989 and the Australian Open in 1991 and 1996.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            2,
            0
        ],
        "entropy": {
            "raw": 1.7264810800552368,
            "normalized": 0.35042316573006765
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.85,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 399,
        "original_data": {
            "instruction": "What is the number one selling dog?",
            "input": "",
            "output": "The American Kennel Club released its 2022 registration statistics and found that after being the most popular dog breed for three decades, the Labrador Retriever was outranked by the French Bulldog.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.992698907852173,
            "normalized": 0.7121996879577637
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 400,
        "original_data": {
            "instruction": "What causes that feeling of weightlessness when you're on a rollercoaster?",
            "input": "",
            "output": "The weightless feeling from a rollercoaster is caused by gravitational force, also known as g-force. Different rollercoaster elements lead to different types of g-force: there is positive g-force, which drives blood downwards towards the feet, and negative g-force, which drives blood upwards towards the head. Negative g-force acts opposite to gravity, and it occurs when you accelerate downwards faster than free-fall. The acceleration causes you to pull out of your seat, evoking a sense of weightlessness.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.0062005519866943,
            "normalized": 0.716057300567627
        },
        "potential_entropy": {
            "strategy_component": 1.2175,
            "strategy_details": {
                "raw": 2.435,
                "normalized": 2.435,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 401,
        "original_data": {
            "instruction": "Who is Roger Federer?",
            "input": "Roger Federer (born 8 August 1981) is a Swiss former professional tennis player. He was ranked world No. 1 by the Association of Tennis Professionals (ATP) for 310 weeks, including a record 237 consecutive weeks, and finished as the year-end No. 1 five times. He won 103 singles titles on the ATP Tour, the second most of all time, including 20 major men's singles titles, a record eight men's singles Wimbledon titles, an Open Era joint-record five men's singles US Open titles, and a joint-record six year-end championships. In his home country, he is regarded as \"the greatest and most successful\" Swiss sportsperson in history.",
            "output": "Roger Federer is a Swiss former professional tennis player. He has won 20 major's singles title, totally 103 singles titles on the ATP tour. He was ranked world No.1 for 310 weeks, and was considered as the great of all time by many people.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9986186027526855,
            "normalized": 0.7138910293579102
        },
        "potential_entropy": {
            "strategy_component": 0.30000000000000004,
            "strategy_details": {
                "raw": 0.6000000000000001,
                "normalized": 0.6000000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.15000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 402,
        "original_data": {
            "instruction": "Make a list, using dot points, of the last five cities who hosted the Olympics.",
            "input": "",
            "output": "The last five cities to host the Olympic Games were:\n- Tokyo, Japan (2021),\n- Rio, Brazil (2016),\n- London, England (2012),\n- Beijing, China (2008),\n- Athens, Greece (2004).",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 1.5275739431381226,
            "normalized": 0.29359255518232075
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 403,
        "original_data": {
            "instruction": "Who is the greatest basketball player of all time?",
            "input": "",
            "output": "This is a highly subjective topic and hotly debated.   Most basketball experts would agree that a player who has acquired the most NBA championships should be on the list.  Bill Russell, with 12 championships, has to be in the conversation.  LeBron James is the all time scoring leader and has had one of the longest NBA careers to go along with 4 titles.  Kobe Bryant at 5 titles is also on the list.  But the most dominant player of his era in the 1990's, winning 6 titles in 6 attempts, was Michael Jordan.  Michael Jordan was considered one of the best two way players, excelling at both defense and offense.  So while some might not agree, Michael Jordan is always in the conversation for best NBA player of all time.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.8837826251983643,
            "normalized": 0.6810807500566755
        },
        "potential_entropy": {
            "strategy_component": 1.0750000000000002,
            "strategy_details": {
                "raw": 2.1500000000000004,
                "normalized": 2.1500000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 404,
        "original_data": {
            "instruction": "Which of the following sports involve rackets: Tennis, Badminton, Squash, Football, Soccer, Basketball, Cricket, Ping Pong.",
            "input": "",
            "output": "Tennis, Badminton, Squash, Ping Pong are racket sports.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.361276626586914,
            "normalized": 0.8175076075962612
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 405,
        "original_data": {
            "instruction": "Which is a species of fish? Nurseryfish or Nursery",
            "input": "",
            "output": "Nurseryfish",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 11.414021492004395,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 406,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Claves, Rabeca",
            "input": "",
            "output": "Rabeca is string, Claves is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.313250541687012,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 407,
        "original_data": {
            "instruction": "What is microeconomics?",
            "input": "",
            "output": "Microeconimics is concerned with decision-making by individual economic agents such as firms and consumers.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.401750326156616,
            "normalized": 0.8290715217590332
        },
        "potential_entropy": {
            "strategy_component": 1.2175,
            "strategy_details": {
                "raw": 2.435,
                "normalized": 2.435,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 408,
        "original_data": {
            "instruction": "Identify which animal is domesticated or wild: Society finch, Nutria",
            "input": "",
            "output": "Nutria is wild, Society finch is domesticated",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.5,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.5,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 6.6505279541015625,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5975,
            "strategy_details": {
                "raw": 1.195,
                "normalized": 1.195,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.31499999999999995
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.5,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 409,
        "original_data": {
            "instruction": "Where was Samuel Loew born?",
            "input": "Samuel ben Nathan Ha-Levi Loew (Kelin or Kolin) (also \"Lōw\" or \"Löw\", Hebrew: שמואל בן נתן נטע הלוי קעלין; c. 1720–1806) was a Talmudist and Halakhist, best known for his work Machatzit HaShekel (Ashkenazi pronunciation, Machatzis HaShekel).\n\nThe son of Nathan Naṭe ha-Levi, he was born at Kolín, Bohemia. For nearly 60 years he headed over a yeshiva at Boskovice (Boskowitz), Moravia, where he died on May 20, 1806. He had the title Av Beis Din of Boskowitz.\n\nSamuel had 3 sons and 3 daughters: Benjamin Ze'ev Ha-Levi (Wolf) Boskowitz (1746-1818), Jacob (c 1750-1833), Jehuda (c 1751-1800), Sara (Hinda) married Markus (Mordechai Lob) Wassertrilling of Boskovice, Esther married Bernhard Biach. His son Wolf Boskowitz delivered the sermon at his funeral. His descendant in the 5th generation, Dr. Max Anton Löw, a convert to Roman Catholicism, was the attorney of the anti-Semite Francis Deckert.[3",
            "output": "Samuel Loew was born at Kolin, Bohemia.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.85,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.742331027984619,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5675,
            "strategy_details": {
                "raw": 1.135,
                "normalized": 1.135,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 410,
        "original_data": {
            "instruction": "What is the name of William Gibson's first comic book series?",
            "input": "Archangel, also written as William Gibson Archangel or William Gibson's Archangel, is a five-issue limited series comic book that was created by William Gibson and Michael St. John Smith, written by William Gibson, illustrated by Butch Guice and story-edited by Michael Benedetto. It is Gibson's first comic book series, which is set in an alternative version of 2016 in which the Vice President of America travels back in time to 1945 to secure power.\n\nIDW Publishing first released Archangel from May 2016 to August 2017, then in October as a hardcover graphic novel. It was nominated for an Eisner Award for Best Limited Series and was adapted into an audio play that was released by Audible.",
            "output": "Archangel",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 15.867149353027344,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 411,
        "original_data": {
            "instruction": "What percentage of the world's population did the British Empire hold sway over?",
            "input": "The British Empire was composed of the dominions, colonies, protectorates, mandates, and other territories ruled or administered by the United Kingdom and its predecessor states. It began with the overseas possessions and trading posts established by England between the late 16th and early 18th centuries. At its height it was the largest empire in history and, for over a century, was the foremost global power. By 1913, the British Empire held sway over 412 million people, 23 per cent of the world population at the time, and by 1920, it covered 35.5 million km2 (13.7 million sq mi), 24 per cent of the Earth's total land area. As a result, its constitutional, legal, linguistic, and cultural legacy is widespread. At the peak of its power, it was described as \"the empire on which the sun never sets\", as the Sun was always shining on at least one of its territories.",
            "output": "23 per cent of the world population",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.88,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.12,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.7146480083465576,
            "normalized": 0.9184708595275879
        },
        "potential_entropy": {
            "strategy_component": 0.3425,
            "strategy_details": {
                "raw": 0.685,
                "normalized": 0.685,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.21999999999999997,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.12,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 412,
        "original_data": {
            "instruction": "How do I keep my plants alive?",
            "input": "",
            "output": "To keep your plants alive you will first need to understand what type of nutrition and environment is best suited for them. For environment, some plants prefer full sun, whereas some prefer partial or full shade. There's also humidity and temperature to consider. For nutrition, determine what types or food and how much water to provide at what frequency. You can usually find this information by consulting plant expert resources online or at your local library. Once you understand the ideal climate and nutrition for your plant, adjust the climate to best suit it, while following the ideal schedule for feeding and watering. Also, once the plant outgrows the pot it is in, it's time to move it to a bigger pot. This should keep your plant happy and healthy.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.8195481300354004,
            "normalized": 0.6627280371529716
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 413,
        "original_data": {
            "instruction": "What is the relationship between interest rates and the economy?",
            "input": "",
            "output": "Central banks typically adjust interest rates as a way to influence inflation and economic growth. When a country is experiencing high rates of inflation, its central bank will typically increase the interest rate which will increase the cost of credit and therefore reduce economic demand and consumption. Reduced demand will lead to reduced prices and less inflation. When a country is experiencing deflation, the central bank will typically decrease interest rates to reduce cost of credit and increase consumption. Central banks must be careful with interest rates. High interest rates for a sustained period of time can lead to a recession. Whereas, low interest rates for a sustained period of time can lead to hyperinflation and an overheated economy.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.128594160079956,
            "normalized": 0.4653126171657017
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 414,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Bonang, Chiwang",
            "input": "",
            "output": "Chiwang is string, Bonang is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.033542633056641,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 415,
        "original_data": {
            "instruction": "Given this reference text about beneficence, what can I do to ensure compliance to the beneficence concept?",
            "input": "Beneficence is a concept in research ethics that states that researchers should have the welfare of the research participant as a goal of any clinical trial or other research study. The antonym of this term, maleficence, describes a practice that opposes the welfare of any research participant. According to the Belmont Report, researchers are required to follow two moral requirements in line with the principle of beneficence: do not harm and maximize possible benefits for research while minimizing any potential harm on others.\n\nThe concept that medical professionals and researchers would always practice beneficence seems natural to most patients and research participants, but in fact, every health intervention or research intervention has potential to harm the recipient. There are many different precedents in medicine and research for conducting a cost–benefit analysis and judging whether a certain action would be a sufficient practice of beneficence, and the extent to which treatments are acceptable or unacceptable is under debate.\n\nDespite differences in opinion, there are many concepts on which there is wide agreement. One is that there should be community consensus when determining best practices for dealing with ethical problems.",
            "output": "Ensure you are not harming your research participants, while maximizing benefits and minimizing any potential harm.  Engage with your community to determine best practices when encountering ethical concerns.  This should ensure your compliance with beneficence.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.131065368652344,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 416,
        "original_data": {
            "instruction": "Toronto Maple Leafs is a team in the NHL. Who was their captain and star player in the early 2000s?",
            "input": "",
            "output": "Mats Sundin",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.4,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.6,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 8.44107723236084,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6125,
            "strategy_details": {
                "raw": 1.225,
                "normalized": 1.225,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.6,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 417,
        "original_data": {
            "instruction": "Should I take extra fuel when traveling to Alaska?",
            "input": "",
            "output": "There are limited roads in Alaska, as well as northern Canada. While there are some gas stops along the way, options are limited and you might run out of gas in the wilderness. As Alaska is an extremely large state, being stranded in the middle of nowhere is potentially very harmful. It’s a very good idea to take extra fuel when taking a road trip to or through Alaska. Make sure you buy a proper gas can that will not dissolve when in contact with gasoline or diesel.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.5647754669189453,
            "normalized": 0.875650133405413
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 418,
        "original_data": {
            "instruction": "Write a haiku about cats",
            "input": "",
            "output": "Whiskers fluttering\nA silent leap, and then: crash\nMy coffee mug breaks.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            1,
            3
        ],
        "entropy": {
            "raw": 5.714759826660156,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.065,
            "strategy_details": {
                "raw": 2.13,
                "normalized": 2.13,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.8,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.9,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 419,
        "original_data": {
            "instruction": "How many James Bond movies did Daniel Craig star in?",
            "input": "",
            "output": "Daniel Craig starred in 5 James Bond movies",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.1921281814575195,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 420,
        "original_data": {
            "instruction": "What was the The Treaty of Tordesillas?",
            "input": "In 1383, John I of Castile, husband of Beatrice of Portugal and son-in-law of Ferdinand I of Portugal, claimed the throne of Portugal. A faction of petty noblemen and commoners, led by John of Aviz (later King John I of Portugal) and commanded by General Nuno Álvares Pereira defeated the Castilians in the Battle of Aljubarrota. With this battle, the House of Aviz became the ruling house of Portugal.\n\nThe new ruling dynasty would proceed to push Portugal to the limelight of European politics and culture, creating and sponsoring works of literature, like the Crónicas d'el Rei D. João I by Fernão Lopes, the first riding and hunting manual Livro da ensinança de bem cavalgar toda sela and O Leal Conselheiro both by King Edward of Portugal and the Portuguese translations of Cicero's De Oficiis and Seneca's De Beneficiis by the well traveled Prince Peter of Coimbra, as well as his magnum opus Tratado da Vertuosa Benfeytoria. In an effort of solidification and centralization of royal power the monarchs of this dynasty also ordered the compilation, organization and publication of the first three compilations of laws in Portugal: the Ordenações d'el Rei D. Duarte, which was never enforced; the Ordenações Afonsinas, whose application and enforcement was not uniform across the realm; and the Ordenações Manuelinas, which took advantage of the printing press to reach every corner of the kingdom. The Avis Dynasty also sponsored works of architecture like the Mosteiro da Batalha (literally, the Monastery of the Battle) and led to the creation of the manueline style of architecture in the 16th century.\n\nPortugal also spearheaded European exploration of the world and the Age of Discovery. Prince Henry the Navigator, son of King John I of Portugal, became the main sponsor and patron of this endeavour. During this period, Portugal explored the Atlantic Ocean, discovering the Atlantic archipelagos the Azores, Madeira, and Cape Verde; explored the African coast; colonized selected areas of Africa; discovered an eastern route to India via the Cape of Good Hope; discovered Brazil, explored the Indian Ocean, established trading routes throughout most of southern Asia; and sent the first direct European maritime trade and diplomatic missions to China and Japan.\n\nIn 1415, Portugal acquired the first of its overseas colonies by conquering Ceuta, the first prosperous Islamic trade centre in North Africa. There followed the first discoveries in the Atlantic: Madeira and the Azores, which led to the first colonization movements.\n\nIn 1422, by decree of King John I, Portugal officially abandoned the previous dating system, the Era of Caesar, and adopted the Anno Domini system, therefore becoming the last catholic realm to do so.\n\n\nThroughout the 15th century, Portuguese explorers sailed the coast of Africa, establishing trading posts for several common types of tradable commodities at the time, ranging from gold to slaves, as they looked for a route to India and its spices, which were coveted in Europe.\n\nThe Treaty of Tordesillas, intended to resolve the dispute that had been created following the return of Christopher Columbus, was made by Pope Alexander VI, the mediator between Portugal and Spain. It was signed on 7 June 1494, and divided the newly discovered lands outside Europe between the two countries along a meridian 370 leagues west of the Cape Verde islands (off the west coast of Africa).\n\n\nIn 1498, Vasco da Gama accomplished what Columbus set out to do and became the first European to reach India by sea, bringing economic prosperity to Portugal and its population of 1.7 million residents, and helping to start the Portuguese Renaissance. In 1500, the Portuguese explorer Gaspar Corte-Real reached what is now Canada and founded the town of Portugal Cove-St. Philip's, Newfoundland and Labrador, long before the French and English in the 17th century, and being just one of many Portuguese colonizations of the Americas.\n\nIn 1500, Pedro Álvares Cabral discovered Brazil and claimed it for Portugal. Ten years later, Afonso de Albuquerque conquered Goa in India, Muscat and Ormuz in the Persian Strait, and Malacca, now a state in Malaysia. Thus, the Portuguese empire held dominion over commerce in the Indian Ocean and South Atlantic. Portuguese sailors set out to reach Eastern Asia by sailing eastward from Europe, landing in such places as Taiwan, Japan, the island of Timor, and in the Moluccas.\n\nAlthough for a long period it was believed the Dutch were the first Europeans to arrive in Australia, there is also some evidence that the Portuguese may have discovered Australia in 1521. From 1519 to 1522, Ferdinand Magellan (Fernão de Magalhães) organized a Spanish expedition to the East Indies which resulted in the first circumnavigation of the globe. Magellan never made it back to Europe as he was killed by natives in the Philippines in 1521.\n\nThe Treaty of Zaragoza, signed on 22 April 1529 between Portugal and Spain, specified the anti-meridian to the line of demarcation specified in the Treaty of Tordesillas.\n\nAll these factors made Portugal one of the world's major economic, military, and political powers from the 15th century until the late 16th century.",
            "output": "After Christopher Columbus returned, a dispute arose between Portugal and Spain, which Pope Alexander VI mediated for the Treaty of Tordesillas.\nAccording to this joint treaty, the newly discovered lands beyond Europe are divided between the two countries along a meridian located 370 leagues west of the Cape Verde islands.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1475088596343994,
            "normalized": 0.7564311027526855
        },
        "potential_entropy": {
            "strategy_component": 0.43999999999999995,
            "strategy_details": {
                "raw": 0.8799999999999999,
                "normalized": 0.8799999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 421,
        "original_data": {
            "instruction": "What are some interesting facts around Wilmer McLean's residencies when it comes to the American Civil War?",
            "input": "Wilmer McLean (May 3, 1814 – June 5, 1882) was an American wholesale grocer from Virginia. His house, near Manassas, Virginia, was involved in the First Battle of Bull Run in 1861. After the battle, he moved to Appomattox, Virginia, to escape the war, thinking that it would be safe. Instead, in 1865, General Robert E. Lee surrendered to Ulysses S. Grant in McLean's house in Appomattox. His houses were, therefore, involved in one of the first and one of the last encounters of the American Civil War.",
            "output": "He lived near Manassas, Virginia, where one for the first battles of the Civil War was fought, and then moved to Appomattox, also in Virginia, on this later home General Robert E. Lee surrendered to Ulysses S. Grant, near the end of the Civil War.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.156749725341797,
            "normalized": 0.7590713500976562
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 422,
        "original_data": {
            "instruction": "here is a short history of Plainview – Old Bethpage John F. Kennedy High School, which color was changed when the name changed in 1991?",
            "input": "The school was opened in 1966 under the name John F. Kennedy High School. It was Plainview's second high school. Originally, all senior high school students went to Plainview High School, but were split by residential location when JFK High School opened. In 1991, Plainview-Old Bethpage High School and John F. Kennedy High School were merged into the John F. Kennedy High School building. The school was renamed Plainview-Old Bethpage John F. Kennedy High School. The newly merged school abandoned Plainview High School's colors of green and white in favor of John F. Kennedy's blue and white. Both the original Plainview High School mascot, the Gulls, and the John F. Kennedy High School mascot, the Eagles, were replaced with a new mascot, The Hawks. It is now the only public high school for residents of the community of Plainview and Old Bethpage.",
            "output": "The school colors swapped out green for blue and white stayed the same.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.313035011291504,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 423,
        "original_data": {
            "instruction": "Give me a list of the subjects I need to learn to become a telecommunication engineer",
            "input": "",
            "output": "The list of subjects you need to learn depends on the specific task and the level of the job, but in general the following list can be a good starting point: \n-- signals and systems\n-- probability\n-- linear algebra",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.8245108127593994,
            "normalized": 0.6641459465026855
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 424,
        "original_data": {
            "instruction": "What are some things to do in Rochester, NY?",
            "input": "",
            "output": "- Eat a Garbage Plate, a local specialty\n- Visit the Eastman House, the world's oldest museum dedicated to photography\n- Spend a day at the Museum of Play, a museum dedicated to exploration and play\n- See the Lamberton Conservatory, a botanical garden with walking trails\n- Watch a Red Wings game, Rochester's minor league baseball team",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.6872286796569824,
            "normalized": 0.9106367656162807
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 425,
        "original_data": {
            "instruction": "Given this text about speakers of varieties of Louisiana French, has the number of French speakers in Texas increased or decreased since the middle of the twentieth century?",
            "input": "Reliable counts of speakers of Louisiana French are difficult to obtain as distinct from other varieties of French. However, the vast majority of native residents of Louisiana and east and southeast Texas who speak French are likely speakers of Louisiana French.\n\nIn Louisiana, as of 2010, the population of French speakers was approximately 115,183. These populations were concentrated most heavily in the southern, coastal parishes.\n\nIn Texas, as of 2010, the French-speaking population was 55,773, though many of these were likely immigrants from France and other locations, living in the urban areas. Nevertheless, in the rural eastern/southeastern Texas counties of Orange, Jefferson, Chambers, Newton, Jasper, Tyler, Liberty, and Hardin alone—areas where it can be reasonably presumed that almost all French speakers are Louisiana French speakers—the total French-speaking population was composed of 3,400 individuals. It is likely a substantial portion of the 14,493 speakers in Houston's Harris county are also Louisiana French speakers. With this in mind, a marked decline in the number of French speakers in Texas has been noticed in the last half of the twentieth century. For example, at one point[when?] the French-speaking population of Jefferson County was 24,049 as compared to the mere 1,922 today. Likewise, in Harris County the French-speaking population has shifted from 26,796 to 14,493 individuals.\n\nLouisiana French-speaking populations can also be found in southern Mississippi and Alabama, as well as pockets in other parts of the United States.",
            "output": "It has generally decreased.  For example, in the second half of the twentieth century, the French-speaking population of Jefferson County was 24,049 as compared to 1,922 today. Similarly, in Harris County the French-speaking population has shifted from 26,796 to 14,493.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8334689140319824,
            "normalized": 0.6667054040091378
        },
        "potential_entropy": {
            "strategy_component": 0.3325,
            "strategy_details": {
                "raw": 0.665,
                "normalized": 0.665,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 426,
        "original_data": {
            "instruction": "give me a list of counties in Virginia by population",
            "input": "",
            "output": "Fairfax County, Prince William County, Virginia Beach county, Loudoun County, Chesterfield County, Henrico County, Chesapeake County, Norfolk County, Arlington County, Richmond County",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.5,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.5,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.9757678508758545,
            "normalized": 0.707362243107387
        },
        "potential_entropy": {
            "strategy_component": 0.725,
            "strategy_details": {
                "raw": 1.45,
                "normalized": 1.45,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.5,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 427,
        "original_data": {
            "instruction": "Who is Juliette Roche?",
            "input": "Juliette Roche (1884–1980), also known as Juliette Roche Gleizes, was a French painter and writer who associated with members of the Cubist and Dada movements. She was married to the artist Albert Gleizes.\n\nShe was born in 1884 to a wealthy Parisian family. Her father, Jules Roche, was a prominent member of both the French government and avant-garde art world. Other strong connections to the art world were manifested in her relationships with her godmother, Élisabeth, Countess Greffulhe, and her father's godson, Jean Cocteau. Juliette Roche studied painting at the Académie Ranson in Paris, with the support of her father. There, she was introduced to the artistic style of Les Nabis. In her poetic and pictorial work she showed profiles of independent women capable of self-expression.\n\nIn 1913, she exhibited at the Salon des Indépendants and began writing poetry, inserting phrases, such as advertising slogans; experimenting with typographic elements. In 1914 she held her first solo exhibition at the Bernheim-Jeune gallery.\n\nWhen the First World War broke out, she traveled to New York City with her soon to be husband, the Cubist artist Albert Gleizes, who she met through the intermediary of Ricciotto Canudo, a film theoretician who published an avant-garde magazine Montjoie!, promoting Cubism. Juliette Roche and Albert Gleizes were married in September 1915.\n\nIn New York, she took part in Dada activities with Marcel Duchamp and Francis Picabia, The Gleizes' then traveled to Barcelona to exhibit in the Galeries Dalmau before returning to New York. collaborating with Duchamp in the preparation of the first exhibition of the Society of Independent Artists of 1917, and Duchamp submitted his infamous readymade Fountain.\n\nIn 1919, she returned to Paris and began writing La minéralisation de Dudley Craving Mac Adam, published in 1924, a story that tells of the adventures of Ather Cravan and other artists in exile in New York.\n\nIn 1920-21, she wrote État... Colloidal, published by the Chilean journalist Vicente Huidobro in the magazine Creación.\n\nIn 1927, together with Albert Gleizes, they founded the Moly-Sabata , a residence of artists in Sablons, which offered studios and workshops. She continued to exhibit the rest of his life in group exhibitions.",
            "output": "Juliette Roche was born in 1884 and was a French painter and writer. She was married to the artist Albert Gleizes.\nShe was also the founder of Moly-Sabata, a residence of artists in Sablons.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.6,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.45
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.4,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.55
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.62937068939209,
            "normalized": 0.8941059112548828
        },
        "potential_entropy": {
            "strategy_component": 0.5325,
            "strategy_details": {
                "raw": 1.065,
                "normalized": 1.065,
                "components": {
                    "instruction": 0.3,
                    "input": 0.15000000000000002,
                    "output": 0.6149999999999999
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.4,
                    "out_div": 0.6,
                    "out_dens": 0.5,
                    "out_bg": 0.55
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 428,
        "original_data": {
            "instruction": "What is IFSC?",
            "input": "",
            "output": "The Indian Financial System Code (IFS Code or IFSC) is an alphanumeric code that facilitates electronic funds transfer in India. A code uniquely identifies each bank branch participating in the three main Payment and settlement systems in India: the National Electronic Funds Transfer (NEFT), Real Time Gross Settlement (RTGS) and Immediate Payment Service (IMPS) systems.\n\nThe IFSC is an 11-character code with the first four alphabetic characters representing the bank name, and the last six characters (usually numeric, but can be alphabetic) representing the branch. The fifth character is 0 (zero) and reserved for future use. Bank IFS Code is used by the NEFT & RTGS systems to route the messages to the destination banks/branches.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 1.0,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.0,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 2.4491279125213623,
            "normalized": 0.5568936892918178
        },
        "potential_entropy": {
            "strategy_component": 1.1724999999999999,
            "strategy_details": {
                "raw": 2.3449999999999998,
                "normalized": 2.3449999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.105
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.0,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 429,
        "original_data": {
            "instruction": "Show me the current P4P boxer ratings in the world",
            "input": "",
            "output": "Here's a list of P4P boxing rankings as of March 23\n1 Saul Alvarez \n2 Dmitrii Bivol \n3 Terence Crawford \n4 Naoya Inoue \n5 Gennadiy Golovkin \n6 Shakur Stevenson \n7 Errol Spence Jr \n8 Stephen Fulton \n9 Brandon Figueroa \n10 Tyson Fury",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.463817596435547,
            "normalized": 0.8468050275530133
        },
        "potential_entropy": {
            "strategy_component": 0.6050000000000001,
            "strategy_details": {
                "raw": 1.2100000000000002,
                "normalized": 1.2100000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 430,
        "original_data": {
            "instruction": "What is Python?",
            "input": "",
            "output": "Python is a high-level, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation via the off-side rule.\n\nPython is dynamically typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a \"batteries included\" language due to its comprehensive standard library.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 2.2548484802246094,
            "normalized": 0.5013852800641742
        },
        "potential_entropy": {
            "strategy_component": 1.18,
            "strategy_details": {
                "raw": 2.36,
                "normalized": 2.36,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 431,
        "original_data": {
            "instruction": "How many member states does the European Union have?",
            "input": "The European Union (EU) is a supranational political and economic union of 27 member states that are located primarily in Europe. The union has a total area of 4,233,255.3 km2 (1,634,469.0 sq mi) and an estimated total population of nearly 447 million. The EU has often been described as a sui generis political entity (without precedent or comparison) combining the characteristics of both a federation and a confederation.",
            "output": "27",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 8.4689302444458,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 432,
        "original_data": {
            "instruction": "Classify the following numbers into natural, rational, irrational, integer, and complex: \n\n1, 0, -10, pi, e, root(25), I*pi, root(2), 6/7, 5/6",
            "input": "",
            "output": "Natural: 1\nRational: 1, 0, -10, root(25), 6/7, 5/6\nIrrational: pi, e, root(2)\nComplex: 1, 0, -10, pi, e, root(25), I*pi, root(2), 6/7, 5/6",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.0297694206237793,
            "normalized": 0.4370769773210798
        },
        "potential_entropy": {
            "strategy_component": 0.39,
            "strategy_details": {
                "raw": 0.78,
                "normalized": 0.78,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.44999999999999996,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 433,
        "original_data": {
            "instruction": "Where is Muckle Water?",
            "input": "Muckle Water is a long, narrow fresh water loch on Ward Hill on Rousay, Orkney, Scotland. It is the biggest loch on the island and is popular for fishing. It can be reached by a track from the roadside. The Suso Burn on the north eastern shore drains the loch into the Sound of Rousay.",
            "output": "Muckle water is located in Rousay, Orkney, Scotland.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.050277233123779,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4025,
            "strategy_details": {
                "raw": 0.805,
                "normalized": 0.805,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.4,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 434,
        "original_data": {
            "instruction": "What is does the stars represent on the United States of America flag?",
            "input": "The national flag of the United States of America (often referred to as the American flag, or the U.S. flag) consists of thirteen equal horizontal stripes of red (top and bottom) alternating with white, with a blue rectangle in the canton (referred to specifically as the \"union\") bearing fifty small, white, five-pointed stars arranged in nine offset horizontal rows, where rows of six stars (top and bottom) alternate with rows of five stars. The 50 stars on the flag represent the 50 U.S. states, and the 13 stripes represent the thirteen British colonies that declared independence from Great Britain, and became the first states in the U.S. Nicknames for the flag include the Stars and Stripes, Old Glory, and the Star-Spangled Banner.",
            "output": "The 50 stars on the United States of America flag represent the 50 U.S. states.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.703845739364624,
            "normalized": 0.6296702112470355
        },
        "potential_entropy": {
            "strategy_component": 0.35,
            "strategy_details": {
                "raw": 0.7,
                "normalized": 0.7,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 435,
        "original_data": {
            "instruction": "Identify which car manufacturer is German or American: RAM, Artega",
            "input": "",
            "output": "Artega is German, RAM is American",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 6.406636714935303,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.025,
            "strategy_details": {
                "raw": 2.05,
                "normalized": 2.05,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 436,
        "original_data": {
            "instruction": "What math classes are offered at high schools in the United States?",
            "input": "",
            "output": "Math classes that are offered at high schools within the United States are Algebra 1, Algebra 2, Geometry, Statistics, Pre-Calculus, and Calculus.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.44466495513916,
            "normalized": 0.5556185586111886
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 437,
        "original_data": {
            "instruction": "What team has won the most Stanley Cups?",
            "input": "",
            "output": "The Montreal Canadiens have won the most Stanley Cups with 24.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.2949635982513428,
            "normalized": 0.7985610280718122
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 438,
        "original_data": {
            "instruction": "Which version of the church existed in 1809?",
            "input": "The area was settled by Canary Islanders and Acadians from 1779 on and the parish was formed in 1793 with its first church being \"little more than a shack\". It was replaced by a more permanent one in 1819, which in turn was replaced by this one in 1856, quite close to the site of the 1793 church. The parish's governing \"Fabrique\", or council of wardens, reportedly appointed a committee to measure the Catholic church in Thibodaux, Louisiana, and",
            "output": "The original version of the church still existed in 1809, it was 10 years later that a more permanent one replaced it.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.4670825004577637,
            "normalized": 0.8477378572736468
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 439,
        "original_data": {
            "instruction": "List five titles by Stephanie Meyer.",
            "input": "",
            "output": "Twilight, Midnight Sun, The Host, The Chemist, Eclipse",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.8,
            "s_out_div": 1.0,
            "s_out_dens": 1.0,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.0,
            "s_out_dens": 0.0,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            1,
            0
        ],
        "entropy": {
            "raw": 5.720299243927002,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.075,
            "strategy_details": {
                "raw": 2.15,
                "normalized": 2.15,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 2.0,
                    "output": 0.08999999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.0,
                    "out_dens": 0.0,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 440,
        "original_data": {
            "instruction": "are these seas or oceans?\natlantic\nindian\ncaspian\nred\narabian\npacific\nbaltic\nyellow\nsouthern",
            "input": "",
            "output": "seas: caspian, red, arabian, baltic, yellow, southern\noceans: atlantic, indian, pacific",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.136382579803467,
            "normalized": 0.7532521656581334
        },
        "potential_entropy": {
            "strategy_component": 0.5974999999999999,
            "strategy_details": {
                "raw": 1.1949999999999998,
                "normalized": 1.1949999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 441,
        "original_data": {
            "instruction": "Given a reference text about the Build Back Better Plan, what are the 3 parts that make up the plan?",
            "input": "The Build Back Better Plan or Build Back Better agenda was a legislative framework proposed by U.S. president Joe Biden between 2020 and 2021. Generally viewed as ambitious in size and scope, it sought the largest nationwide public investment in social, infrastructural, and environmental programs since the 1930s Great Depression-era policies of the New Deal.\n\nThe Build Back Better plan was divided into three parts:\n\nAmerican Rescue Plan (ARP), a COVID-19 pandemic-relief bill;\nAmerican Jobs Plan (AJP), a proposal to address long-neglected infrastructure needs and reduce America's contributions to destructive effects of climate change; and\nAmerican Families Plan (AFP), a proposal to fund a variety of social policy initiatives, some of which (e.g., paid family leave) had never before been enacted nationally in the U.S.\nThe first part was passed as the $1.9 trillion American Rescue Plan Act of 2021, and was signed into law in March 2021. The other two parts were reworked into different bills over the course of extensive negotiations. Aspects of the AJP's infrastructure goals were diverted into the Infrastructure Investment and Jobs Act, which was signed into law in November 2021. Other AJP priorities (e.g., climate change remediation, home health care reform, etc.) were then merged with the AFP to form the Build Back Better Act. The bill passed the Democratic-controlled House of Representatives but struggled to gain the support of Democrats Joe Manchin of West Virginia and Kyrsten Sinema of Arizona in the evenly divided Senate, with unified Republican opposition. Manchin and Sinema negotiated the reduction of Build Back Better Act's size, scope, and cost significantly with Biden and Democratic congressional leaders, but Manchin, widely viewed as the key swing vote needed to pass the bill in the Senate, ultimately rejected it over the procedural tactics used. Continued negotiations between Manchin and Senate Majority Leader Chuck Schumer eventually resulted in the Inflation Reduction Act of 2022, which was signed into law in August 2022, and incorporated some of the Build Back Better Act's climate change, healthcare, and tax reform proposals while excluding its social safety net proposals.",
            "output": "The three components of the Build Back Better plan are:\n- American Rescue Plan\n- American Jobs Plan\n- American Families Plan",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5525319576263428,
            "normalized": 0.8721519878932408
        },
        "potential_entropy": {
            "strategy_component": 0.29000000000000004,
            "strategy_details": {
                "raw": 0.5800000000000001,
                "normalized": 0.5800000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 442,
        "original_data": {
            "instruction": "Classify each of the following as either a classical or jazz artist",
            "input": "",
            "output": "Richard Wagner, Miles Davis, Ella Fitzgerald, Frederic Chopin, Amadeus Mozart, Dave Brubeck, Claude Debussy, Thelonious Monk, Robert Schumann, Louis Armstrong",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.8444812297821045,
            "normalized": 0.6698517799377441
        },
        "potential_entropy": {
            "strategy_component": 0.265,
            "strategy_details": {
                "raw": 0.53,
                "normalized": 0.53,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 443,
        "original_data": {
            "instruction": "Tell me a few highlights about Madonna's impact on pop music.",
            "input": "Madonna Louise Ciccone (born August 16, 1958) is an American singer, songwriter, and actress. Dubbed the \"Queen of Pop\", Madonna has been widely recognized for her continual reinvention and versatility in music production, songwriting, and visual presentation. She has pushed the boundaries of artistic expression in mainstream music while continuing to maintain control over every aspect of her career. Her works, which incorporate social, political, sexual, and religious themes, have generated both controversy and critical acclaim. A prominent cultural figure of the 20th and 21st centuries, Madonna remains one of the most \"well-documented figures of the modern age\", with a broad amount of scholarly reviews and literature works on her, as well as an academic mini subdiscipline devoted to her named Madonna studies.\n\nWith sales of over 300 million records worldwide, Madonna is the best-selling female recording artist of all time. She is the most successful solo artist in the history of the U.S. Billboard Hot 100 chart and has achieved the most number-one singles by a woman in Australia, Canada, Italy, Spain, and the United Kingdom. With a revenue of over U.S. $1.5 billion from her concert tickets, she remains the highest-grossing female touring artist worldwide. Forbes has named Madonna the annual top-earning female musician a record 11 times across four decades (1980s–2010s). She was inducted into the Rock and Roll Hall of Fame in 2008, her first year of eligibility. Madonna was ranked as the greatest woman in music by VH1, and as the greatest music video artist ever by MTV and Billboard. Rolling Stone also listed her among its greatest artists and greatest songwriters of all time.",
            "output": "Madonna is known worldwide as the top-selling female pop artist of all time. She has enjoyed global acclaim -- and criticism -- for her sometimes provocative work spanning over four decades. Her work includes recorded music, live performance, and several film credits.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.4155943393707275,
            "normalized": 0.8330269541059222
        },
        "potential_entropy": {
            "strategy_component": 0.32500000000000007,
            "strategy_details": {
                "raw": 0.6500000000000001,
                "normalized": 0.6500000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 444,
        "original_data": {
            "instruction": "Identify which animal species is alive or extinct: Saber-Toothed Tiger, Mountain Plover",
            "input": "",
            "output": "Mountain Plover is alive, Saber-Toothed Tiger is extinct.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.85,
            "s_out_div": 0.5,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.5,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 5.6324238777160645,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.8825,
            "strategy_details": {
                "raw": 1.765,
                "normalized": 1.765,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 1.2999999999999998,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.5,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 445,
        "original_data": {
            "instruction": "When do schools close and reopen from summer holidays in India?",
            "input": "Schools and universities typically have a summer break to take advantage of the warmer weather and longer days. In almost all countries, children are out of school during this time of year for summer break, although dates vary. Many families will take holidays for a week or two over summer, particularly in Southern Hemisphere Western countries with statutory Christmas and New Year holidays.\nIn the United States, public schools usually end in late May in Memorial Day weekend, while colleges finish in early May. Public school traditionally resumes near Labor Day, while higher institutions often resume in mid-August.\nIn England and Wales, school ends in mid-July and resumes again in early September. In Scotland, the summer holiday begins in late June and ends in mid-to late-August. Similarly, in Canada the summer holiday starts on the last or second-last Friday in June and ends in late August or on the first Tuesday of September, with the exception of when that date falls before Labour Day, in which case, ends on the second Tuesday of the month. In Russia the summer holiday begins at the end of May and ends on 31 August.\nIn the Southern Hemisphere, school summer holiday dates include the major holidays of Christmas and New Year's Day. School summer holidays in Australia, New Zealand and South Africa begin in early December and end in early February, with dates varying between states. In South Africa, the new school year usually starts during the second week of January, thus aligning the academic year with the Calendar year. In India, school ends in late April and resumes in early or mid-June. In Cameroon and Nigeria, schools usually finish for summer vacation in mid-July and resume in the later weeks of September or the first week of October.",
            "output": "In India, school ends in late April and resumes in early or mid-June.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.2,
            "s_out_dens": 0.85,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.8,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.516831874847412,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6950000000000001,
            "strategy_details": {
                "raw": 1.3900000000000001,
                "normalized": 1.3900000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.57
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.4,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.8,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 446,
        "original_data": {
            "instruction": "What are some common types of high heel shoes?",
            "input": "",
            "output": "Common types of high heel shoes include the stiletto heel, block heel, flare heel, kitten heel, and wedge heel.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.160099744796753,
            "normalized": 0.760028498513358
        },
        "potential_entropy": {
            "strategy_component": 1.105,
            "strategy_details": {
                "raw": 2.21,
                "normalized": 2.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 447,
        "original_data": {
            "instruction": "What are the essential skills required for a communication?",
            "input": "",
            "output": "Effective communication is the process of exchanging ideas, thoughts, opinions, knowledge, and data so that the message is received and understood with clarity and purpose. When we communicate effectively, both the sender and receiver feel satisfied. The most essential skill required is listening during a conversation. The other skills include being humble and understanding others opinions. Effective communication also requires you to help others understand in simplest ways possible about your points and views where the receiver should feel satisfied.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.3150393962860107,
            "normalized": 0.8042969703674316
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 448,
        "original_data": {
            "instruction": "Name a few Bhakti poets from the passage.",
            "input": "The Bhakti movement of late medieval Hinduism started in the 7th century, but rapidly expanded after the 12th century. It was supported by the Puranic literature such as the Bhagavata Purana, poetic works, as well as many scholarly bhasyas and samhitas.This period saw the growth of Vashnavism Sampradayas (denominations or communities) under the influence of scholars such as Ramanujacharya, Vedanta Desika, Madhvacharya and Vallabhacharya. Bhakti poets or teachers such as Manavala Mamunigal, Namdev, Ramananda, Sankardev, Surdas, Tulsidas, Eknath, Tyagaraja, Chaitanya Mahaprabhu and many others influenced the expansion of Vaishnavism.Even Meera (princess of Mehwar and Rajasthan) took part in this specific movement. These Vaishnavism sampradaya founders rejected Shankara's doctrines of Advaita Vedanta, particularly Ramanuja in the 12th century, Vedanta Desika and Madhva in the 13th, building their theology on the devotional tradition of the Alvars (Sri Vaishnavas).In North and Eastern India, Vaishnavism gave rise to various late Medieval movements Ramananda in the 14th century, Sankaradeva in the 15th and Vallabha and Chaitanya in the 16th century. Historically, it was Chaitanya Mahaprabhu who founded congregational chanting of holy names of Krishna in the early 16th century after becoming a sannyasi.",
            "output": "Manavala Mamunigal, Namdev, Ramananda, Sankardev, Surdas, Tulsidas, Eknath, Tyagaraja, Chaitanya Mahaprabhu",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.985677480697632,
            "normalized": 0.9959078516278949
        },
        "potential_entropy": {
            "strategy_component": 0.29500000000000004,
            "strategy_details": {
                "raw": 0.5900000000000001,
                "normalized": 0.5900000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 449,
        "original_data": {
            "instruction": "What is the most important element of swimming breaststroke?",
            "input": "",
            "output": "Timing is the most important aspect of swimming breaststroke. The swimmer needs to make sure to do the leg stroke (kicking) at a moment when the body is streamlined and presents the least resistance to the water. To achieve that, the arm stroke should be finished or almost finished before kicking begins. After kicking it is important to keep the streamlined position for some time to maintain the speed through gliding. The exact duration of gliding depends on the target distance of a given event or race. The longer the distance of the race is, the longer the gliding phase should be. For example, for a 50m race the gliding phase could be below 1 second, while for a 200m race it may be over 2 seconds.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.517777442932129,
            "normalized": 0.5765078408377511
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 450,
        "original_data": {
            "instruction": "Test",
            "input": "",
            "output": "Mike",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.3,
            "s_out_div": 0.1,
            "s_out_dens": 0.4,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.6,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": NaN,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.42,
            "strategy_details": {
                "raw": 2.84,
                "normalized": 2.84,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.8999999999999999
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.7,
                    "out_div": 0.9,
                    "out_dens": 0.6,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 451,
        "original_data": {
            "instruction": "Given a reference text about Qamus, when was it attacked and defeated?",
            "input": "Qamus (القموص) was one of the fortresses of the Jewish poet Al-Rabi ibn Abu al-Huqayq, and his Jewish tribe called Banu Nadir. The fortress was situated near Khaybar in what is now Saudi Arabia. The fortress was attacked by Muslim forces and defeated circa 629/30 CE. It was after this event that Muhammad married Safiyyah bint Huyayy. The Jewish presence in the region has been attested to the late seventh century who pioneered the cultivation in the area.\n\nIt has traditionally been identified with the remains of the Husn al-Qamus (Qamus Fortress) in the old Khaybar oasis, though there's no direct archaeological evidence for this.",
            "output": "Qamus was attacked and defeated by Muslim forced near the year 630 CE",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.549430847167969,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 452,
        "original_data": {
            "instruction": "Who wrote Gone with the Wind",
            "input": "",
            "output": "Margaret Mitchell",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.869640827178955,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.85,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 453,
        "original_data": {
            "instruction": "What are some ways that I can conserve water at home?",
            "input": "",
            "output": "Some ways you can conserve water at home include limiting or eliminating taking baths, installing high pressure shower heads in your shower, and reducing toilet flushing. Some more creative ways include reusing water you use to rinse your vegetables to water houseplants and replacing your lawn with drought-tolerant plants.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.564882755279541,
            "normalized": 0.875680787222726
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 454,
        "original_data": {
            "instruction": "Can you tell me about different types of hot water kettles?",
            "input": "",
            "output": "I'd be happy to tell you about hot water kettles. Hot water kettles are used to heat up water for beverages like tea or coffee. There are a few different kinds:\n - Stove-top kettles, which are placed on a stove and make a sound when the water is boiling.\n - Electric water kettles, which heat up using electricity rather than a stove. These can have many controls and contain thermostats to control the target water heat.\n - Gooseneck kettles, which can be electric or stovetop but feature a thin spout which is useful for making pour-over coffee.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.402419090270996,
            "normalized": 0.5435483115059989
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 455,
        "original_data": {
            "instruction": "How did Robert Baratheon die?",
            "input": "",
            "output": "King Robert I Baratheon was on a boar hunting trip when he was gored by a boar. Lancel Lannister, one of his squires, provided the king with fortified wine, to ensure the king was overly drunk and would make a mistake. Had he not died from the boar, the king would have been deliberately knocked off his horse or shot with an arrow, according to Lord Varys. The king's death was arranged by his wife, Cersei Lannister, so he would not learn the truth about who actually sired his children he shared with Cersei.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.393037796020508,
            "normalized": 0.8265822274344308
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 456,
        "original_data": {
            "instruction": "What is the PEDALS method",
            "input": "",
            "output": "PEDALS is a mnemonic to help in system design interviews, which stands for Process, Estimate, Design, Articulate Data Model, List Architecture and Scale",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.407556533813477,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 457,
        "original_data": {
            "instruction": "Who is the greatest tennis player of all time?",
            "input": "",
            "output": "Rafael Nadal",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.3,
            "s_out_div": 0.2,
            "s_out_dens": 0.15,
            "s_out_bg": 0.25
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 4.845025062561035,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.495,
            "strategy_details": {
                "raw": 2.99,
                "normalized": 2.99,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.7000000000000002,
                    "output": 0.9299999999999999
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.7,
                    "out_div": 0.8,
                    "out_dens": 0.85,
                    "out_bg": 0.75
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 458,
        "original_data": {
            "instruction": "Classify these dishes into the cuisines that they belong to.\nPanipuri, Risotto, Butter Chicken, Lasagna, Naan, Ravioli, Jalebi",
            "input": "",
            "output": "Indian - Panipuri, Butter Chicken, Naan, Jalebi\nItalian - Risotto, Lasagna, Ravioli",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.790252923965454,
            "normalized": 0.9400722639901298
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 459,
        "original_data": {
            "instruction": "Please extract Sinclair's awards from this paragraph and list them in bullets in order from oldest to most recent.",
            "input": "Sinclair has received numerous awards for his research, including the Irving S. Wright Award of Distinction from the American Federation for Aging Research in 2018, the Advance Award in Life Sciences from the Australian government in 2017, and the Australian Society for Medical Research Medal in 2014. In 2014, Sinclair was included in Time 100 as one of the hundred most influential people in the world, and in 2018 he was included in Time magazine's 50 Most Influential People in Health Care. In 2018, Sinclair was made an officer of the Order of Australia (AO) for \"distinguished service to medical research into the biology of ageing and lifespan extension, as a geneticist and academic, to biosecurity initiatives, and as an advocate for the study of science\" (2018 Australia Day Honours).",
            "output": "-Time's 100 most influential people in the world\n-Australian Society for Medical Research Medal\n-Advance Award in Life Sciences\n-Irving S Wright Award of Distinction\n-Time magazine's 50 Most Influential People in Health Care\n-Officer of the Order of Australia",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.426218032836914,
            "normalized": 0.8360622950962612
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 460,
        "original_data": {
            "instruction": "Write a letter to a school asking for an appointment to tour their facility",
            "input": "",
            "output": "Dear Sir / Madam:\n\nWe live in Amsterdam and are parents of a bright 8 year old. We have heard wonderful things about your school and are interested in a tour. We would love to meet the admissions director and tour the facilities.\n\nCan you please let us know your availability in the coming few weeks?\n\nThank you!",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.926931619644165,
            "normalized": 0.6934090341840472
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 461,
        "original_data": {
            "instruction": "What are four different recipes that you can make with Brussels Sprouts?",
            "input": "",
            "output": "1. You can make roasted Brussels Sprouts by cooking them in the oven at 400 degrees Fahrenheit for about 20 minutes. \n2. You can make a shaved Brussels sprouts salad using raw Brussels sprouts, sunflower seeds, dried cranberries, parmesan cheese, and a lemon mustard vinaigrette.\n3. You can saute Brussels sprouts with bacon and onions.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.367558479309082,
            "normalized": 0.533588136945452
        },
        "potential_entropy": {
            "strategy_component": 0.6275,
            "strategy_details": {
                "raw": 1.255,
                "normalized": 1.255,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 462,
        "original_data": {
            "instruction": "Who is the greatest cricketer of all time.",
            "input": "",
            "output": "There are many cricketers who could lay claim to the mantle of greatest cricketer of all time. Cricket is a bat and ball game that has been played internationally for nearly 150 years. Two names are often suggested to be the greatest. Sir Donald Bradman and Sir Gary Sobers. Bradman for having a batting average that will never be beaten and Sobers  for being an all rounder who was outstanding in bowling batting and fielding\nBoth of these players would be picked in any best test team of all time.  The only caveat is that neither player excelled or played in the shorter forms of the game.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.8,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.2505340576171875,
            "normalized": 0.7858668736049107
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.7000000000000002,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 463,
        "original_data": {
            "instruction": "Based on this passage, how many Boeing 747s are required to ship 10 of ASML's best-selling EUVs?",
            "input": "After decades of development, ASML shipped the first production extreme ultraviolet lithography machine in 2013. These machines produce light in the 13.5 nm wavelength range when a high-energy laser is focused on microscopic droplets of molten tin to produce a plasma, which then emits EUV light. The light is bounced off Zeiss mirrors that are the flattest mirrors in the world, on to the surface of a silicon wafer to deliver the designs for the chip. ASML's best-selling EUV product has been the Twinscan NXE:3600D, which costs up to $200 million. Shipping the machine requires 40 shipping containers, 20 trucks and three Boeing 747s. As of 2022, ASML has shipped around 140 EUV systems, and it is the only company to manufacture them.\n\nASML is working on the next generation of EUV systems, with the first shipments to customers for R&D purposes expected to take place at the end of 2023. The platform is designated High-NA as it will increase the numerical aperture (NA) from 0.33 to 0.55, and each system is expected to cost $300 million.",
            "output": "The best-selling EUV is the Twinscan NXE:3600D, and it takes 3 Boeing 747s per EUV, so 30 Boeing 747s are required to ship ten Twinscan NXE:3600Ds.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.626415729522705,
            "normalized": 0.8932616370064872
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 464,
        "original_data": {
            "instruction": "Give me a list of 5 famous archeological sites to visit in Mexico.",
            "input": "",
            "output": "- Chichen Itza\n- Teotihuacan\n- Templo Mayor\n- Palenque\n- Tulum",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 1.710734248161316,
            "normalized": 0.3459240709032331
        },
        "potential_entropy": {
            "strategy_component": 0.6599999999999999,
            "strategy_details": {
                "raw": 1.3199999999999998,
                "normalized": 1.3199999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 465,
        "original_data": {
            "instruction": "here is some text about Andre Weiss, how many years was Andre at University of Dijon in Paris?",
            "input": "Charles André Weiss (September 30, 1858 in Mulhouse - August 31, 1928 in the Hague) was a French jurist. He was professor at the Universities of Dijon and Paris and served from 1922 until his death as judge of the Permanent Court of International Justice.\n\nLife\nAndré Weiss was born in Mulhouse in 1858 and completed a degree in law at the University of Paris in 1880. The following year he became a professor at the University of Dijon. In 1891 he moved to the Law School of the University of Paris. There he was from 1896 to 1908 a full professor of civil law, and from 1908 he held the chair for international law and private international law. From 1907 he also acted as legal advisor to the French Ministry of Foreign Affairs. He was a close associate of Secretary of State Aristide Briand and participated as a delegate to the Paris Peace Conference at the end of World War I. He led one of the subcommissions of the conference.\n\nFrom 1920, he was a member of the Permanent Court of Arbitration in The Hague. In September 1921, he was elected by the Assembly and the Council of the League of Nations to the post of judge on the newly formed Permanent International Court. His fellow judges elected him vice president at the beginning of the Tribunal's work on February 3, 1922. He was confirmed in this office in 1924 and 1927. He died before the end of his term as Judge and Vice President in August 1928 in the Hague. His compatriot, Henri Fromageot, was elected his successor, and in the office of vice-president he was succeeded by the Swiss Max Huber.\n\nAndré Weiss taught in 1923 as a lecturer at the Hague Academy of International Law. He was a member of the Institut de Droit International from 1887, acting as president of its 30th session in 1922 in Grenoble. From 1914 he was a member of the Académie des sciences morales et politiques.",
            "output": "Andre Weiss was at the University of Dijon in Paris for 10 years, 1881 to 1891, before moving to the University of Paris Law School.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.691391944885254,
            "normalized": 0.6261119842529297
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 466,
        "original_data": {
            "instruction": "Write a bulleted list of the plants that grow around Stanwell-Fletcher Lake.",
            "input": "Stanwell-Fletcher Lake is the largest lake on Somerset Island, the tenth-largest island of the Canadian Arctic Archipelago. The lake, along with most of Somerset Island, is located within the Qikiqtaaluk Region of the Canadian territory of Nunavut.\n\nThe harsh tundral climate and minimal soil cover of the highlands surrounding the lake allows only a few types of plants to grow there, mostly lichens, mosses, and arctic willow. Vegetation is more prominent in the thermokarst lowlands to the north of the lake, which are dominated by sedges during the summer. The low-lying areas to the southeast of the lake, and the nearby Creswell Bay, host a Canadian Important Bird Area. The area supports large bird populations during the summer, especially breeding populations of various shorebirds. 13 shorebird species have been observed in the area, while 11 are known to breed there. These species includes the white-rumped sandpiper, red phalarope, black-bellied plover, sanderling, American golden plover, ruddy turnstone, Baird's sandpiper, buff-breasted sandpiper, pectoral sandpiper, semipalmated sandpiper, and red knot. Other birds with populations in the area include the snow goose, king eider, long-tailed duck, northern fulmar, black-legged kittiwake, and peregrine falcon. Stanwell-Fletcher Lake is also a large and important habitat for anadromous populations of arctic char, which spend summer in coastal areas including Creswell Bay and surrounding waters, but migrate via the Union River to in the lake before it freezes over. Fourhorn sculpin are also present within the lake.",
            "output": "* lichens\n* mosses\n* arctic willow",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.95,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.2207932472229,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.47250000000000003,
            "strategy_details": {
                "raw": 0.9450000000000001,
                "normalized": 0.9450000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.44999999999999996,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 467,
        "original_data": {
            "instruction": "Where was Ostad Elahi born and who was his father",
            "input": "Elahi was born in Jeyhunabad, a small Kurdish village near the eponymous capital of Kermanshah Province. His father, Hajj Nematollah (1871–1920), was a mystic and poet who was a leader of the Ahl-e Haqq and revered as a saint. From early childhood, he led an ascetic, secluded life of rigorous discipline under his father's supervision with a special focus on mysticism, music, and ethics. In addition to religious and moral instruction, he received the classical education of the time. During study and contemplation in his youth, he established the basis of his philosophical and spiritual reflections.",
            "output": "He was born in Jeyhunabad, a small Kurdish village and his father was Hajj Nematollah, a mystic and poet and a leader of Ahl-e Haqq",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.0613932609558105,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4624999999999999,
            "strategy_details": {
                "raw": 0.9249999999999998,
                "normalized": 0.9249999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 468,
        "original_data": {
            "instruction": "Given this paragraph about marathon, what is the distance covered in kilometres and what are the most common ways to complete one?",
            "input": "The marathon is a long-distance foot race with a distance of 42.195 km (26 mi 385 yd), usually run as a road race, but the distance can be covered on trail routes. The marathon can be completed by running or with a run/walk strategy. There are also wheelchair divisions. More than 800 marathons are held throughout the world each year, with the vast majority of competitors being recreational athletes, as larger marathons can have tens of thousands of participants.",
            "output": "The distance covered in a marathon is 42.195 kilometers and it can be completed by running and walking combined, though there are also possibilities for wheelchair participants.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.4079456329345703,
            "normalized": 0.8308416094098773
        },
        "potential_entropy": {
            "strategy_component": 0.335,
            "strategy_details": {
                "raw": 0.67,
                "normalized": 0.67,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.4,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 469,
        "original_data": {
            "instruction": "Write some points about Ministry of Public Health (Thailand) using given text as a reference",
            "input": "In Thailand before 1888 there were no permanent, public hospitals to provide care to sick people. Temporary hospitals were set up to care for patients during epidemics, then disbanded when the epidemic subsided. Under King Chulalongkorn (Rama V) a hospital was constructed and completed in 1888 and named \"Siriraj Hospital\" in commemoration of the king's young son, Prince Siriraj Kakudhabhand, who had died of dysentery. King Vajiravudh, King Chulalongkorn's successor, established Department of Health on 27 November 1918.\n\nDuring the reign of King Rama VIII, the Ministry of Public Health was established on 10 March 1942 as a result of the enactment of the Ministries and Departments Reorganization Act (Amendment No. 3) of B.E. 2485. Later in 1966, the date 27 November was chosen as the commemoration day of the Ministry of Public Health's foundation.",
            "output": "1. There were no permanent public hospitals in Thailand prior to 1888 to care for the ill.\n2. When an epidemic broke out, temporary hospitals were established to provide medical care for patients.\n3. A hospital was built and finished in 1888 under King Chulalongkorn (Ram V), and given the name \"Siriraj Hospital\" in honour of the king's little son, Prince Siriraj Kakudhabhand, who had passed away from dysentery.\n4. On November 27, 1918, King Vajiravudh, who succeeded King Chulalongkorn, created the Department of Health.\n5. On March 10, 1942, under King Rama VIII, the Ministries and Departments Reorganization Act (Amendment No. 3) of B.E. 2485 was passed, which led to the creation of the Ministry of Public Health.\n6. Later in 1966, the establishment of the Ministry of Public Health was commemorated on November 27.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.5101888179779053,
            "normalized": 0.5743396622794015
        },
        "potential_entropy": {
            "strategy_component": 0.298,
            "strategy_details": {
                "raw": 0.596,
                "normalized": 0.596,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.126
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 470,
        "original_data": {
            "instruction": "What do you do when the engine in your single engine airplane fails in flight?",
            "input": "",
            "output": "While it is rare every pilot should be prepared in the event.  The first step is to stay calm.  Remember you need to aviate, navigate and communicate in that order.  You should have the planes best glide speed memorized.  Next establish that glide speed.  Then consult the checklist for engine failure and restart procedures.  In event it does not restart look for suitable landing site.  If at all possible find a site that allows for a landing into the wind.  Declare an emergency, convey location if possible.  Refer to emergency landing procedures with a non-operating engine.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 4.021040439605713,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 471,
        "original_data": {
            "instruction": "Who was given the Booker Prize 2020?",
            "input": "",
            "output": "Douglas Stuart",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 10.665627479553223,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 472,
        "original_data": {
            "instruction": "Who is the greatest college football quarterback of all time?",
            "input": "",
            "output": "Different people have different opinions when ranking college football players. Both individual and team metrics are considered when determining the best quarterback. Key individual metrics include individual passing metrics (such as passer rating, number of passing yards, and number of touchdowns thrown) and individual awards won (such as the Heisman trophy). Key team metrics include the team's win percentage with the individual as the starting quarterback and the number of conference and nation championships won. A quarterback that excels at passing the football and running the football is commonly referred to as a \"dual threat\" quarterback. Individual rushing statistics may also be considered when determining their rankings. Many people consider Sam Bradford, Matt Leinart, and Peyton Manning to be among the best pure passing quarterbacks in college football history. Many people consider Cam Newton, Vince Young, and Johnny Manziel to be among the best dual threat quarterbacks in college football history. Opinions vary, but many people consider Tim Tebow to be the greatest college football quarterback of all-time due to his individual accomplishments, his team's accomplishments and revolutionary offensive play style, and his popularity amongst fans and the sports media.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.857201337814331,
            "normalized": 0.6734860965183803
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 473,
        "original_data": {
            "instruction": "Write a description of your favorite place to visit in San Francisco and why you like it so much.",
            "input": "",
            "output": "My favorite place in San Francisco is the JFK promenade in Golden Gate Park, which is a dedicated pedestrian walkway in Golden Gate park. The promenade is decorated with street paintings, art exhibits and public sitting places. It also goes near iconic parts of Golden Gate Park such as the DeYoung Museum and the Academy of Sciences. I love that it is always bustling with people enjoying the natural beauty of the park and getting to experience San Francisco like the locals.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.4252328872680664,
            "normalized": 0.8357808249337333
        },
        "potential_entropy": {
            "strategy_component": 1.2175,
            "strategy_details": {
                "raw": 2.435,
                "normalized": 2.435,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 474,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Ghatam, Monochord",
            "input": "",
            "output": "Monochord is string, Ghatam is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 6.724634170532227,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 475,
        "original_data": {
            "instruction": "What was the Bismarck battleship?",
            "input": "Bismarck was the first of two Bismarck-class battleships built for Nazi Germany's Kriegsmarine. Named after Chancellor Otto von Bismarck, the ship was laid down at the Blohm & Voss shipyard in Hamburg in July 1936 and launched in February 1939. Work was completed in August 1940, when she was commissioned into the German fleet. Bismarck and her sister ship Tirpitz were the largest battleships ever built by Germany, and two of the largest built by any European power.\n\nIn the course of the warship's eight-month career, Bismarck conducted only one offensive operation that lasted 8 days in May 1941, codenamed Rheinübung. The ship, along with the heavy cruiser Prinz Eugen, was to break into the Atlantic Ocean and raid Allied shipping from North America to Great Britain. The two ships were detected several times off Scandinavia, and British naval units were deployed to block their route. At the Battle of the Denmark Strait, the battlecruiser HMS Hood initially engaged Prinz Eugen, probably by mistake, while HMS Prince of Wales engaged Bismarck. In the ensuing battle Hood was destroyed by the combined fire of Bismarck and Prinz Eugen, which then damaged Prince of Wales and forced her retreat. Bismarck suffered sufficient damage from three hits by Prince of Wales to force an end to the raiding mission.\n\nThe destruction of Hood spurred a relentless pursuit by the Royal Navy involving dozens of warships. Two days later, heading for occupied France to effect repairs, Bismarck was attacked by fifteen Fairey Swordfish torpedo bombers from the aircraft carrier HMS Ark Royal; one scored a hit that rendered the battleship's steering gear inoperable. In her final battle the following morning, the already-crippled Bismarck was engaged by two British battleships and two heavy cruisers, and sustained incapacitating damage and heavy loss of life. The ship was scuttled to prevent her being boarded by the British, and to allow the ship to be abandoned so as to limit further casualties. Most experts agree that the battle damage would have caused her to sink eventually.\n\nThe wreck was located in June 1989 by Robert Ballard, and has since been further surveyed by several other expeditions.",
            "output": "The Bismarck was a battleship built for the German Navy in World War 2.  After one offensive engagement where it helped sink the ship HMS Hood the Bismarck became a priority target of the British Navy and was sunk by German forces after suffering catastrophic damage after only 8 months of service.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.328918218612671,
            "normalized": 0.8082623481750488
        },
        "potential_entropy": {
            "strategy_component": 0.32500000000000007,
            "strategy_details": {
                "raw": 0.6500000000000001,
                "normalized": 0.6500000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 476,
        "original_data": {
            "instruction": "Was the hermit hummingbird hawkmoth first described in the 18th century or the 19th century?",
            "input": "Macroglossum insipida, the hermit hummingbird hawkmoth, is a moth of the family Sphingidae. It was described by Arthur Gardiner Butler in 1875.",
            "output": "The hermit hummingbird hawkmoth was first described in 1875, which is in the 19th century.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 1.0,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.0,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.495413064956665,
            "normalized": 0.5701180185590472
        },
        "potential_entropy": {
            "strategy_component": 0.41250000000000003,
            "strategy_details": {
                "raw": 0.8250000000000001,
                "normalized": 0.8250000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.0,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 477,
        "original_data": {
            "instruction": "Which of the following countries are in Europe, and which are in the rest of the world: Nicaragua, Australia, Austria, New Zealand, Marocco, Spain, Greece, China, Chile?",
            "input": "",
            "output": "Austria, Spain and Greece are in Europe. Nicaragua, Australia, New Zealand, Marocco, China and Chile are in the rest of the world.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.567742109298706,
            "normalized": 0.876497745513916
        },
        "potential_entropy": {
            "strategy_component": 0.9124999999999999,
            "strategy_details": {
                "raw": 1.8249999999999997,
                "normalized": 1.8249999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 478,
        "original_data": {
            "instruction": "What is Grimsthorpe?",
            "input": "Grimsthorpe is a hamlet in the South Kesteven district of Lincolnshire, England. It is situated on the A151 road, and 3 miles (5 km) north-west from Bourne. Grimsthorpe falls within the civil parish of Edenham, which is governed by Edenham Grimsthorpe Elsthorpe & Scottlethorpe Parish Council.\n\nGrimsthorpe Castle is 500 yards (460 m) to the west.\n\nJohn Marius Wilson's 1870 Imperial Gazetteer of England and Wales described Grimsthorpe as: a hamlet in Edenham parish, Lincoln; on the river Glen, 1½ mile W of Edenham village. Pop., 135. Grimsthorpe Park was the seat once of the Duke of Ancaster, afterwards of Lord Gwyder; is now the seat of Lord Willoughby d'Eresby; was built partly in the time of Henry III., but principally by the Duke of Suffolk, to entertain Henry VIII.; is a large, irregular, but magnificent structure; and stands in an ornate park, about 16 miles in circuit. A Cistertian abbey, founded about 1451, by the Earl of Albemarle, and called Vallis Dei, or, vulgarly, Vaudy, formerly stood in the park, about a mile from the castle.",
            "output": "Grimsthorpe is a hamlet in the South Kesteven district of Lincolnshire, England. It is situated on the A151 road, and 3 miles (5 km) north-west from Bourne. Grimsthorpe falls within the civil parish of Edenham, which is governed by Edenham Grimsthorpe Elsthorpe & Scottlethorpe Parish Council.\n\nGrimsthorpe Castle is 500 yards (460 m) to the west.\n\nJohn Marius Wilson's 1870 Imperial Gazetteer of England and Wales described Grimsthorpe as:\n\na hamlet in Edenham parish, Lincoln; on the river Glen, 1½ mile W of Edenham village. Pop., 135. Grimsthorpe Park was the seat once of the Duke of Ancaster, afterwards of Lord Gwyder; is now the seat of Lord Willoughby d'Eresby; was built partly in the time of Henry III., but principally by the Duke of Suffolk, to entertain Henry VIII.; is a large, irregular, but magnificent structure; and stands in an ornate park, about 16 miles in circuit. A Cistertian abbey, founded about 1451, by the Earl of Albemarle, and called Vallis Dei, or, vulgarly, Vaudy, formerly stood in the park, about a mile from the castle.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.349050283432007,
            "normalized": 0.814014366694859
        },
        "potential_entropy": {
            "strategy_component": 0.298,
            "strategy_details": {
                "raw": 0.596,
                "normalized": 0.596,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.126
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 479,
        "original_data": {
            "instruction": "Who were the children of the legendary Garth Greenhand, the High King of the First Men in the series A Song of Ice and Fire?",
            "input": "",
            "output": "Garth the Gardener, John the Oak, Gilbert of the Vines, Brandon of the Bloody Blade, Foss the Archer, Owen Oakenshield, Harlon the Hunter, Herndon of the Horn, Bors the Breaker, Florys the Fox, Maris the Maid, Rose of the Red Lake, Ellyn Ever Sweet, Rowan Gold-Tree",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.8,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 4.72951078414917,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4625,
            "strategy_details": {
                "raw": 0.925,
                "normalized": 0.925,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.19499999999999998
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 480,
        "original_data": {
            "instruction": "I am looking at items on my desk right now, tell me which ones are actually considered office accessories.",
            "input": "",
            "output": "Trident mint, Macbook Pro, Pixel 3, Apple Airpod, Tumi backpack, Logitech Mouse, Microsoft Surface Wireless Keyboard, Samsung 22 inch flat monitor",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.524131774902344,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.865,
            "strategy_details": {
                "raw": 1.73,
                "normalized": 1.73,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.1,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.5,
                    "inp_complex": 0.6,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 481,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Ikembe, Cimbalom",
            "input": "",
            "output": "Cimbalom is string, Ikembe is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.186580657958984,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 482,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Kundu, Kabosy",
            "input": "",
            "output": "Kabosy is string, Kundu is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.980691432952881,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 483,
        "original_data": {
            "instruction": "Classify each of the following as part of the upper or lower body: arm, toe, finger, ankle, elbow, chin, chest, shin",
            "input": "",
            "output": "Arm: Upper Body\nToe: Lower Body\nFinger: Upper Body\nAnkle: Lower Body\nElbow: Upper Body\nChin: Upper Body\nChest: Upper Body\nShin: Lower Body",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 1.950297474861145,
            "normalized": 0.4143707071031843
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 484,
        "original_data": {
            "instruction": "What’s it like living in NYC compared to other US cities?",
            "input": "",
            "output": "New York City is a large metropolis and is known to be the city that never sleeps. While downtowns of typical American cities become empty during non-working hours, every area of NYC is full of people at any given time. Most people living there don’t own cars and use the subway to get around to different parts of the city.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.066397190093994,
            "normalized": 0.7332563400268555
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 485,
        "original_data": {
            "instruction": "How many colours does a rainbow have?",
            "input": "",
            "output": "A rainbow typically comprises 7 colors - red, orange, yellow, green, blue, indigo and violet, which can be remembered using acronyms like \"ROY G BIV\" or simple sentences like \"Richard Of York Gave Battle In Vain\", where the starting letter of each word corresponds to the starting letter of the colors of the rainbow, in order.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.693824529647827,
            "normalized": 0.6268070084708077
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 486,
        "original_data": {
            "instruction": "Classify each of the following objects as either heavy or lightweight: bubble, aircraft, bear, mouse, bus, raisin, strawberry, popcorn, elephant, piano, plastic cup, train, butterfly, whale, refrigerator, feather, pencil, ship, tree, flower.",
            "input": "",
            "output": "Heavy weight objects include: aircraft, bear, bus, elephant, piano, refrigerator, ship, tree, train, whale.\nLightweight objects include: bubble, butterfly, feather, flower, mouse, pencil, plastic cup, popcorn, raisin, strawberry.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.510859966278076,
            "normalized": 0.8602457046508789
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 487,
        "original_data": {
            "instruction": "What shape is the constellation of Orion?",
            "input": "",
            "output": "The constellation of Orion is also known as Orion's belt. The arrangement of it's three bright stars (Alnitak, Alnilam and Mintaka) are in a straight line and the constellation is considered to look like a hunter.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.025228261947632,
            "normalized": 0.7214937891278949
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 488,
        "original_data": {
            "instruction": "Is switchgrass a good biofuel source?",
            "input": "",
            "output": "Switchgrass is an excellent biofuel source for many reasons.  Switchgrass grows natively throughout the United States, Mexico, and Canada.  It has a dramatically higher energy yield than corn.  It requires almost no pesticides to grow and no fertilizer.  Switchgrass actually pulls carbon out of the air and puts it into the ground through it's large root system, thus enriching the soil it grows in rather than depleting it.  Once a switchgrass plant is established it grows back naturally every year for 18 to 20 years.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9099068641662598,
            "normalized": 0.6885448183332171
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 489,
        "original_data": {
            "instruction": "Given this paragraph about the history of triathlon, what is the earliest known triathlon event.",
            "input": "The evolution of triathlon as a distinct sport is difficult to trace with precision. Many, including triathlon historian and author Scott Tinley, consider events in early twentieth century France to be the beginnings of triathlon, with many three element multisport events of differing composition appearing, all called by different names. The earliest record for an event was from 1901 in Joinville-le-Pont, Val-de-Marne: it called itself \"Les Trois Sports\" (The Three Sports). It was advertised as an event for \"the sportsmen of the time\" and consisted of a run, a bicycle and a canoe segment. By 19 June 1921, the event in Joinville-le-Pont had become more like a standard triathlon, with the canoe segment being replaced with a swim. According to the newspaper L’Auto, the race consisted of a 3 km (1.9 mi) run, a 12 km (7.5 mi) bike ride and the crossing of the river Marne, all staged consecutively and without a break. Throughout the 1920s other bike, run, and swim events appeared in different cities, such as the “Course des Trois Sports” in Marseille and \"La Course des Débrouillards\" in Poissy. These multisport events would continue to slowly spread and grow in popularity: by 1934 \"Les Trois Sports\" was being hosted in the city of La Rochelle, though it consisted of three distinct events, swimming a 200 m (660 ft) channel crossing, a 10 km (6 mi) bike competition around the harbour of La Rochelle and the parc Laleu, and a 1.2 km (0.75 mi) run in the stadium André-Barbeau. Throughout this growth with new events appearing no unified rules ever existed and as a whole triathlon would remain a minority event on the world stage",
            "output": "Triathlon events have evolved of the years, and its origins are not fully known. Triathlon historians have evidenced multisports events dating back to 1901. In this era \"Les Trois Sports\" did not include the swim as it does in modern day triathlon, but instead included a canoe discipline. As the name suggest, triathlon appears to have first surfaced in the history books in france in the suburbs of paris known as Joinville-le-pont, Val-de-marne.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.221031188964844,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 490,
        "original_data": {
            "instruction": "Given a reference text about Friedrich Rudolf Ludwig Freiherr von Canitz, when and where was he born?",
            "input": "Friedrich Rudolf Ludwig Freiherr von Canitz (27 November 1654 – 11 August 1699) was a German poet and diplomat. He was one of the few German poets of his era that Frederick the Great enjoyed.\n\nBiography\nCanitz was born in Berlin, Brandenburg. He was influenced by Boileau.  He attended the universities of Leiden and Leipzig, travelled in England, France, Italy and the Netherlands, and on his return was appointed groom of the bedchamber (Kammerjunker) to Frederick William, Elector of Brandenburg, whom he accompanied on his campaigns in Pomerania and Sweden.\n\nIn 1680 Canitz became councillor of legation, and he was employed on various embassies. In 1697 Elector Frederick III made him a privy councillor, and Emperor Leopold I created him a baron of the Empire. Having fallen ill on an embassy to the Hague, he obtained his discharge and died at Berlin in 1699.\n\nHe wrote verse in a restrained, sober style.  He was considered as one of the poets who “produced verse that was specifically aimed at entertaining and flattering princes and high courtiers.” Although he liked making verse in many languages, he never published any of them. Canitz's poems (Nebenstunden unterschiedener Gedichte), which did not appear until after his death (1700), are for the most part dry and stilted imitations of French and Latin models, but they formed a healthy contrast to the coarseness and bombast of the later Silesian poets.\n\nA complete edition of Canitz's poems was published by U. König in 1727; see also L. Fulda, Die Gegner der zweiten schlesischen Schule, ii. (1883).",
            "output": "Friedrich Rudolf Ludwig Freiherr von Canitz was born in Berlin on November 27, 1654.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0583765506744385,
            "normalized": 0.7309647287641253
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 491,
        "original_data": {
            "instruction": "What is Anandapuram mandal",
            "input": "Anandapuram mandal is one of the 46 mandals in Visakhapatnam district of the Indian state of Andhra Pradesh. It is under the administration of Visakhapatnam revenue division and the headquarters are located at Anandapuram. Anandapuram mandal is bounded by Bheemunipatnam mandal to the east, Chinagadila to the south, Padmanabham to the north and Kothavalasa to the west.",
            "output": "Anandapuram mandal is one of the 46 mandals in Visakhapatnam district of the Indian state of Andhra Pradesh. It is under the administration of Visakhapatnam revenue division and the headquarters are located at Anandapuram.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.287407875061035,
            "normalized": 0.5106879643031529
        },
        "potential_entropy": {
            "strategy_component": 0.46499999999999997,
            "strategy_details": {
                "raw": 0.9299999999999999,
                "normalized": 0.9299999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.45000000000000007,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 492,
        "original_data": {
            "instruction": "Tell me whether these are European countries or Asian countries: Spain, France, Norway, the Czech Republic, Denmark, Portugal, Brazil, South Africa, Canada, China, Japan, Malaysia, Singapore, Sri Lanka, The Philippines",
            "input": "",
            "output": "Not all of them are European countries or Asian countries! Brazil, South Africa, Canada are not part of Europe or Asia. \n\nThese are the European countries: Spain, France, Norway, the Czech Republic, Denmark, Portugal\n\nAnd these are Asian countries: China, Japan, Malaysia, Singapore, Sri Lanka, The Philippines",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.774885654449463,
            "normalized": 0.6499673298427037
        },
        "potential_entropy": {
            "strategy_component": 0.5075000000000001,
            "strategy_details": {
                "raw": 1.0150000000000001,
                "normalized": 1.0150000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 493,
        "original_data": {
            "instruction": "Which of the following rivers are in India?",
            "input": "",
            "output": "Ganges, Nile, Yamuna, Indus, Thames",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.7,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.114984035491943,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.04,
            "strategy_details": {
                "raw": 2.08,
                "normalized": 2.08,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.4,
                    "out_div": 0.5,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 494,
        "original_data": {
            "instruction": "Which film owns the record for the most Oscar wins?",
            "input": "",
            "output": "Three films own the record for the most Oscar wins. They are Titanic, Ben Hur, and Lord of the Rings: Return of the King, all of which won 11 awards.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.5,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.9171013832092285,
            "normalized": 0.6906003952026367
        },
        "potential_entropy": {
            "strategy_component": 0.76,
            "strategy_details": {
                "raw": 1.52,
                "normalized": 1.52,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.1,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.5,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 495,
        "original_data": {
            "instruction": "How do you prepare for case study interview for digital transformation case?",
            "input": "",
            "output": "Here is one suggested approach. 1. Gather all requirements related to business problem 2. Understand data sources the business is using. 3. Understand if business would like to build or buy solutions 4. Check for industry specific accelerators 5. Start with one business unit and show the impact of digital transformation and then expand to other business units. 6. Do business value assessment like ROI 7. Focus on tangible and intangible benefits of digital transformation 8. Cost benefit analysis 9. Implementation plan and timelines 10. Production plan  11. Maintenance and support",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.55,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.44999999999999996,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.053673505783081,
            "normalized": 0.7296210016523089
        },
        "potential_entropy": {
            "strategy_component": 0.725,
            "strategy_details": {
                "raw": 1.45,
                "normalized": 1.45,
                "components": {
                    "instruction": 0.3,
                    "input": 0.85,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.44999999999999996,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 496,
        "original_data": {
            "instruction": "Classify the below numbers based on whether they are perfect square or not.\n64, 9, 81, 100, 57, 16",
            "input": "",
            "output": "Perfect square - 64, 9, 81, 16\nNot perfect square - 100, 57",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.6,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.4,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.5409958362579346,
            "normalized": 0.583141667502267
        },
        "potential_entropy": {
            "strategy_component": 0.47500000000000003,
            "strategy_details": {
                "raw": 0.9500000000000001,
                "normalized": 0.9500000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.4,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 497,
        "original_data": {
            "instruction": "Who were the founding members of id Software?",
            "input": "id Software LLC (/ɪd/) is an American video game developer based in Richardson, Texas. It was founded on February 1, 1991, by four members of the computer company Softdisk: programmers John Carmack and John Romero, game designer Tom Hall, and artist Adrian Carmack.",
            "output": "The founding members of id Software were John Carmack, John Romero, Tom Hall, and Adrian Carmack.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.5,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.5,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.5974557399749756,
            "normalized": 0.8849873542785645
        },
        "potential_entropy": {
            "strategy_component": 0.595,
            "strategy_details": {
                "raw": 1.19,
                "normalized": 1.19,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.5,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 498,
        "original_data": {
            "instruction": "Classify each of the following as something used for drinking or eating: cup, bottle, plate, bowl, fork, spoon, can",
            "input": "",
            "output": "Cup: Drinking\nBottle: Drinking\nPlate: Eating\nBowl: Eating\nFork: Eating\nSpoon: Eating\nCan: Drinking",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.3915305137634277,
            "normalized": 0.5404372896466937
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 499,
        "original_data": {
            "instruction": "What is the most popular sport?",
            "input": "",
            "output": "Soccer also known as football outside of USA is the most popular sport with over 3.5 billion fans. It consists of 11 players and maximum 5 substitutions during a 90 minutes regulation time allowed. The formation consists of a goalie, defense, midfield and offense. 191 countries represent a national soccer team which is more than any other nationally represented sport.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.187412977218628,
            "normalized": 0.7678322792053223
        },
        "potential_entropy": {
            "strategy_component": 1.1275000000000002,
            "strategy_details": {
                "raw": 2.2550000000000003,
                "normalized": 2.2550000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 500,
        "original_data": {
            "instruction": "Tell me whether matches in each sport is usually played individually or on a team: baseball, golf, tennis, wrestling, hockey",
            "input": "",
            "output": "Baseball: Team\nGolf: Individually\nTennis: Individually\nWrestling: Individually\nHockey: Team",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.450402021408081,
            "normalized": 0.5572577204023089
        },
        "potential_entropy": {
            "strategy_component": 0.6824999999999999,
            "strategy_details": {
                "raw": 1.3649999999999998,
                "normalized": 1.3649999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 501,
        "original_data": {
            "instruction": "Tell me if these are baseball teams or football teams: Seattle Mariners, Chicago Cubs, Tampa Bay Devil Rays",
            "input": "",
            "output": "They are all baseball teams",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.343952178955078,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.025,
            "strategy_details": {
                "raw": 2.05,
                "normalized": 2.05,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 502,
        "original_data": {
            "instruction": "What is geomorphometry?",
            "input": "",
            "output": "Geomorphometry, or geomorphometrics, is the science and practice of measuring the characteristics of terrain, the shape of the surface of the Earth, and the effects of this surface form on human and natural geography. It gathers various mathematical, statistical and image processing techniques that can be used to quantify morphological, hydrological, ecological and other aspects of a land surface. Common synonyms for geomorphometry are geomorphological analysis (after geomorphology), terrain morphometry, terrain analysis, and land surface analysis. Geomorphometrics is the discipline based on the computational measures of the geometry, topography and shape of the Earth's horizons, and their temporal change. This is a major component of geographic information systems (GIS) and other software tools for spatial analysis.\n\nIn simple terms, geomorphometry aims at extracting (land) surface parameters (morphometric, hydrological, climatic etc.) and objects (watersheds, stream networks, landforms etc.) using input digital land surface model (also known as digital elevation model, DEM) and parameterization software.[Extracted surface parameters and objects can then be used, for example, to improve mapping and modelling of soils, vegetation, land use, geomorphological and geological features and similar.\n\nWith the rapid increase of sources of DEMs today (and especially due to the Shuttle Radar Topography Mission and LIDAR-based projects), extraction of land surface parameters is becoming more and more attractive to numerous fields ranging from precision agriculture, soil-landscape modelling, climatic and hydrological applications to urban planning, education and space research. The topography of almost all Earth has been today sampled or scanned, so that DEMs are available at resolutions of 100 m or better at global scale. Land surface parameters are today successfully used for both stochastic and process-based modelling, the only remaining issue being the level of detail and vertical accuracy of the DEM.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.92,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.07999999999999996,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 2.9475371837615967,
            "normalized": 0.6992963382175991
        },
        "potential_entropy": {
            "strategy_component": 1.1425,
            "strategy_details": {
                "raw": 2.285,
                "normalized": 2.285,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.105
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.07999999999999996,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 503,
        "original_data": {
            "instruction": "What is Augusta National Golf Club?",
            "input": "Augusta National Golf Club, sometimes referred to as Augusta or the National, is a golf club in Augusta, Georgia, United States. Unlike most private clubs which operate as non-profits, Augusta National is a for-profit corporation, and it does not disclose its income, holdings, membership list, or ticket sales.\n\nFounded by Bobby Jones and Clifford Roberts, the course was designed by Jones and Alister MacKenzie and opened for play in 1932. Since 1934, the club has played host to the annual Masters Tournament, one of the four men's major championships in professional golf, and the only major played each year at the same course. It was the top-ranked course in Golf Digest's 2009 list of America's 100 greatest courses and was the number ten-ranked course based on course architecture on Golfweek Magazine's 2011 list of best classic courses in the United States.\n\nIn 2019, the course began co-hosting the Augusta National Women's Amateur with Champions Retreat Golf Club.",
            "output": "It is a for-profit, private golf club located in Augusta, Georgia. It opened in 1932 and hosts the Masters Tournament every year. It is a top-ranked course and does not disclose its earnings or member list.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.538621425628662,
            "normalized": 0.8681775501796177
        },
        "potential_entropy": {
            "strategy_component": 0.32500000000000007,
            "strategy_details": {
                "raw": 0.6500000000000001,
                "normalized": 0.6500000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 504,
        "original_data": {
            "instruction": "Tell me about the new premier of New South Wales",
            "input": "Christopher John Minns (born 17 September 1979) is an Australian politician serving as the 47th and current premier of New South Wales since March 2023. He has been the leader of the New South Wales branch of the Australian Labor Party (ALP) since June 2021 and a member of the New South Wales Legislative Assembly (MLA) for the division of Kogarah since 2015. He was previously the deputy mayor of Hurstville from 2007 to 2008.\nMinns was first elected at the 2015 New South Wales state election, and was elected unopposed as leader of the Labor Party in leadership election in June 2021, following the resignation of Jodi McKay. He led the party to victory at the 2023 state election.\nThe son of a school principal, Minns was raised in the St George region of Sydney in the suburb of Penshurst. He studied at Marist College Kogarah. He holds a bachelors degree from the University of New England in Armidale and attended Princeton University in the United States where he was awarded a Masters in Public Policy (2012–13).\nMinns is married and has three sons with his wife Anna. Upon his return from Princeton, while his wife pursued her own business opportunities, Minns became the carer of his sons before nominating for parliament.\nHe joined the Australian Labor Party in 1998 when he was 18 years old and was elected as the member for Kogarah in 2015.\nMinns is a supporter of NRL club the St George Illawarra Dragons.",
            "output": "Chris Minns led the Labor party to victory at the 2023 NSW state election and becomes the 47th premier of New South Wales.  Minns is married with three sons and is a supporter of the NRL club, St George Illawarra Dragons.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.3,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.6,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.4,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.9832253456115723,
            "normalized": 0.7094929558890206
        },
        "potential_entropy": {
            "strategy_component": 0.935,
            "strategy_details": {
                "raw": 1.87,
                "normalized": 1.87,
                "components": {
                    "instruction": 0.84,
                    "input": 0.4,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.7,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.6,
                    "out_div": 0.5,
                    "out_dens": 0.4,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 505,
        "original_data": {
            "instruction": "Is he still an active football player?",
            "input": "Mariano Sánchez Martínez (born 28 January 1978) is a Spanish former professional footballer who played as a defensive midfielder.\n\nHe appeared in 108 Segunda División games over three seasons, scoring two goals for Cartagena.\n\nClub career\nBorn in San Pedro del Pinatar, Region of Murcia, Sánchez did not reach the Segunda División B until he was 26, in 2004, arriving at CD Alcoyano from amateurs AD Mar Menor-San Javier. In the following year he moved to another club at that level, FC Cartagena, helping it promote to Segunda División in his fourth season.\n\nSánchez made his debut in the competition on 29 August 2009 at the age of 31 years and seven months, playing the full 90 minutes in a 1–0 away win against Girona FC. He scored his first league goal on 22 May 2010 in the 3–5 home loss to Levante UD, and never appeared in less than 34 league matches during his three seasons in that tier, suffering relegation in his last and renewing his contract for a further two years in June 2012.\n\nOn 14 May 2014, the 36-year-old Sánchez announced he would retire at the end of the campaign while hoping to help his team promote, which eventually did not befell.\n\nPersonal life\nSánchez rejected an offer to play youth football for Real Murcia when he was 18, after deciding to move to Madrid to study architecture. Not being able to enter Real Madrid's youth system, he chose to retire from football.\n\nAfter his playing days, Sánchez continued to work as an architect. Still as an active player, he was the figurehead behind the creation of the sports complex Pinatar Arena, in his hometown.",
            "output": "No, he retired at the age of 36 on May 14, 2014. However he is still working as an architect which he studied during his graduation.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.058375597000122,
            "normalized": 0.7309644562857491
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 506,
        "original_data": {
            "instruction": "How can I better learn a new language?",
            "input": "",
            "output": "Learning a language by using it is the most efficient way. Having a partner to practice not only help you work on your pronunciation and accent, it also incentivize you to keep practicing (similar to how having a gym buddy makes you go to the gym more often). \n\nLanguage are also meant to communicate. Allowing yourself for mistakes, whether it is an accent, bad conjugation would allow you to keep practicing and would result having a broader set of words and sentences you can use. From there correcting mistakes becomes easier as you naturally find relations between conjugation, forms etc.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.9013350009918213,
            "normalized": 0.9718100002833775
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 507,
        "original_data": {
            "instruction": "Why do people enjoy baseball?",
            "input": "",
            "output": "Baseball requires skill, practice, hard work, hand eye coordination and grit.  There are one hundred and fifty plus games per year in the major league, and most players will be on the field for 70% of those games.  There are few thrills better than being in the stands watching your home team win a game in the last inning by hitting a game-winning homerun.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.298591375350952,
            "normalized": 0.7995975358145577
        },
        "potential_entropy": {
            "strategy_component": 1.0875,
            "strategy_details": {
                "raw": 2.175,
                "normalized": 2.175,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 508,
        "original_data": {
            "instruction": "What is the difference between stress and strain?",
            "input": "",
            "output": "Stress is calculated as the pressure applied per unit area. So stress is essentially the force applied to an object divided by the cross sectional area of where the force is being applied to.\n\nStrain is calculated as the change in  parameter value (length, height or angle) as it experiences stress. It is unitless. So strain is essentially the deformation that occurs due to the stress that is applied.\n\nYoung's Modulus is a mechanical property that is computed by dividing stress over strain. It is typically expressed in GPa (giga Pascals)",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.4953582286834717,
            "normalized": 0.5701023510524205
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 509,
        "original_data": {
            "instruction": "Can we move away from fossil fuels in the next decade  ?",
            "input": "",
            "output": "According to the most recent report from the Intergovernmental Panel on Climate Change (IPCC) and a consensus statement from over 97% of the world’s climate scientists, burning fossil fuels for electricity, heat, and transportation is the largest contributor to human driven climate change. Fossil fuels are responsible for over 75% of Greenhouse gas emissions and over 97% of carbon dioxide emissions. These reports have also stressed that there’s an immediate need to move away from these fuels in the near future. \n\nThe challenge to this paradigm shift from fossil fuels to renewable energy is a combination of social and political barriers, the cost for new infrastructure to manage new energy portfolios , fossil fuel phase-out may lead to an increment in electricity prices and challenges to energy storage. \n\nIn April 2022, McKinsey published their Energy Insights Global Energy Perspective report, where they highlighted a rapid increase in traditional energy ( Oil and Gas) over the next 5 years, however the report also shows a rise in renewable energy and Hydrogen following that 5 year mark. While each adoption of this will differ in each country, the consensus is we will have a mixed energy portfolio (renewables and traditional) for the next 3 decades. The data also suggests as we see technology improve for more sustainable management of assets, batteries and resources, we will see a decline in the extraction , transportation  and consumption of fossil fuels.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.054508924484253,
            "normalized": 0.7298596927097866
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 510,
        "original_data": {
            "instruction": "What is the city of Seattle known for?",
            "input": "",
            "output": "Seattle is located in the northwest corner of the United States and is known for its overcast and rainy weather. Seattle has a strong coffee culture, in part because Starbucks was started there. The surrounding Cascade mountains and Puget Sound provide plenty of outdoor adventures, such as boating, rock climbing, and hiking. Many famous musicians started in Seattle, including Nirvana and Macklemore.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            1,
            3
        ],
        "entropy": {
            "raw": 2.976827383041382,
            "normalized": 0.707664966583252
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 511,
        "original_data": {
            "instruction": "Is beauty objective or subjective?",
            "input": "",
            "output": "The opinions on this question vary from person to person. \nStatistically, a pattern of preference is easily visible on what people would describe as beautiful. An evident example of this phenomenon is a beauty standard for women and men. Society seems to agree on what features, both physical and internal, can deem a person beautiful. The standard of beauty is also projected beyond humans, onto animal breeds, for example, in cat shows. \nA link between beauty and how closely it resembles perfection appears to exist. For example, a rose with many petals and no tears or scratches is generally seen as more beautiful than one with fewer, visibly ruined petals. \nHowever, the question of perfection also raises a problem as it is difficult and arguably impossible to define perfection. Beauty does not necessarily mean perfection. For example, large eyes are statistically considered to be more attractive however, in terms of their biological  purpose they possess no superiority to small eyes. Therefore, both beauty and perfection depend on the perspective or purpose with which you observe an object.\n\nFurthermore, the clear fluctuations through centuries and cultures on what is considered to be beautiful should also be taken into account. Fashion trends and art styles, for instance,  are constantly changing. Additionally, the diversity in opinions suggests there is nothing that is considered beautiful by everyone, so even if the majority deem a particular object or thing as beautiful, it’s beauty remains subjective because it remains based on an ever-changing preference rather than objective truth.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.475466728210449,
            "normalized": 0.8501333509172712
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 512,
        "original_data": {
            "instruction": "What is gardening?",
            "input": "Gardening is the practice of growing and cultivating plants as part of horticulture. In gardens, ornamental plants are often grown for their flowers, foliage, or overall appearance; useful plants, such as root vegetables, leaf vegetables, fruits, and herbs, are grown for consumption, for use as dyes, or for medicinal or cosmetic use.\n\nGardening ranges in scale from fruit orchards, to long boulevard plantings with one or more different types of shrubs, trees, and herbaceous plants, to residential back gardens including lawns and foundation plantings, all the way to container gardens grown inside or outside. Gardening may be very specialized, with only one type of plant grown, or involve a variety of plants in mixed plantings. It involves an active participation in the growing of plants, and tends to be labor-intensive, which differentiates it from farming or forestry.",
            "output": "Gardening is laying out and caring for a plot of ground devoted partially or wholly to growing plants such as flowers, herbs, or vegetables.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5020077228546143,
            "normalized": 0.8577164922441755
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 513,
        "original_data": {
            "instruction": "What is a “knock knock joke”?",
            "input": "",
            "output": "It’s a joke with a specific format that requires interaction between two people. The person saying the joke begins with the phrase “knock knock” and the other person responds with “who’s there?” The person asking the joke then gives the setup, and the other person repeats back the setup verbatim and adds the word “who” at the end. The person asking the joke then delivers the punchline. An example of a knock knock joke is: \nKnock knock\nWho’s there?\nOrange\nOrange who?\nOrange you glad I’m only doing one knock knock joke!?",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.876211166381836,
            "normalized": 0.678917476109096
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 514,
        "original_data": {
            "instruction": "Which months were added to the calendar and why?",
            "input": "",
            "output": "In order for the Roman calendar year which was ten months long, to be in sync with the lunar year, the months of January and February were added.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.4,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.6,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.2445781230926514,
            "normalized": 0.7841651780264718
        },
        "potential_entropy": {
            "strategy_component": 0.8175,
            "strategy_details": {
                "raw": 1.635,
                "normalized": 1.635,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.6,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 515,
        "original_data": {
            "instruction": "Which one among the following three words is different to the others: Novelty, Playful, Irresistible? Why?",
            "input": "",
            "output": "The word 'Novelty' is different to the other two words because it is an noun, while the others are adjectives.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.307647228240967,
            "normalized": 0.802184922354562
        },
        "potential_entropy": {
            "strategy_component": 1.025,
            "strategy_details": {
                "raw": 2.05,
                "normalized": 2.05,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 516,
        "original_data": {
            "instruction": "Why don't women have beards?",
            "input": "",
            "output": "Facial hair is typical in our ancestors, but now it's absent in most women. The difference seems down to evolution. Humans have become less hairy in the millions of years we developed from apes. Over time, men came to prefer women with little or no facial hair. Those women will pass this \"hairless\" gene to their daughters. Then those girls would have a head start in the ancient dating game.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.55,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.590092658996582,
            "normalized": 0.8828836168561663
        },
        "potential_entropy": {
            "strategy_component": 1.085,
            "strategy_details": {
                "raw": 2.17,
                "normalized": 2.17,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.2999999999999998,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.4,
                    "out_div": 0.5,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 517,
        "original_data": {
            "instruction": "I am about to have my third child under the age of 4.  What car should I get that would fit my family of 5, 3 kids in car seats, a dog and the possibility of adding another child in a few years?",
            "input": "",
            "output": "You should consider test driving and purchasing a minivan.  With three children under the age of five, all of which are going to need to be in car seats for the foresee able future, the built in features of a minivan are going to fit your current and short term needs.  They include, but are not limited to automatic side doors, automatic rear doors, front, backup, and in cabin cameras, in car vacuum, parking assist, cruise control, built-in sun shades, captain chairs with a removable middle seat and rear entertainment.  A minivan can easily sit 8 adults or children comfortably.  It is the perfect car for a family of five or more should your family grow.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.7016236782073975,
            "normalized": 0.9147496223449707
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 518,
        "original_data": {
            "instruction": "What is Kotlin?",
            "input": "",
            "output": "Kotlin is a cross-platform, statically typed, general-purpose high-level programming language with type inference. Kotlin is designed to interoperate fully with Java, and the JVM version of Kotlin's standard library depends on the Java Class Library, but type inference allows its syntax to be more concise.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 2.861043930053711,
            "normalized": 0.674583980015346
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 519,
        "original_data": {
            "instruction": "Which things are typically hot and which are cold: an icicle, a snowball, lava, a cat, tea, a meat locker, summer on the east coast of the United States, a sauna and a Bikram yoga.",
            "input": "",
            "output": "The hot items in this list are lava, summer, a sauna and Bikram yoga. The cold items are an icicle, a snowball, and a meat locker. A cat is typically not categorized as either hot or cold, and tea can be either hot or cold.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.681122303009033,
            "normalized": 0.9088920865740094
        },
        "potential_entropy": {
            "strategy_component": 0.46,
            "strategy_details": {
                "raw": 0.92,
                "normalized": 0.92,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 520,
        "original_data": {
            "instruction": "From the passage provided, extract the  population of Seattle metropolitan area",
            "input": "Seattle (/siˈætəl/ (listen) see-AT-əl) is a seaport city on the West Coast of the United States. It is the seat of King County, Washington. With a 2020 population of 737,015, it is the largest city in both the state of Washington and the Pacific Northwest region of North America. The Seattle metropolitan area's population is 4.02 million, making it the 15th-largest in the United States. Its growth rate of 21.1% between 2010 and 2020 made it one of country's fastest-growing large cities.",
            "output": "The Seattle metropolitan area's population is 4.02 million, making it the 15th-largest in the United States",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.2017219066619873,
            "normalized": 0.48620625904628206
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 521,
        "original_data": {
            "instruction": "Given a reference text about Richard Brandon Morris, tell me what when and where he was born as well as the different colleges and universities he attended.",
            "input": "Richard Brandon Morris (July 24, 1904 – March 3, 1989) was an American historian best known for his pioneering work in colonial American legal history and the early history of American labor. In later years, he shifted his research interests to the constitutional, diplomatic, and political history of the American Revolution and the making of the United States Constitution.\n\nBackground\nRichard Brandon Morris was born on July 24, 1904, in New York City. He attended high school at Towsend Harris Hall in New York City. In 1924, he received a BA degree from City College. In 1925, he received an MA from Columbia University, and in 1930 he received a PhD in history at the university with Evarts Boutell Greene as his dissertation advisor. Morris' dissertation, published by Columbia University Press as Studies in the History of American Law, with Special Reference to the Seventeenth and Eighteenth Centuries (1930), still defines the research agenda for historians working on early American law, though at the time it attracted bitter denunciations from law school practitioners of legal historym, including Julius Goebel, Jr. and Karl Llewellyn, both then Columbia Law School faculty members.",
            "output": "Richard Brandon Morris was born on July 24, 1904. He attended City College and Columbia University.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.052776336669922,
            "normalized": 0.7293646676199776
        },
        "potential_entropy": {
            "strategy_component": 0.36250000000000004,
            "strategy_details": {
                "raw": 0.7250000000000001,
                "normalized": 0.7250000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 522,
        "original_data": {
            "instruction": "Which of these are soccer players? Maradona, Joe Biden, Michelle Platini, John Terry, John Oliver, Mike Anderson, Steven Gerrard, Mo Salah, Obama, Michael Jordan",
            "input": "",
            "output": "Maradona, Michelle Platini, John Terry, Steven Gerrard and Mo Salah are soccer players.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.537158012390137,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 523,
        "original_data": {
            "instruction": "What decade is the Great Gatsby set in, and what is the nickname for that time period?",
            "input": "",
            "output": "The Great Gatsby is set in the 1920s, known as the Roaring Twenties",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 1.673795461654663,
            "normalized": 0.3353701319013323
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 524,
        "original_data": {
            "instruction": "Identify which instrument is string or woodwind: Bandol, Soprano clarinet",
            "input": "",
            "output": "Soprano clarinet is woodwind, Bandol is string.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.935803413391113,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.47000000000000003,
            "strategy_details": {
                "raw": 0.9400000000000001,
                "normalized": 0.9400000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 525,
        "original_data": {
            "instruction": "Which is a species of fish? Pickerel or Pickle",
            "input": "",
            "output": "Pickerel",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.956930160522461,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 526,
        "original_data": {
            "instruction": "What is seek speed in HDFS and why is it no longer going to be important?",
            "input": "",
            "output": "Seek speed is the speed i.e. rpm (revolutions per minute) a magnetic drive or an optical reader rotates to read the data from those HDDs Hard Drive Disks; This speed is dependent on the motor's rotation speed which directly impacts reading and writing of data into these devices. The speed of the motor is limited as it is powered by 12V capacity and also should not drain the battery if they are in the laptops. However, more and more newer age laptops and devices are now using SSDs Solid State Drives which are not only faster because the speed is not dependent on the revolving motor and also does not drain your laptops battery charge. This allowed the laptops to be less bulkier/heavier and reduce the overall battery size to make it a little lighter.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.4,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.6,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            0
        ],
        "entropy": {
            "raw": 3.695918083190918,
            "normalized": 0.9131194523402623
        },
        "potential_entropy": {
            "strategy_component": 0.825,
            "strategy_details": {
                "raw": 1.65,
                "normalized": 1.65,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.6,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 527,
        "original_data": {
            "instruction": "Q: Products online claim to prevent or treat COVID-19. Where can I report websites selling products with fraudulent claims?",
            "input": "",
            "output": "A: The FDA advises consumers to be beware of websites and stores selling products that claim to prevent, treat, or cure COVID-19. If you have a question about a product sold online that claims to prevent, treat, or cure COVID-19, talk to your health care provider or doctor.\n\nRead this Consumer Update to learn how to protect yourself and your family from coronavirus fraud. \n\nPlease report websites selling products with fraudulent claims about treatment or prevention of COVID-19. If you have experienced a bad reaction to a product sold with COVID-19 claims, report it to the FDA’s MedWatch Adverse Event Reporting program:\n\nComplete and submit the report online; or\nDownload and complete the form, then submit it via fax at 1-800-FDA-0178.\nInclude as much information as you can about the product that caused the reaction, including the product name, the manufacturer, and the lot number (if available).",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 2.51676082611084,
            "normalized": 0.5762173788888114
        },
        "potential_entropy": {
            "strategy_component": 1.045,
            "strategy_details": {
                "raw": 2.09,
                "normalized": 2.09,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 528,
        "original_data": {
            "instruction": "Who did Donald's grandfather body guard for?",
            "input": "Ross was born in Nairn, in Middlesex County, Ontario, the son of Donald Ross and Margaret Halbert. His grandfather was part of Duke Wellington's body guard at Waterloo, and was an escort to Napoleon Bonaparte on his exile to St. Helena. Ross was educated at Nairn Public School, and moved to Manitoba in 1874, where he worked on the first telegraph line. In 1877, he established a real estate business. Ross served on the Winnipeg City Council for eight years, the Winnipeg School Board for twenty-three years, and the Winnipeg Public Parks Board for eleven years. In 1880, he married Margaret McIvor.",
            "output": "Donald's grandfather was a body guard for Duke Wellington.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 6.505151271820068,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 529,
        "original_data": {
            "instruction": "From the passage provided, extract the government posts George Osborne held.",
            "input": "George Osborne was elected as MP for Tatton in 2001, becoming the youngest Conservative member of the House of Commons. He was appointed Shadow Chief Secretary to the Treasury by Conservative leader Michael Howard in 2004. The following year he ran David Cameron's successful party leadership campaign, and was subsequently appointed Shadow Chancellor of the Exchequer. After the 2010 general election, Osborne was appointed Chancellor of the Exchequer in the Cameron–Clegg coalition. He succeeded Alistair Darling, inheriting a large deficit in government finances due to the effects of the late-2000s financial crisis. As Chancellor, Osborne's tenure pursued austerity policies aimed at reducing the budget deficit and launched the Northern Powerhouse initiative. After the Conservatives won an overall majority in the 2015 general election, Cameron reappointed him Chancellor in his second government and gave him the additional title of First Secretary of State. He was widely viewed as a potential successor to David Cameron as Leader of the Conservative Party; one Conservative MP, Nadhim Zahawi, suggested that the closeness of his relationship with Cameron meant that the two effectively shared power during the duration of the Cameron governments. Following the 2016 referendum vote to leave the European Union and Cameron's consequent resignation, he was dismissed by Cameron's successor, Theresa May.",
            "output": "Shadow Chief Secretary to the Treasury, Shadow Chancellor of the Exchequer, Chancellor of the Exchequer, First Secretary of State",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.363755464553833,
            "normalized": 0.5325015613010952
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 530,
        "original_data": {
            "instruction": "Please provide me with ideas for dinner tonight.",
            "input": "",
            "output": "Sure! Here are 2 ideas. \n\n1) Ordering in:\n\nYou can see what restaurants in your area offer home delivery, and select something that works well for you! Some common things to order are Pizza, Sushi, Tacos, Fried Rice, and more! \n\n2) Check in your fridge and pantry, and cook at home! Here are a few easy meal ideas: \n\n- Pasta\n- Stir Fry\n- Flat bread \n- Chicken, rice, and vegetables\n- Wraps or sandwiches",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.2011640071868896,
            "normalized": 0.7717611449105399
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.85,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 531,
        "original_data": {
            "instruction": "What is the difference between a sedan and a coupe?",
            "input": "",
            "output": "Sedans are typically 4 door vehicles with a low profile and coupes are 2 door vehicles with a sportier look. There is a new trend to call sedans with a more athletic silhouette 4 door coupes, but that's debatable.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.14957594871521,
            "normalized": 0.7570216996329171
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 532,
        "original_data": {
            "instruction": "Please label which of the following appear in the game Clue as suspects, weapons, or rooms: Atrium, Billiard Room, Colonel Mustard, Conservatory, General Ketchup, Lead Pipe, Mrs. Peacock, Rope",
            "input": "",
            "output": "In the classic edition of board game Clue, Colonel Mustard and Mrs. Peacock are suspects, the Lead Pipe and Rope are weapons, and Billiard Room and Conservatory are rooms. While the Atrium and General Ketchup seem like a plausible room and suspect respectively, neither is in game.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.044838905334473,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 533,
        "original_data": {
            "instruction": "I'm considering buying football boots. How do I know which one to buy?",
            "input": "",
            "output": "Choosing the correct football boot depends on several factors. Of primary importance would be the surface you are playing on. Certain soleplate patterns are optimized for Hard Ground, Firm Ground, Soft Ground and Artificial Grass. Another important factor is the position you play. Midfielders prefer boots with more control, forwards prefer boots with a better striking face, while wingers prefer lightweight boots that are optimized for speed.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.366007089614868,
            "normalized": 0.818859168461391
        },
        "potential_entropy": {
            "strategy_component": 1.105,
            "strategy_details": {
                "raw": 2.21,
                "normalized": 2.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 534,
        "original_data": {
            "instruction": "Who invented Boltzmann Machines?",
            "input": "",
            "output": "Boltzmann machines were invented in 1985 by Geoffrey Hinton and Terrence Sejnowski.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.9931039810180664,
            "normalized": 0.712315423148019
        },
        "potential_entropy": {
            "strategy_component": 1.2999999999999998,
            "strategy_details": {
                "raw": 2.5999999999999996,
                "normalized": 2.5999999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 535,
        "original_data": {
            "instruction": "How was the universe created?",
            "input": "The Big Bang event is a physical theory that describes how the universe expanded from an initial state of high density and temperature. Various cosmological models of the Big Bang explain the evolution of the observable universe from the earliest known periods through its subsequent large-scale form. These models offer a comprehensive explanation for a broad range of observed phenomena, including the abundance of light elements, the cosmic microwave background (CMB) radiation, and large-scale structure. The overall uniformity of the Universe, known as the flatness problem, is explained through cosmic inflation: a sudden and very rapid expansion of space during the earliest moments. However, physics currently lacks a widely accepted theory of quantum gravity that can successfully model the earliest conditions of the Big Bang.",
            "output": "We can't know for sure but a popular theory called the Big Bang suggests that the universe was created from an infinitesimally small point of infinite energy some four and a half billion years ago.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.55,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.7899482250213623,
            "normalized": 0.6542709214346749
        },
        "potential_entropy": {
            "strategy_component": 0.5375,
            "strategy_details": {
                "raw": 1.075,
                "normalized": 1.075,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.5850000000000001
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.4,
                    "out_div": 0.5,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 536,
        "original_data": {
            "instruction": "What is Github Copilot?",
            "input": "GitHub Copilot is a cloud-based artificial intelligence tool developed by GitHub and OpenAI to assist users of Visual Studio Code, Visual Studio, Neovim, and JetBrains integrated development environments (IDEs) by autocompleting code. Currently available by subscription to individual developers, the tool was first announced by GitHub on 29 June 2021, and works best for users coding in Python, JavaScript, TypeScript, Ruby, and Go.",
            "output": "GitHub Copilot is a cloud-based artificial intelligence tool developed by GitHub and OpenAI",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.424006700515747,
            "normalized": 0.835430485861642
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 537,
        "original_data": {
            "instruction": "Who does what in U2?",
            "input": "U2 are an Irish rock band from Dublin, formed in 1976. The group consists of Bono (lead vocals and rhythm guitar), the Edge (lead guitar, keyboards, and backing vocals), Adam Clayton (bass guitar), and Larry Mullen Jr. (drums and percussion). Initially rooted in post-punk, U2's musical style has evolved throughout their career, yet has maintained an anthemic quality built on Bono's expressive vocals and the Edge's chiming, effects-based guitar sounds. Bono's lyrics, often embellished with spiritual imagery, focus on personal and sociopolitical themes. Popular for their live performances, the group have staged several ambitious and elaborate tours over their career.",
            "output": "The rock band U2 features Bono on lead vocals and guitar, the Edge on lead guitar, keyboard and vocals, Adam Clayton on bass guitar and Larry Mullen Jr. on drums and percussion.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5812854766845703,
            "normalized": 0.8803672790527344
        },
        "potential_entropy": {
            "strategy_component": 0.46249999999999997,
            "strategy_details": {
                "raw": 0.9249999999999999,
                "normalized": 0.9249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 538,
        "original_data": {
            "instruction": "What happens if I let go of a helium balloon?",
            "input": "",
            "output": "Because helium is lighter than air, a helium balloon will float upwards if you let go of it.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.268341541290283,
            "normalized": 0.5052404403686523
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 539,
        "original_data": {
            "instruction": "From the text below, list the years that the White House Chief of Staff was removed from the United States Cabinet. Separate them with a comma.",
            "input": "Positions intermittently elevated to Cabinet-rank\nAmbassador to the United Nations (1953–1989, 1993–2001, 2009–2018, 2021–present)\nDirector of the Office of Management and Budget (1953–1961, 1969–present)\nWhite House Chief of Staff (1953–1961, 1974–1977, 1993–present)\nCounselor to the President (1969–1977, 1981–1985, 1992–1993): A title used by high-ranking political advisers to the president of the United States and senior members of the Executive Office of the President since the Nixon administration. Incumbents with Cabinet rank included Daniel Patrick Moynihan, Donald Rumsfeld and Anne Armstrong.\nWhite House Counsel (1974–1977)\nUnited States Trade Representative (1975–present)\nChair of the Council of Economic Advisers (1977–1981, 1993–2001, 2009–2017, 2021–present)\nNational Security Advisor (1977–1981)\nDirector of Central Intelligence (1981–1989, 1995–2001)\nAdministrator of the Environmental Protection Agency (1993–present)\nDirector of the Office of National Drug Control Policy (1993–2009)\nAdministrator of the Small Business Administration (1994–2001, 2012–present)\nDirector of the Federal Emergency Management Agency (1996–2001): Created as an independent agency in 1979, raised to Cabinet rank in 1996, and dropped from Cabinet rank in 2001.\nDirector of National Intelligence (2017–present)\nDirector of the Central Intelligence Agency (2017–2021)\nDirector of the Office of Science and Technology Policy (2021–present)",
            "output": "1961, 1977",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 1.0,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.0,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.965240001678467,
            "normalized": 0.7043542861938477
        },
        "potential_entropy": {
            "strategy_component": 0.215,
            "strategy_details": {
                "raw": 0.43,
                "normalized": 0.43,
                "components": {
                    "instruction": 0.0,
                    "input": 0.25,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.0,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 540,
        "original_data": {
            "instruction": "Classify the following as either a solid, liquid, or gas: carbon dioxide, water, oil, granite, aluminum, gold, oxygen",
            "input": "",
            "output": "Carbon Dioxide: Gas\nWater: Liquid\nOil: Liquid\nGranite: Solid\nAluminum: Solid\nGold: Solid\nOxygen: Gas",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.2422730922698975,
            "normalized": 0.49779231207711355
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 541,
        "original_data": {
            "instruction": "Who is Rentala Madhubala",
            "input": "",
            "output": "Rentala Madhubala is an Indian scientist who studied molecular parasitology and functional genomics. She is the director of the Academic Staff College in Jawaharlal Nehru University. She was the Dean at the School of Life Sciences and the director of the Advanced Instrumentation Research Facility there.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.477484703063965,
            "normalized": 0.8507099151611328
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 542,
        "original_data": {
            "instruction": "Based on this passage, what is the average number of people per family in Zangisheh?",
            "input": "Zangisheh (Persian: زنگيشه, also Romanized as Zangīsheh) is a village in Qarah Su Rural District, in the Central District of Kermanshah County, Kermanshah Province, Iran. At the 2006 census, its population was 153, in 34 families.",
            "output": "I can only tell you what it was in 2006, because this is when the survey data provided is from. The average number of people per family at that time is the population divided by the number of families, or 153 / 34.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.95,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 3.181037425994873,
            "normalized": 0.7660106931413923
        },
        "potential_entropy": {
            "strategy_component": 0.765,
            "strategy_details": {
                "raw": 1.53,
                "normalized": 1.53,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.9,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.6,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 543,
        "original_data": {
            "instruction": "Does Las Vegas have a professional hockey team?",
            "input": "",
            "output": "The Vegas Golden Knights are a professional ice hockey team based in the Las Vegas metropolitan area. They compete in the National Hockey League as a member of the Pacific Division in the Western Conference.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 1.5371235609054565,
            "normalized": 0.29632101740155903
        },
        "potential_entropy": {
            "strategy_component": 0.8149999999999998,
            "strategy_details": {
                "raw": 1.6299999999999997,
                "normalized": 1.6299999999999997,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.2999999999999998,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 544,
        "original_data": {
            "instruction": "In computer science, where did the term big-endian originate?",
            "input": "Danny Cohen introduced the terms big-endian and little-endian into computer science for data ordering in an Internet Experiment Note published in 1980.\n\nThe adjective endian has its origin in the writings of 18th century Anglo-Irish writer Jonathan Swift. In the 1726 novel Gulliver's Travels, he portrays the conflict between sects of Lilliputians divided into those breaking the shell of a boiled egg from the big end or from the little end. Because the emperor's son had cut his finger while opening an egg from the big end, doing so was prohibited by an imperial edict; those who rebelled and did so were called \"Big-Endians\" (Swift did not use the term Little-Endians in the work). Cohen makes the connection to Gulliver's Travels explicit in the appendix to his 1980 note.",
            "output": "Danny Cohen introduced the terms big-endian in an Internet Experiment Note published in 1980. In the appendix to the 1980 note he connected the idea to Gulliver's Travels.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.024448871612549,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 545,
        "original_data": {
            "instruction": "What are some games you can play in the middle of nature and a bunch of people?",
            "input": "",
            "output": "There are the classic games you can always play, like tag, or I spy, or Truth or Dare, but there are plenty of ways to be more creative.\n\nYou can collect a bunch of rocks. Then about 10 feet away create a small stack of rocks. Each person takes turns with their own pebbles and tries to knock the stack of rocks over.\n\nIf you are near moving water, you can have a \"stick boat\" race. Everyone chooses a stick and places it at one point in the river, and you cheer and goad each other to see who's stick wins the race.\n\nYou can divide into teams and create \"nature art\" like Andy Goldsworthy who assembles leaves and sticks into beautiful images.\n\nYou can simulate games like Bocce or Frisbee Golf with things you find; rocks can be bocce balls, sticks can be frisbees.\n\nYou can make a game out of anything.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.3773550987243652,
            "normalized": 0.8221014567783901
        },
        "potential_entropy": {
            "strategy_component": 0.9724999999999999,
            "strategy_details": {
                "raw": 1.9449999999999998,
                "normalized": 1.9449999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 546,
        "original_data": {
            "instruction": "What is success",
            "input": "",
            "output": "Satisfaction levels define the success, success defined for one person may not be true for other",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.85031270980835,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6575,
            "strategy_details": {
                "raw": 1.315,
                "normalized": 1.315,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 547,
        "original_data": {
            "instruction": "What is Biryani?",
            "input": "Biryani is a mixed rice dish originating among the Muslims of the Indian subcontinent. It is made with Indian spices, rice, and usually some type of meat (chicken, beef, goat, lamb, prawn, and fish), or in some cases without any meat, and sometimes, in addition, eggs and potatoes.\n\nBiryani is one of the most popular dishes in South Asia, as well as among the diaspora from the region. Similar dishes are also prepared in other parts of the world such as in Iraq, Myanmar, Thailand, and Malaysia. Biryani is the single most-ordered dish on Indian online food ordering and delivery services and has been labeled as the most popular dish overall in India.",
            "output": "The term biryani comes from the Farsi phrase birinj biriyan, “fried rice.” Rice is fried separately until about half-cooked, usually in oil or ghee, and then placed in a pot along with marinated meat or vegetables and spices. Biryani was first recorded in India in the Deccan region of the south, suggesting that it was brought there by merchants via a sea route. Biryani has since spread to all parts of the Indian subcontinent, with numerous local and regional variations. Varieties of biryani are also found in Sri Lanka, Malaysia, Thailand, and Oman.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.824932336807251,
            "normalized": 0.6642663819449288
        },
        "potential_entropy": {
            "strategy_component": 0.43249999999999994,
            "strategy_details": {
                "raw": 0.8649999999999999,
                "normalized": 0.8649999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 548,
        "original_data": {
            "instruction": "Which of the following is a database or Streaming solution? Oracle, Kinesis, Event hub, SQL Server",
            "input": "",
            "output": "Oracle- Database, Kinesis- Streaming solution, Event hub- Streaming solution, SQL Server- Database",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.799994945526123,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.005,
            "strategy_details": {
                "raw": 2.01,
                "normalized": 2.01,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.65,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 549,
        "original_data": {
            "instruction": "Who won the 1988 Superbowl",
            "input": "",
            "output": "Washington Redskins",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 12.738723754882812,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 550,
        "original_data": {
            "instruction": "How many sinking ships did Violet Jessop survive? Be precise.",
            "input": "Violet Constance Jessop (2 October 1887 – 5 May 1971), often referred to as the \"Queen of sinking ships\" or \"Miss Unsinkable,\" was an Argentine woman of Irish heritage who worked as an ocean liner stewardess, memoirist, and nurse in the early 20th century. Jessop is most well known for having survived the sinking of both the RMS Titanic in 1912 and her sister ship the HMHS Britannic in 1916, as well as having been onboard the eldest of the three sister ships, the RMS Olympic, when it collided with the British warship HMS Hawke in 1911.",
            "output": "2.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 9.0743989944458,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 551,
        "original_data": {
            "instruction": "What is GitHub?",
            "input": "",
            "output": "GitHub is an Internet hosting service for software development and version control using Git. It provides the distributed version control of Git plus access control, bug tracking, software feature requests, task management, continuous integration, and wikis for every project.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 3.535879611968994,
            "normalized": 0.8673941748482841
        },
        "potential_entropy": {
            "strategy_component": 1.1949999999999998,
            "strategy_details": {
                "raw": 2.3899999999999997,
                "normalized": 2.3899999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 552,
        "original_data": {
            "instruction": "How is electricity recorded in ancient Egyptian texts as per the passage below.",
            "input": "Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the \"Thunderer of the Nile\", and described them as the \"protectors\" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by electric catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients with ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them.\nAncient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing.\nThales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.\nElectricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word electricus (\"of amber\" or \"like amber\",, elektron, the Greek word for \"amber\") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words \"electric\" and \"electricity\", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.\nFurther work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges\nIn 1775, Hugh Williamson reported a series of experiments to the Royal Society on the shocks delivered by the electric eel; that same year the surgeon and anatomist John Hunter described the structure of the fish's electric organs. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his \"On Physical Lines of Force\" in 1861 and 1862. \nWhile the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.\nIn 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for \"his discovery of the law of the photoelectric effect\". The photoelectric effect is also employed in photocells such as can be found in solar panels.\nThe first solid-state device was the \"cat's-whisker detector\" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.\nSolid-state electronics came into its own with the emergence of transistor technology. The first working transistor, a germanium-based point-contact transistor, was invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947, followed by the bipolar junction transistor in 1948.",
            "output": "Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the \"Thunderer of the Nile\", and described them as the \"protectors\" of all other fish.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1346817016601562,
            "normalized": 0.7527662004743304
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 553,
        "original_data": {
            "instruction": "Who composed the 'Moonlight Sonata', and when?",
            "input": "",
            "output": "Commonly known as the Moonlight Sonata, the Piano Sonata No. 14 in C-sharp minor \"Quasi una fantasia\", Op. 27, No. 2, was composed by Ludwig van Beethoven.\nThe composition was completed in 1801 and dedicated to the Countess Giulietta Guicciardi in 1802. It's one of the most popular musical compositions for the piano.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.0979740619659424,
            "normalized": 0.45656401770455496
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 554,
        "original_data": {
            "instruction": "Which US city is known to be the bachelorette party center of the country, besides Las Vegas?",
            "input": "",
            "output": "Nashville, TN",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.005029201507568,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0099999999999998,
            "strategy_details": {
                "raw": 2.0199999999999996,
                "normalized": 2.0199999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 555,
        "original_data": {
            "instruction": "How many people visit the zoo in the US each year?",
            "input": "A zoo (short for zoological garden; also called an animal park or menagerie) is a facility in which animals are kept within enclosures for public exhibition and often bred for conservation purposes.\n\nThe term zoological garden refers to zoology, the study of animals. The term is derived from the Greek ζώον, zoon, 'animal', and the suffix -λογία, -logia, 'study of'. The abbreviation zoo was first used of the London Zoological Gardens, which was opened for scientific study in 1828 and to the public in 1847. In the United States alone, zoos are visited by over 181 million people annually.",
            "output": "181 million people in the US visit the zoo annually each year.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.4,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.6,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.55194616317749,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5875,
            "strategy_details": {
                "raw": 1.175,
                "normalized": 1.175,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.6,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 556,
        "original_data": {
            "instruction": "What is Pura Vida?",
            "input": "",
            "output": "Pura Vida is a phrase used in Costa Rica and exactly translates to Pure Life, however it is a style of living of people from Costa Rica, and it means that \"everything is fine\" or even \"amazing\". It is about waking up every day and remembering that just waking up it is a good reason to be happy and satisfied in life, having a chance to enjoy every moment.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.4863622188568115,
            "normalized": 0.8532463482448033
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 557,
        "original_data": {
            "instruction": "From the passage note down the name of the countries which have most voting power. List the results in comma separated format.",
            "input": "The World Bank is an international financial institution that provides loans and grants to the governments of low- and middle-income countries for the purpose of pursuing capital projects. The World Bank is the collective name for the International Bank for Reconstruction and Development (IBRD) and International Development Association (IDA), two of five international organizations owned by the World Bank Group. It was established along with the International Monetary Fund at the 1944 Bretton Woods Conference. After a slow start, its first loan was to France in 1947. In the 1970s, it focused on loans to developing world countries, shifting away from that mission in the 1980s. For the last 30 years, it has included NGOs and environmental groups in its loan portfolio. Its loan strategy is influenced by the United Nations' Sustainable Development Goals, as well as environmental and social safeguards.\n\nAs of 2022, the World Bank is run by a president and 25 executive directors, as well as 29 various vice presidents. IBRD and IDA have 189 and 174 member countries, respectively. The U.S., Japan, China, Germany and the U.K. have the most voting power. The bank aims loans at developing countries to help reduce poverty. The bank is engaged in several global partnerships and initiatives, and takes a role in working toward addressing climate change. The World Bank operates a number of training wings and it works with the Clean Air Initiative and the UN Development Business. It works within the Open Data Initiative and hosts an Open Knowledge Repository.\n\nThe World Bank has been criticized as promoting inflation and harming economic development, causing protests in 1988 and 2000. There has also been criticism of the bank's governance and response to the COVID-19 pandemic.",
            "output": "U.S., Japan, China, Germany, U.K.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.340456247329712,
            "normalized": 0.8115589278084892
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 558,
        "original_data": {
            "instruction": "Which is an imperial or a metric measurement: gallon, litre",
            "input": "",
            "output": "gallon is imperial, litre is metric",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.816819667816162,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.225,
            "strategy_details": {
                "raw": 2.45,
                "normalized": 2.45,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 559,
        "original_data": {
            "instruction": "Name the Chinese satellite launched for Mars which failed?",
            "input": "",
            "output": "Yinghuo-1 was the Chinese satellite that failed the launch for Mars",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 5.115696907043457,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3599999999999999,
            "strategy_details": {
                "raw": 2.7199999999999998,
                "normalized": 2.7199999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 560,
        "original_data": {
            "instruction": "Extract a list of notable dates and events in the mentioned text in the format, choosing the earliest year when the text provides a range of dates, and place them in a bullet list: {Year} - {Event}",
            "input": "The first two centuries of the Roman Empire saw a period of unprecedented stability and prosperity known as the Pax Romana (lit. 'Roman Peace'). Rome reached its greatest territorial expanse during the reign of Trajan (AD 98–117); a period of increasing trouble and decline began with the reign of Commodus (177–192). In the 3rd century, the Empire underwent a crisis that threatened its existence, as the Gallic and Palmyrene Empires broke away from the Roman state, and a series of short-lived emperors, often from the legions, led the Empire. It was reunified under Aurelian (r. 270–275). To stabilize it, Diocletian set up two different imperial courts in the Greek East and Latin West in 286; Christians rose to positions of power in the 4th century following the Edict of Milan of 313. Shortly after, the Migration Period, involving large invasions by Germanic peoples and by the Huns of Attila, led to the decline of the Western Roman Empire. With the fall of Ravenna to the Germanic Herulians and the deposition of Romulus Augustus in AD 476 by Odoacer, the Western Roman Empire finally collapsed; the Eastern Roman emperor Zeno formally abolished it in AD 480. The Eastern Roman Empire survived for another millennium, until Constantinople fell in 1453 to the Ottoman Turks under Mehmed II.",
            "output": "* 98 - Rome reached it's greatest territorial expanse during the reign of Trajan\n* 177 - Increasing trouble and decline began with the rain of Commodus\n* 270 - Rome reunified under Aurelian after Gallic and Palmyrene Empires broke away\n* 286 - Two imperial courts setup by Diocletian in the Greek East and Latin West\n* 313 - Edict of Milan led to Christians rising to positions of power\n* 476 - Deposition of Romulus Augustus by Odoacer following the fall of Ravenena to the Germanic Herulians\n* 480 - Eastern Roman emperor Zeno formally abolished the Western Roman Empire\n* 1453 - Eastern Roman Empire fell to the Ottoman Turks",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.3425657749176025,
            "normalized": 0.8121616499764579
        },
        "potential_entropy": {
            "strategy_component": 0.29000000000000004,
            "strategy_details": {
                "raw": 0.5800000000000001,
                "normalized": 0.5800000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 561,
        "original_data": {
            "instruction": "How does meditation help mental health",
            "input": "",
            "output": "Meditation is beneficial for reducing stress. Studies show that it can also reduce depression and anxiety. Meditation can give you a sense of calm, peace and balance that benefits both your emotional well-being and your overall health. Research has found several physiological benefits such as better blood pressure, heart rate, metabolism, breathing pattern, oxygen utilization, and carbon dioxide elimination and relative blood flow to the brain.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.933055877685547,
            "normalized": 0.6951588221958706
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 562,
        "original_data": {
            "instruction": "What is the FAANG equivalent in China?",
            "input": "",
            "output": "The FAANG equivalent in China is called BAT. It consist of three Chinese technology companies, namely Baidu, Alibaba and Tencent",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.132045269012451,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 563,
        "original_data": {
            "instruction": "How are cats different than dogs?",
            "input": "",
            "output": "Dogs are domesticated animals that usually are more active and rely on their owner. Cats, on average, are more solitary creatures. They can be left alone longer and are more self sufficient. Dogs need lots of attention and activity while cats can entertain themselves.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.609553337097168,
            "normalized": 0.8884438105991909
        },
        "potential_entropy": {
            "strategy_component": 1.3524999999999998,
            "strategy_details": {
                "raw": 2.7049999999999996,
                "normalized": 2.7049999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 564,
        "original_data": {
            "instruction": "What year did the Calgary Flames win the Stanley Cup?",
            "input": "",
            "output": "1989",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.252131938934326,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 565,
        "original_data": {
            "instruction": "Where is the Stade Louis-ll located?",
            "input": "The Stade Louis-II, also known as Louis II (French pronunciation: ​[stad(ə) lwi dø]) is a stadium located in the Fontvieille district of Monaco. It serves primarily as a venue for football, being the home of AS Monaco and the Monaco national football team. The stadium is most notable for its distinctive nine arches at the away end of the ground. The arena is also used for the Herculis, a track and field meet of the Diamond League. The stadium hosted the 1986 and 1998–2012 UEFA Super Cup matches. Due to Monaco's small size, the stadium is the only football and athletics stadium in the country.",
            "output": "The Stade Louis-ll is located in the Fontvielle district of Monaco.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.92,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.07999999999999996,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 5.38588285446167,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.38949999999999996,
            "strategy_details": {
                "raw": 0.7789999999999999,
                "normalized": 0.7789999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.309
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.07999999999999996,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 566,
        "original_data": {
            "instruction": "Extract the owner of Lamborghini and a listing of the different types of Huracan cars that Lamborghini has produced for its Motorsport division.",
            "input": "Automobili Lamborghini S.p.A. (Italian pronunciation: [autoˈmɔːbili lamborˈɡiːni]) is an Italian manufacturer of luxury sports cars and SUVs based in Sant'Agata Bolognese. The company is owned by the Volkswagen Group through its subsidiary Audi.\n\nFerruccio Lamborghini (1916–1993), an Italian manufacturing magnate, founded Automobili Ferruccio Lamborghini S.p.A. in 1963 to compete with Ferrari. The company was noted for using a rear mid-engine, rear-wheel drive layout. Lamborghini grew rapidly during its first decade, but sales plunged in the wake of the 1973 worldwide financial downturn and the oil crisis. The firm's ownership changed three times after 1973, including a bankruptcy in 1978. American Chrysler Corporation took control of Lamborghini in 1987 and sold it to Malaysian investment group Mycom Setdco and Indonesian group V'Power Corporation in 1994. In 1998, Mycom Setdco and V'Power sold Lamborghini to the Volkswagen Group where it was placed under the control of the group's Audi division.\n\nNew products and model lines were introduced to the brand's portfolio and brought to the market and saw an increased productivity for the brand. In the late 2000s, during the worldwide financial crisis and the subsequent economic crisis, Lamborghini's sales saw a drop of nearly 50 per cent.\n\nLamborghini currently produces the V12-powered Aventador and the V10-powered Huracán, along with the Urus SUV powered by a twin-turbo V8 engine. In addition, the company produces V12 engines for offshore powerboat racing.\n\nLamborghini Trattori, founded in 1948 by Ferruccio Lamborghini, is headquartered in Pieve di Cento, Italy and continues to produce tractors. Since 1973, Lamborghini Trattori has been a separate entity from the automobile manufacturer.\n\nHistory\nMain article: History of Lamborghini\n\nFerruccio Lamborghini with a Jarama and a tractor of his brand\nManufacturing magnate Italian Ferruccio Lamborghini founded the company in 1963 with the objective of producing a refined grand touring car to compete with offerings from established marques such as Ferrari. The company's first models, such as the 350 GT, were released in the mid-1960s. Lamborghini was noted for the 1966 Miura sports coupé, which used a rear mid-engine, rear-wheel drive layout.\n\nLamborghini grew rapidly during its first ten years, but sales fell in the wake of the 1973 worldwide financial downturn and the oil crisis. Ferruccio Lamborghini sold the company to Georges-Henri Rossetti and René Leimer and retired in 1974. The company went bankrupt in 1978, and was placed in the receivership of brothers Jean-Claude and Patrick Mimran in 1980. The Mimrans purchased the company out of receivership by 1984 and invested heavily in its expansion. Under the Mimrans' management, Lamborghini's model line was expanded from the Countach to include the Jalpa sports car and the LM002 high-performance off-road vehicle.\n\nThe Mimrans sold Lamborghini to the Chrysler Corporation in 1987. After replacing the Countach with the Diablo and discontinuing the Jalpa and the LM002, Chrysler sold Lamborghini to Malaysian investment group Mycom Setdco and Indonesian group V'Power Corporation in 1994. In 1998, Mycom Setdco and V'Power sold Lamborghini to the Volkswagen Group where it was placed under the control of the group's Audi division. New products and model lines were introduced to the brand's portfolio and brought to the market and saw an increased productivity for the brand Lamborghini. In the late 2000s, during the worldwide financial crisis and the subsequent economic crisis, Lamborghini's sales saw a drop of nearly 50 per cent.\n\nIn 2021, the CEO of Lamborghini said that by 2024 all its models will be hybrid.\n\nAutomobiles produced\nLamborghini Motorsport Division Squadra Corse produces GT3 cars and cars for their Super Trofeo events based on the Gallardo and Huracán. Apart from them, the Squadra Corse builds cars upon customer request.\n\nGT3 and Super Trofeo Cars\nGallardo LP 570-4 Super Trofeo\nGallardo LP 560-4 Super Trofeo\nHuracán LP 620-2 Super Trofeo EVO\nHuracán LP 620-2 Super Trofeo EVO2\nHuracán Super Trofeo GT2\nHuracán GT3\nHuracán GT3 Evo\nHuracán GT3 Evo 2\nSpecial cars\nThese cars were built by Squadra Corse upon customer request.\n\nEssenza SCV12\nSC18 Alston\nSC20",
            "output": "Vokswagen Group owns Lamborghini through its subsidiary, Audi.\n\nLamborghini has produced the following Huracan motorsport models:\n- Huracán LP 620-2 Super Trofeo EVO\n- Huracán LP 620-2 Super Trofeo EVO2\n- Huracán Super Trofeo GT2\n- Huracán GT3\n- Huracán GT3 Evo\n- Huracán GT3 Evo 2",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.208372116088867,
            "normalized": 0.4881063188825335
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 567,
        "original_data": {
            "instruction": "Can you name a few of Leonardo's famous works",
            "input": "Leonardo is identified as one of the greatest painters in the history of art and is often credited as the founder of the High Renaissance. Despite having many lost works and fewer than 25 attributed major works—including numerous unfinished works—he created some of the most influential paintings in Western art. His magnum opus, the Mona Lisa, is his best known work and often regarded as the world's most famous painting. The Last Supper is the most reproduced religious painting of all time and his Vitruvian Man drawing is also regarded as a cultural icon. In 2017, Salvator Mundi, attributed in whole or part to Leonardo, was sold at auction for US$450.3 million, setting a new record for the most expensive painting ever sold at public auction.",
            "output": "Mona Lisa, The Last Supper, Vitruvian Man, Salvator Mundi",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.850515604019165,
            "normalized": 0.6715758868626186
        },
        "potential_entropy": {
            "strategy_component": 0.34,
            "strategy_details": {
                "raw": 0.68,
                "normalized": 0.68,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 568,
        "original_data": {
            "instruction": "What was the US civil war about?",
            "input": "",
            "output": "The US civil war had a complex start but the main aim was the preservation of slavery for the confederacy and the elimination of slavery for the union. Slavery was the principal political issue between the parties. \nThere was also an aspect of the confederacy wanting to secede from the Union to preserve slavery. The union did not want the slave stares to secede. After war broke out, it is interesting to note, no foreign government recognized the confederate government. \nBy the mid 1800s, the US had two distinct regions: free states with fast growing economies based on family farming, manufacturing, mining, business and transporting goods. The slave states were based on a plantation system and the only fast growth was in texas. These states were largely rural with few large cities and little industry except in close proximity to free states.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.2480969429016113,
            "normalized": 0.7851705551147461
        },
        "potential_entropy": {
            "strategy_component": 1.255,
            "strategy_details": {
                "raw": 2.51,
                "normalized": 2.51,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 569,
        "original_data": {
            "instruction": "My daughter's friend is lactose intolerant. What foods should we avoid when we take out to eat with us?",
            "input": "",
            "output": "You'll want to look out for anything that contains milk, butter, cream, buttermilk, cheese, or ice cream. This means pizza is off the table (it almost always has cheese), and much Italian cuisine like lasagna or alfredo sauces must be avoided. You also need to be careful about fried foods, because batters often contain milk or buttermilk. Ask about the ingredients of bread, as these can contain butter: croissants typically do. Even salads can be a concern, because many restaurants will add cheese as a topping, even without listing cheese on the menu. Desserts are also problematic, because chocolate nearly always contains milk, as do most ice creams. Look for vegan products where you can, such as vegan cheeses and vegan ice cream, which can be delicious.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.051542282104492,
            "normalized": 0.7290120806012835
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 570,
        "original_data": {
            "instruction": "What is non dual philosophy?",
            "input": "",
            "output": "The word non dual refers to things or experiences that happen to us which has the  characteristics of uniformity. As an example in daily life we see or experience highs and lows, haves and have nots that result in emotions or feelings of happiness or sadness. The concept of non duality is to go deep within and understand that everything is temporary, and experience things before human thought labels each experience as good or bad.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.8989665508270264,
            "normalized": 0.9711333002362933
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 571,
        "original_data": {
            "instruction": "What would you do if you had all the resources in the world and nothing to worry about?",
            "input": "",
            "output": "Wow, as dreamy as it sounds, I would love to live that dream. I would like to go by see, living on a cliff in a glass house that shows both the sunrise and sunset. And it is windy with a big garden where my dog can run and play. I really can not imagine the sun bask, knowing that I don't have to go back from this vacation ever that this is my home and I can just do this alone for the rest of my life :)",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.6540613174438477,
            "normalized": 0.9011603764125279
        },
        "potential_entropy": {
            "strategy_component": 1.39,
            "strategy_details": {
                "raw": 2.78,
                "normalized": 2.78,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 572,
        "original_data": {
            "instruction": "Which president was 1st to win a Nobel prize.",
            "input": "",
            "output": "Theodore Roosevelt was the 1st president to win a Nobel Peace Prize. Roosevelt won the prize for helping negotiate an end to the Russo-Japanese War in 1904-1905.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.344618320465088,
            "normalized": 0.527033805847168
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 573,
        "original_data": {
            "instruction": "On a visit to a movie theater, classify the likelihood of finding each of the following as either 'highly likely', 'likely', 'unlikely' or 'highly unlikely'. popcorn, M&Ms, sushi, salad, pasta.",
            "input": "",
            "output": "It is highly likely that popcorn would be found at a movie theater.\nIt is highly likely that M&Ms would be found at a movie theater.\nIt is highly unlikely that sushi would be found at a movie theater.\nIt is highly unlikely that salad would be found at a movie theater.\nIt is highly unlikely that pasta would be found at a movie theater.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.4,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.6,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 1.8103766441345215,
            "normalized": 0.3743933268955776
        },
        "potential_entropy": {
            "strategy_component": 0.605,
            "strategy_details": {
                "raw": 1.21,
                "normalized": 1.21,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.7000000000000001,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.6,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 574,
        "original_data": {
            "instruction": "Why is Grand Canyon a big deal?",
            "input": "The Grand Canyon is a river valley in the Colorado Plateau that exposes uplifted Proterozoic and Paleozoic strata, and is also one of the six distinct physiographic sections of the Colorado Plateau province. Even though it is not the deepest canyon in the world (Kali Gandaki Gorge in Nepal is much deeper), the Grand Canyon is known for its visually overwhelming size and its intricate and colorful landscape. Geologically, it is significant because of the thick sequence of ancient rocks that are well preserved and exposed in the walls of the canyon. These rock layers record much of the early geologic history of the North American continent.",
            "output": "While Gran Canyon is not the deepest canyon on earth, it bears significant geological significance because of its exposed yet well-preserved rock layers. The rock layers record much of the early geologic history of the North American continent",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1547420024871826,
            "normalized": 0.7584977149963379
        },
        "potential_entropy": {
            "strategy_component": 0.41,
            "strategy_details": {
                "raw": 0.82,
                "normalized": 0.82,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 575,
        "original_data": {
            "instruction": "Classify each of the following as either a lepton or a hadron: electron, proton, neutron, pi meson, electron neutrino, muon neutrino, tau neutrino.",
            "input": "",
            "output": "Protons, neutrons and pi mesons are members of the hadron family of particles. Hadrons are particles that interact through the strong force.\n\nElectrons, electron neutrinos, muon neutrinos and tau neutrinos are members of the lepton family. Leptons interact through the weak force, and if charged, the electromagnetic force.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 1.9723138809204102,
            "normalized": 0.4206611088344029
        },
        "potential_entropy": {
            "strategy_component": 0.47000000000000003,
            "strategy_details": {
                "raw": 0.9400000000000001,
                "normalized": 0.9400000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 576,
        "original_data": {
            "instruction": "What are some noteworthy facts from Gabriel García Márquez present on the text?",
            "input": "Gabriel José de la Concordia García Márquez (American Spanish: [ɡaˈβɾjel ɣaɾˈsi.a ˈmaɾkes] (listen); 6 March 1927 – 17 April 2014) was a Colombian novelist, short-story writer, screenwriter, and journalist, known affectionately as Gabo ([ˈɡaβo]) or Gabito ([ɡaˈβito]) throughout Latin America. Considered one of the most significant authors of the 20th century, particularly in the Spanish language, he was awarded the 1972 Neustadt International Prize for Literature and the 1982 Nobel Prize in Literature. He pursued a self-directed education that resulted in leaving law school for a career in journalism. From early on he showed no inhibitions in his criticism of Colombian and foreign politics. In 1958, he married Mercedes Barcha Pardo; they had two sons, Rodrigo and Gonzalo.\n\nGarcía Márquez started as a journalist and wrote many acclaimed non-fiction works and short stories, but is best known for his novels, such as One Hundred Years of Solitude (1967), Chronicle of a Death Foretold (1981), and Love in the Time of Cholera (1985). His works have achieved significant critical acclaim and widespread commercial success, most notably for popularizing a literary style known as magic realism, which uses magical elements and events in otherwise ordinary and realistic situations. Some of his works are set in the fictional village of Macondo (mainly inspired by his birthplace, Aracataca), and most of them explore the theme of solitude. He is the most-translated Spanish-language author.\n\nUpon García Márquez's death in April 2014, Juan Manuel Santos, the president of Colombia, called him \"the greatest Colombian who ever lived.\"",
            "output": "Gabriel García Márquez was born in Colombia and is recognized as one of the most significant authors of the 20th century. He won a Nobel Prize in Literature in 1982. His work's literary style known as magic realism brought him critical acclaim and commercial success. Some of his works used places of his live as inspiration, and most of them explore the theme of solitude. He is the most-translated Spanish-language author. He was called \"the greates Colombian who ever lived\"by the president of Colombia at the time of his death.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.687141180038452,
            "normalized": 0.6248974800109863
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 577,
        "original_data": {
            "instruction": "Identify which instrument is string or woodwind: Lirone, Fife",
            "input": "",
            "output": "Fife is woodwind, Lirone is string.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.299728870391846,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 578,
        "original_data": {
            "instruction": "Which of the 10 largest earthquakes was the deadliest?",
            "input": "Largest Earthquakes ever recorded\nThe 10 largest magnitudes in recorded history\n1. Valdivia, Chile 22 May 1960 (magnitude 9.5)\nThis earthquake killed 1655 people, injured 3000 and displaced two million. It caused US$550 million damage in Chile, while the tsunami that it spawned caused deaths and damage as far away as Hawaii, Japan and the Philippines. The 'rupture zone' of the quake was more than 1000 km long. Two days after the initial quake, the nearby volcano Puyehue erupted, sending ash and steam up to 6 km into the atmosphere over a period of several weeks.\n\n2. Prince William Sound, Alaska 28 March 1964 (magnitude 9.2)\nCompared to the Chilean earthquake, this earthquake was less damaging: the resulting tsunami took 128 lives and caused overall US$311 million in damage. The earthquake was felt mainly over Alaska, as well as some places in Canada, while the tsunami created by it caused damage as far away as Hawaii. The most damage was sustained by the city of Anchorage, 120 km north-west of the epicentre. Shaking from the quake itself is reported to have lasted for three minutes.\n\n3. Sumatra, Indonesia 26 December 2004 (magnitude 9.1)\nIn terms of damage and loss of life, the scale of the disaster caused by the resulting Boxing Day Tsunami was enormous. In total, 227,900 people were killed or presumed dead, with around 1.7 million displaced over 14 countries in South Asia and East Africa. The epicentre was 250 km south-east of Band Aceh, Indonesia, at a depth of 30 km. Several days later on 28 December, a mud volcano began erupting near Baratang, Andamar Islands, which is thought to have been associated with the earthquake.\n\n4. Sendai, Japan 11 March 2011 (magnitude 9.0)\nSo far the official death toll stands at several thousand from the combined effect of the powerful earthquake, aftershocks and the tsunami. However, the total is expected to rise, with some estimates of a final toll of over 10,000. Economic impacts are expected to be huge, with the shutting down of nuclear reactors which many industries rely on for power.\n\n5. Kamchatka, Russia 4 November 1952 (magnitude 9.0)\nThis earthquake generated a tsunami that caused widespread damage in the Hawaiian Islands. Property damage was estimated at around US$1,000,000. Some reports describe waves of over 9 m high at Kaena Point, Oahu. A farmer on Oahu reported the loss of six cows to the tsunami, but no people were reported killed.\n\n6. Bio-bio, Chile 27 February 2010 (magnitude 8.8)\nThis earthquake and subsequent tsunami killed at least 521 people, with 56 missing and 12,000 injured. More than 800,000 people were displaced with a total of 1.8m people affected across Chile, where damage was estimated at US$30 billion. The epicentre was 335 km south-west of Santiago, at a depth of 35 km. A minor tsunami travelled across the Pacific causing damage to boats as far away as San Diego, California.\n\n7. Off the coast of Ecuador 31 January 1906 (magnitude 8.8)\nThis earthquake caused a tsunami that is reported to have killed between 500 and 1,500 in Ecuador and Colombia. The tsunami travelled as far north as San Francisco, on the west coast of the US, and west to Hawaii and Japan. The tsunami took roughly 12 hours to cross the Pacific to Hilo, Hawaii.\n\n8. Rat Islands, Alaska 2 April 1965 (magnitude 8.7)\nThe worst of the damage attributed to this earthquake was caused by a tsunami, reported to be about 10 m high on Shemya Island. The wave caused flooding on Amchitka Island, causing US$10,000 in property damage. No deaths or injuries were reported.\n\n9. Sumatra, Indonesia 28 March 2005 (magnitude 8.6)\nThis earthquake killed 1313, with over 400 people injured by the tsunami as far away as Sri Lanka. The epicentre was 205 km north-west of Sibolga, Sumatra, at a depth of 30 km. This region, also the site of the 2004 Boxing Day Tsunami, is particularly geologically active, with three of the 15 biggest known earthquakes having happened here.\n\n10. Assam - Tibet 15 August 1950 (magnitude 8.6)\nThis inland earthquake caused widespread damages to buildings as well as large landslides. 780 people were killed in eastern Tibet, with many villages and towns affected across Assam, China, Tibet and India. Oscillations to lake levels occurred as far away as Norway. The total death toll is likely to be higher, as no definitive total was ever estimated. While the earthquake itself is known as the Assam Earthquake, it is believed the epicentre may have been in Tibet.",
            "output": "The 2004 earthquake on Sumatra, Indonesia was the deadliest. It caused a tsunami that killed more than 200,000 people.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 1.0,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.0,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.1088385581970215,
            "normalized": 0.4596681594848633
        },
        "potential_entropy": {
            "strategy_component": 0.22250000000000003,
            "strategy_details": {
                "raw": 0.44500000000000006,
                "normalized": 0.44500000000000006,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.0,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 579,
        "original_data": {
            "instruction": "What are some typical attributes of a tortoiseshell cat?",
            "input": "",
            "output": "Common attributes of a tortoiseshell (also referred to as \"tortie\") cat include golden eyes, a mottled coat that is mostly black and orange, and pink and/or black leathers. There is also an assumption that tortie cats are sassy, which is referenced as \"tortietude.\"",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.2862229347229004,
            "normalized": 0.7960636956351144
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 580,
        "original_data": {
            "instruction": "Classify each of the following as old or young: baby, child, grandma, elderly, grandpa, octogenarian, infant",
            "input": "",
            "output": "Baby: Young\nChild: Young\nGrandma: Old\nElderly: Old\nGrandpa: Old\nOctogenarian: Old\nInfant: Young",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.6657822132110596,
            "normalized": 0.6187949180603027
        },
        "potential_entropy": {
            "strategy_component": 0.7175,
            "strategy_details": {
                "raw": 1.435,
                "normalized": 1.435,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.0,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 581,
        "original_data": {
            "instruction": "Name some famous rock bands from the San Francisco Bay Area.",
            "input": "",
            "output": "Many famous rock bands have come from the San Francisco Bay area. These bands include the Grateful Dead, Journey, The Jefferson Airplane, Metallica, Santana, and Green Day.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.5996742248535156,
            "normalized": 0.5999069213867188
        },
        "potential_entropy": {
            "strategy_component": 1.09,
            "strategy_details": {
                "raw": 2.18,
                "normalized": 2.18,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 582,
        "original_data": {
            "instruction": "How do I save for retirement?",
            "input": "",
            "output": "First thing is to make a plan and the key is to start saving today no matter how small that amount might be.  You will be refining this plan as you move forward to achieving your financial goals.  You should always \"Pay Yourself\" first by allocating a percentage of your pay checks into a company 401K retirement plan or an IRA or Roth IRA.  A certain portion of your monthly pay will be pulled out tax free and put into a retirement savings account.  You should consult with a tax professional to understand the tax implications.  Also, you can work with a professional planner to help create a comprehensive long term plan.  This retirement plan will take into account your target financial goals, desired annual spend, investments, social security, pensions, and other financial sources.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.00986647605896,
            "normalized": 0.7171047074454171
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 583,
        "original_data": {
            "instruction": "What is the 80/20 rule?",
            "input": "",
            "output": "The 80/20 rule can be applied in many contexts. Casually, it may refer to the observation that often 20% of the effort yields 80% of the benefit and the corollary that the last 20% of the benefit requires 80% of the effort. The 80/20 rule also describes a property that is empirically true of the frequency distribution of many sets, such as 20% of unique words accounting for 80% of words in a book or 20% of companies in a stock portfolio accounting for 80% of the return.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 2.1434361934661865,
            "normalized": 0.4695531981331961
        },
        "potential_entropy": {
            "strategy_component": 1.0075,
            "strategy_details": {
                "raw": 2.015,
                "normalized": 2.015,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 584,
        "original_data": {
            "instruction": "Please describe what Business Process Mapping is and why it is used",
            "input": "Business process mapping refers to activities involved in defining what a business entity does, who is responsible, to what standard a business process should be completed, and how the success of a business process can be determined.\n\nThe main purpose behind business process mapping is to assist organizations in becoming more effective. A clear and detailed business process map or diagram allows outside firms to come in and look at whether or not improvements can be made to the current process.\n\nBusiness process mapping takes a specific objective and helps to measure and compare that objective alongside the entire organization's objectives to make sure that all processes are aligned with the company's values and capabilities.\n\nInternational Organization for Standardization or ISO 9001 : 2015 encourages a process approach to quality management. It is important to understand how each process relates to other processes within the organization and how those interactions impact Quality Management.",
            "output": "Business Process Mapping describes the capture of a set of business activities that deliver an outcome.  Mapping is usually performed using visual tools that use standardized shapes to indicate entities.  Mapping involves capturing activities, the roles performing the activities, systems and tools used to execute the activities and inputs and outputs from the activities.  Process mapping is frequently conducted in cross functional workshops with a facilitator capturing the activities performed by Subject Matter Experts (SMEs).  Common uses of process mapping include:\n* automation\n* compliance documentation\n* business process improvement\n* standardization",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.342468500137329,
            "normalized": 0.8121338571820941
        },
        "potential_entropy": {
            "strategy_component": 0.41500000000000004,
            "strategy_details": {
                "raw": 0.8300000000000001,
                "normalized": 0.8300000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 585,
        "original_data": {
            "instruction": "What is the difference between check and checkmate?",
            "input": "When a king is under immediate attack, it is said to be in check. A move in response to a check is legal only if it results in a position where the king is no longer in check. This can involve capturing the checking piece; interposing a piece between the checking piece and the king (which is possible only if the attacking piece is a queen, rook, or bishop and there is a square between it and the king); or moving the king to a square where it is not under attack. Castling is not a permissible response to a check.\n\nThe object of the game is to checkmate the opponent; this occurs when the opponent's king is in check, and there is no legal way to get it out of check. It is never legal for a player to make a move that puts or leaves the player's own king in check. In casual games, it is common to announce \"check\" when putting the opponent's king in check, but this is not required by the rules of chess and is usually not done in tournaments.",
            "output": "When a king is about to be captured in the next opponents move, then the king is in check. If the king cannot get out of check, then the king is checkmated and loses the game",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1566662788391113,
            "normalized": 0.7590475082397461
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 586,
        "original_data": {
            "instruction": "Classify each activity as athletic or academic: reading, running, studying, jumping, writing, sprinting",
            "input": "",
            "output": "Reading: Academic\nRunning: Athletic\nStudying: Academic\nJumping: Athletic\nWriting: Academic\nSprinting: Athletic",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 2.642308235168457,
            "normalized": 0.6120880671909877
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 587,
        "original_data": {
            "instruction": "Why are cats better than dogs?",
            "input": "",
            "output": "This is a subjective question whose answer depends on individual preferences. There are many reasons that some people prefer cars over dogs. For example, some cats are very affectionate and become very attached to their owners. They have personalities that appeal to some people more than dogs. They also may be easier to care for and cheaper to own. Because they are typically smaller house pets, they can be easier to keep clean. These are just a few examples of how cats may be preferred to dogs for some people.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.6660878658294678,
            "normalized": 0.618882247379848
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 588,
        "original_data": {
            "instruction": "Which American cultural artifacts are most known worldwide?",
            "input": "",
            "output": "Historically speaking, blue jeans, rock and roll and Coca Cola are considered quintessentially American. These icons became especially significant among youth in eastern Europe during the fall of communism in the 1980s and 1990s. In more modern times, you could argue that the iPhone and tech giants like Google and Facebook are most recognizable. However, in all times, the President of the United States is perhaps the most famous person in the world.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.9205644130706787,
            "normalized": 0.6915898323059082
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 589,
        "original_data": {
            "instruction": "What are the various passing routes in football?",
            "input": "",
            "output": "Streak\nPost\nCrossing route\nHitch\nComeback\nDigg\nOut\nFlat \nWheel",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.765840530395508,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.06,
            "strategy_details": {
                "raw": 2.12,
                "normalized": 2.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 590,
        "original_data": {
            "instruction": "Identify the bird from the list: Hoot, Coot, Foot",
            "input": "",
            "output": "Coot",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 12.653923034667969,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 591,
        "original_data": {
            "instruction": "Categorize the following ingredients as meat, cheese, or spread: mayonnaise, ham, swiss, muenster, turkey, mustard",
            "input": "",
            "output": "mayonnaise - spread\nham - meat\nswiss - cheese\nmuenster - cheese\nturkey - meat\nmustard - spread",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5474910736083984,
            "normalized": 0.8707117353166852
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 592,
        "original_data": {
            "instruction": "I want to write a some new articles for my data blog, can you give me some snappy ideas?",
            "input": "",
            "output": "Lakehouse vs Warehouse - Hype vs Reality\nETL in the 2020s - What's in and what's out\nIs the Data Warehouse dead - modern analytics for the modern era\nOpen data sharing - The whys, hows and gotchas",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.8,
            "s_out_div": 0.85,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.034377098083496,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.9574999999999999,
            "strategy_details": {
                "raw": 1.9149999999999998,
                "normalized": 1.9149999999999998,
                "components": {
                    "instruction": 0.3,
                    "input": 1.2999999999999998,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 593,
        "original_data": {
            "instruction": "Identify which instrument is string or woodwind: Igil, Koudi",
            "input": "",
            "output": "Koudi is woodwind, Igil is string.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.905704975128174,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 594,
        "original_data": {
            "instruction": "what is the Bieh State",
            "input": "Bieh State (previously named Eastern Bieh State) was a state in South Sudan that existed between 2 October 2015 and 22 February 2020. It was located in the Greater Upper Nile region and it bordered Fangak to the west, Central Upper Nile to the north, Latjoor and Akobo to the east, and Jonglei to the south.\n\nHistory\nOn 2 October 2015, President Salva Kiir issued a decree establishing 28 states in place of the 10 constitutionally established states. The decree established the new states largely along ethnic lines. A number of opposition parties and civil society groups challenged the constitutionality of the decree. Kiir later resolved to take it to parliament for approval as a constitutional amendment. In November the South Sudanese parliament empowered President Kiir to create new states.",
            "output": "Bieh State (previously named Eastern Bieh State) was in South Sudan between 2 October 2015 and 22 February 2020. It was located in the Greater Upper Nile region, bordering Fangak to the west, Central Upper Nile to the north, Latjoor and Akobo to the east, and Jonglei to the south. On 2 October 2015, President Salva Kiir issued a decree establishing 28 states instead of the ten constitutionally established states. The legislation established the new states primarily along ethnic lines.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0365240573883057,
            "normalized": 0.7247211592538017
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 595,
        "original_data": {
            "instruction": "How many bowl games have the University of Georgia football team won?",
            "input": "The Georgia Bulldogs football program represents the University of Georgia in the sport of American football. The Bulldogs compete in the Football Bowl Subdivision (FBS) of the National Collegiate Athletic Association (NCAA) and the Eastern Division of the Southeastern Conference (SEC). They play their home games at historic Sanford Stadium on the university's Athens, Georgia, campus. Georgia claims four consensus national championships (1942, 1980, 2021, and 2022); while the AP and Coaches Polls have each voted the Bulldogs the national champion three times (1980, 2021, and 2022). Georgia has also been named the National Champion by at least one polling authority in four other seasons (1920, 1927, 1946 and 1968).\n\nThe Bulldogs' other accomplishments include 16 conference championships, of which 14 are SEC championships, second-most in conference history, and appearances in 61 bowl games, second-most all-time.",
            "output": "The University of Georgia football team is defined by greatness.   Known as the Georgia Bulldogs (Dawgs) and compete in the Division 1 Southeastern Conference (SEC).   They play in the historic Sanford Stadium in Athens, Georgia and have appeared in 61 bowl games, second-most all time.   In 2022 and 2023 the Georgia Bulldogs won 2 consecutive National Championships.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0834808349609375,
            "normalized": 0.7381373814174107
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 596,
        "original_data": {
            "instruction": "Without quoting directly from the text give me a summary of what skateboarding is.",
            "input": "Skateboarding is an action sport originating in the United States that involves riding and performing tricks using a skateboard, as well as a recreational activity, an art form, an entertainment industry job, and a method of transportation. Skateboarding has been shaped and influenced by many skateboarders throughout the years. A 2009 report found that the skateboarding market is worth an estimated $4.8 billion in annual revenue, with 11.08 million active skateboarders in the world.",
            "output": "Skateboarding is an action sport originating in the United States that involves riding and performing tricks using a skateboard, as well as a recreational activity, an art form, an entertainment industry job, and a method of transportation. Skateboarding has been shaped and influenced by many skateboarders throughout the years. A 2009 report found that the skateboarding market is worth an estimated $4.8 billion in annual revenue, with 11.08 million active skateboarders in the world. In 2016, it was announced that skateboarding would be represented at the 2020 Summer Olympics in Tokyo, for both male and female teams.\n\nSince the 1970s, skateparks have been constructed specifically for use by skateboarders, freestyle BMXers, aggressive skaters, and more recently, scooters. However, skateboarding has become controversial in areas in which the activity, although illegal, has damaged curbs, stoneworks, steps, benches, plazas, and parks.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.9,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.6826391220092773,
            "normalized": 0.6236111777169364
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 597,
        "original_data": {
            "instruction": "A 30-year-old member at your fitness center has been weight training for 4 months and claims to have made significant gains in strength. Twice a week, he performs 1 set of 12-15 repetitions using 8 weight machines. Over the last 6 weeks, he's noticed that he isn’t achieving the strength gains that he’s used to. How would you modify his training routine to optimize gains in muscular strength?",
            "input": "",
            "output": "First of all, he’s been performing the same routine over the last 4 months, which means his body may have adapted to the specific sets, repetition ranges, and types of exercises he performs, leading to less strength gains. I would recommend him to switch up his routine! He can consider doing more sets per exercise e.g. 3 sets with lower repetitions e.g. 8 repetitions. He can also consider switching weight machines to free weights, which can help target more muscles and larger muscle groups. He can also increase the number of days he trains a week, from two to three. He can also do well to remember to properly warm up, cool down, stretch, and order his exercises properly (from larger muscle groups to smaller).",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.1125473976135254,
            "normalized": 0.7464421136038644
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 598,
        "original_data": {
            "instruction": "Extract the Ferrari automobiles that were introduced from 2020 onwards.  List them by year, and separate them with a comma.",
            "input": "Current models\nModel\tCalendar year\nintroduced\tCurrent model\tVehicle description\nIntroduction\tUpdate/facelift\n2019 Ferrari 812 Superfast S-A 6.5.jpg\n812 Superfast\t2017\t2017\t–\tFront mid-engine, rear-wheel-drive grand tourer.\nFerrari Portofino M IMG 4351.jpg\nPortofino M\t2017\t2017\t2021\tRetractable hardtop convertible grand tourer.\nFerrari Monza SP1, Paris Motor Show 2018, IMG 0643.jpg\nMonza SP1\t2019\t2019\t–\tLimited production single-seater sports car, part of the new Icona range.\n2019 Ferrari Monza SP2.jpg\nMonza SP2\t2019\t2019\t–\tLimited production two-seater sports car, part of the new Icona range.\n2020 Ferrari F8 Tributo 3.9.jpg\nF8\t2019\t2019\t–\tMid-engine sports car that replaced the Ferrari 488.\nRed 2019 Ferrari SF90 Stradale (48264238897) (cropped).jpg\nSF90 Stradale\t2019\t2019\t–\tMid-engine, plug-in hybrid sports car.\nFerrari Roma IMG 5355.jpg\nRoma\t2020\t2020\t–\tGrand tourer sports car.\nFerrari 296 GTB - Paris 06.jpg\n296 GTB\t2022\t2022\t–\tMid-engine, plug-in hybrid sports car.\n2022-08-21 Monterrey Ferrari SP3.jpg\nDaytona SP3\t2022\t2022\t–\tLimited production mid-engine sports car, part of the new Icona range.\n2023 Ferrari Purosangue.jpg\nPurosangue\t2022\t2023\t–\tFerrari's first SUV; uses the same platform as the Roma.",
            "output": "2020 Roma, 2022 296 GTB, 2022 Daytona SP3, 2022 Purosangue",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.88,
            "s_out_bg": 0.82
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.12,
            "s_out_bg": 0.18000000000000005
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.5686097145080566,
            "normalized": 0.8767456327165876
        },
        "potential_entropy": {
            "strategy_component": 0.3025,
            "strategy_details": {
                "raw": 0.605,
                "normalized": 0.605,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.12,
                    "out_bg": 0.18000000000000005
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 599,
        "original_data": {
            "instruction": "Who called Timothy Leary a brave neuronaut?",
            "input": "Timothy Francis Leary (October 22, 1920 – May 31, 1996) was an American psychologist and author known for his strong advocacy of psychedelic drugs. Evaluations of Leary are polarized, ranging from bold oracle to publicity hound. He was \"a hero of American consciousness\", according to Allen Ginsberg, and Tom Robbins called him a \"brave neuronaut\"",
            "output": "Tom Robbins called Timothy Leary a \"brave neuronaut\".",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 6.5659332275390625,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 600,
        "original_data": {
            "instruction": "Give me 5 ways to reduce my electricity bill.",
            "input": "",
            "output": "The following are 5 ways you can reduce your electricity bill:\n1. Turn off your air conditioning or heating\n2. Install insulation\n3. Take shorter showers\n4. Turn off the lights when not in use\n5. Buy energy efficient appliances",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.312373399734497,
            "normalized": 0.5178209713527134
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 601,
        "original_data": {
            "instruction": "Using this definition, can you summarize why hyperventilation is considered dangerous?",
            "input": "Voluntary hyperventilation before beginning voluntary apnea is commonly believed to allow the person involved to safely hold their breath for a longer period. In reality, it will give the impression that one does not need to breathe, while the body is actually experiencing a blood-oxygen level that would normally, and indirectly, invoke a strong dyspnea and eventually involuntary breathing. Some have incorrectly attributed the effect of hyperventilation to increased oxygen in the blood, not realizing that it is actually due to a decrease in CO2 in the blood and lungs. Blood leaving the lungs is normally fully saturated with oxygen, so hyperventilation of normal air cannot increase the amount of oxygen available, as oxygen in blood is the direct factor. Lowering the CO2 concentration increases the pH of the blood, thus increasing the time before blood becomes acidic enough so the respiratory center becomes stimulated, as described above. \nThe primary urge to breathe is triggered by rising carbon dioxide (CO2) levels in the bloodstream. Carbon dioxide builds up in the bloodstream when oxygen is metabolized and it needs to be expelled as a waste product. The body detects carbon dioxide levels very accurately and relies on this as the primary trigger to control breathing. Hyperventilation artificially depletes the resting concentration of carbon dioxide causing a low blood carbon dioxide condition called hypocapnia. Hypocapnia reduces the reflexive respiratory drive, allowing the delay of breathing and leaving the diver susceptible to loss of consciousness from hypoxia. For most healthy people, the first sign of low oxygen levels is a greyout or unconsciousness: there is no bodily sensation that warns a diver of an impending blackout.\nIf a person loses consciousness underwater, there is considerable danger that they will drown. An alert diving partner or nearby lifeguard would be in the best position to rescue such a person. Static apnea blackout occurs at the surface when a motionless diver holds their breath long enough for the circulating oxygen in blood to fall below that required for the brain to maintain consciousness. It involves no pressure changes in the body and is usually performed to enhance breath-hold time. It should never be practiced alone, but under strict safety protocols with a safety guard or equipment beside the diver.",
            "output": "Hyperventilation does not increase the amount of oxygen available but artificially decreases carbon dioxide (CO2) levels in blood and lungs. \nBecause the primary urge to breathe is triggered by rising carbon dioxide levels, it gives false impression that one does not need to breath. This can lead to sudden lost of consciousness. If this happens underwater, there is a considerable risk of drowning. \nThis is why hyperventilation is considered as dangerous and should be avoided.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.944103956222534,
            "normalized": 0.6983154160635812
        },
        "potential_entropy": {
            "strategy_component": 0.30000000000000004,
            "strategy_details": {
                "raw": 0.6000000000000001,
                "normalized": 0.6000000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.15000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 602,
        "original_data": {
            "instruction": "What is meant by \"dual federalism,\" and how is it different from \"cooperative federalism\"?",
            "input": "",
            "output": "Federalism is a system of government where the power is distributed across different levels. In the United States, we have a system with two levels, a federal or national government, and underneath that state governments.  Dual federalism is a period between 1819 and 1936 where there was a clear separation of powers between the federal government and the state governments.  During this time the “national government left the states largely to their own devices in major policy areas” which included “education, transportation, and civil rights, among others”.  \nDuring the dual federalism period, two important cases set the stage for the next time period called “cooperative federalism” (1936-present).  The first case was McCulloch v. Maryland which cited the Constitution's  supremacy clause and implied powers of the federal government in a verdict that prohibited the state of Maryland from taxing a federal bank.  The second case was Gibbons v. Ogden which cited the power of the federal government to regulate interstate commerce giving Gibbons a monopoly to ferry people from New Jersey to New York.  These two cases were used by the Federal government during the New Deal as precedent to enable cooperative federalism which is “an intermingling of power, policy, and resources among the national and state governments”. The “government established programs providing federal funding in areas that were traditionally the exclusive domains of the state, such as health, transportation, education, employment, and poverty relief”.  This differs from dual federalism in that we now have a mixing and sharing of powers between the state and national governments.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            1,
            2
        ],
        "entropy": {
            "raw": 2.625153064727783,
            "normalized": 0.6071865899222237
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 603,
        "original_data": {
            "instruction": "Are there any benefits of fasting?",
            "input": "",
            "output": "Fasting is the method by which one can bring a conscious process to ingestion. It is not simply denying food and water to the system. Rather, it is a process of reducing the need for food. Fasting provides the digestive and assimilation process a break so that the rest of the body begins to excrete on the cellular level. This helps to remove all the toxic impurities that had accumulated in the cells over a long period of time. Hence intermittent fasting is extremely beneficial to the body.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.7,
            "s_out_div": 0.5,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.5,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.1331512928009033,
            "normalized": 0.7523289408002581
        },
        "potential_entropy": {
            "strategy_component": 0.86,
            "strategy_details": {
                "raw": 1.72,
                "normalized": 1.72,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.4799999999999999
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.5,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 604,
        "original_data": {
            "instruction": "Let me know some different ways that someone can plan a trip",
            "input": "",
            "output": "Here are some different ways you can plan a trip:\n- Research on forums such as TripAdvisor and discover how other people are planning trips\n- Talk to a travel agent in person\n- Look at organised package tours\n- Use a traveller aggregator website to explore travel routes\n- Go to a travel show and talk to different companies offering travel experiences",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.6353139877319336,
            "normalized": 0.8958039964948382
        },
        "potential_entropy": {
            "strategy_component": 0.6050000000000001,
            "strategy_details": {
                "raw": 1.2100000000000002,
                "normalized": 1.2100000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 605,
        "original_data": {
            "instruction": "Which of the following are women scientists: Marie Curie, Elizabeth Blackwell, Jane Goodall, Rosalind Franklin, Ada Lovelace, Maria Mitchell, Archimedes, Albert Einstein, Ernest Rutherford, Alfred Nobel, Alexander Fleming, Michael Faraday, Niels Bohr, Stephen Hawking, Alan Turing.",
            "input": "",
            "output": "Marie Curie, Elizabeth Blackwell, Jane Goodall, Rosalind Franklin, Ada Lovelace, Maria Mitchell are women scientists.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.890393018722534,
            "normalized": 0.6829694339207241
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 606,
        "original_data": {
            "instruction": "Who was the Apostle Paul?",
            "input": "",
            "output": "Apostle Paul, who was also known as Saint Paul, was a Christian Apostle who spread the teachings of Jesus Christ. According to the book of Acts in the bible, Paul was Pharisees and lead persecution of early disciples of Jesus. He encountered Jesus on the road to Damascus to find Christians there and bring them bound to Jerusalem. But on his way a light brighter than a sun shone around him and those with him from heaven at mid day. Then he fell to the ground and heard a voice, \"Saul, saul, why are you persecuting me?\". and he said \"who are you?\" then the lord said, \"I am Jesus, whom you are persecuting. It is hard for you to kick against the goads.\" Paul asked \"What do you want me to do?\" then the lord said \"Arise and go into the city, and you will be told what you must do.\" Having been made blind, he was lead to the city and days later his sight was restored by Ananias of Damascus. Then Paul was baptized and immediately started to proclaim Christian message. He proclaimed mainly to the gentiles and he is also known as an Apostle to the gentiles.  He planted several churches in Galatia, Philippi, Thessalonika, Corinth and Ephesus.Thirteen or Fourteen of the twenty seven books in the new Testament was written by Paul through the guidance of the holy spirit.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            1,
            2
        ],
        "entropy": {
            "raw": 2.8899290561676025,
            "normalized": 0.6828368731907436
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 607,
        "original_data": {
            "instruction": "What is calcium chloride?",
            "input": "",
            "output": "Calcium Chloride is an inorganic salt used as an electrolyte in bottled water.  Calcium chloride ingested in large amounts can cause health problems, but the trace amounts found in your water bottles is safe.  They regulate fluid levels in the body, keep the blood’s pH level balanced and enable muscle contractions like the beating of your heart.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.431062698364258,
            "normalized": 0.8374464852469308
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 608,
        "original_data": {
            "instruction": "Which is a species of fish? Steelhead or Metalhead",
            "input": "",
            "output": "Steelhead",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.1,
            "s_out_div": 0.05,
            "s_out_dens": 0.9,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 13.206644058227539,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3424999999999998,
            "strategy_details": {
                "raw": 2.6849999999999996,
                "normalized": 2.6849999999999996,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.65,
                    "output": 0.855
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.9,
                    "out_div": 0.95,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 609,
        "original_data": {
            "instruction": "Why is Chicago known as the Windy City",
            "input": "",
            "output": "Many believe that Chicago is known as the Windy City because it has harsh winters and strong cold winds off the shores of Lake Michigan. Even in the summer, the winds swirling off the lake can change the outcome of a baseball game at Wrigley Field.  While the weather pattern does in fact exist, Chicago is not the windiest city, nor is this the reason for the nickname. Many believe the term is a derogatory reference to  Chicago's vitriolic politics and self-important politicians who are  full of  hot air.  References to Chicago as the Windy City have been found all the way back to the late 1800s.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.084846258163452,
            "normalized": 0.7385275023324149
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 610,
        "original_data": {
            "instruction": "List 10 best switch games of all time.",
            "input": "",
            "output": "According to the metacritic website, the top 10 switch games of all time are:\n\n1. The Legend of Zelda: Breath of the Wild\n2. Super Mario Odyssey\n3. Portal Companion Collection\n4. The House in Fata Morgana - Dreams of the Revenants Edition\n5. Tetris Effect: Connected\n6. Metroid Prime Remastered\n7. Persona 5 Royal\n8. Hades\n9. Divinity: Original Sin II - Definitive Edition\n10. Ori and the Will of the Wisps\n\nThe top 10 lists from other websites may vary.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.683310031890869,
            "normalized": 0.6238028662545341
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 611,
        "original_data": {
            "instruction": "Classify the following as a 'Country', 'State' or 'City': California, Sacramento, Chicago, Singapore, New Delhi, London, Tennessee, New York",
            "input": "",
            "output": "California: State\nSacramento: City \nChicago: City\nSingapore: Both Country and City\nNew Delhi: City\nLondon: City\nTennessee: City\nNew York: Both State and City",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.9579219818115234,
            "normalized": 0.702263423374721
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 612,
        "original_data": {
            "instruction": "What's changed and unchanged for IKEA pencils through the years?",
            "input": "IKEA pencils are small pencils provided for free in IKEA stores worldwide. They are found in small boxes attached to poles, together with maps, measuring tapes and shopping forms. The IKEA pencil has been known for the wide variety of designs. Through the years the color changed from blue, to yellow to the natural color of wood. Despite the different colors, its dimensions have always been 7×87mm. Their common in-store application is for notetaking, with customers making note of selected items from product tags onto their notepads and visiting the self-service furniture warehouse to collect their showroom products in flat pack form, using their notes to locate their products.",
            "output": "The color changed from blue, to yellow to the natural color of wood. The dimensions of the pencil has never changed which is always 7 x 87mm.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.321747779846191,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.56,
            "strategy_details": {
                "raw": 1.12,
                "normalized": 1.12,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 613,
        "original_data": {
            "instruction": "What is the best way to travel across Japan as a foreign tourist staying on a tourist visa?",
            "input": "",
            "output": "JR rail pass is probably the most efficient and economic option to get around the country. You need to purchase this pass in advance before arriving in the country. Once you are in Japan, you go to a train station and activate your pass. If you want to travel around and visit multiple cities in a short time, this pass is definitely worth it.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.7403626441955566,
            "normalized": 0.6401036126273019
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 614,
        "original_data": {
            "instruction": "What is the reported biodiversity of Costa Rica?",
            "input": "",
            "output": "Costa Rica has from a 5% to a 6% of the World's biodiversity.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.8,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.499833345413208,
            "normalized": 0.8570952415466309
        },
        "potential_entropy": {
            "strategy_component": 1.38,
            "strategy_details": {
                "raw": 2.76,
                "normalized": 2.76,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.8,
                    "output": 0.5999999999999999
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.9,
                    "inp_complex": 0.9,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.6,
                    "out_dens": 0.5,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 615,
        "original_data": {
            "instruction": "I'm really good at maths but not great at science, which of the following educational courses would I be more likely to succeed with? algebra, biology, matrices, probability, statistics, geology, and chemistry.",
            "input": "",
            "output": "Algebra, matrices, probability, statistics are all maths subjects so you would be more likely to succeed at those than the science subjects.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.659365177154541,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.9874999999999999,
            "strategy_details": {
                "raw": 1.9749999999999999,
                "normalized": 1.9749999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 616,
        "original_data": {
            "instruction": "What is a Keystone?",
            "input": "",
            "output": "A keystone is the top block of an arch. It is an essential part of an arch because it keeps other stones from falling.  A keystone is shaped like a trapezoid, the widest part at the top, so it doesn't slide out of the arch.  Humans have built remarkable buildings with heavy ceilings on arches supported by a keystone.  Many architects and artisans have been known to sculpt family emblems, symbols and designs on keystones.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.2074787616729736,
            "normalized": 0.7735653604779925
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 617,
        "original_data": {
            "instruction": "What was Tyson Fury's fight record as an amateur?",
            "input": "As an amateur, Fury represented both England and Ireland. Fury represented Ireland three times at international level. He was based out of the Holy Family Boxing Club in Belfast, Northern Ireland, and later switched to the Smithboro Club in County Monaghan, Ireland. In a double international match against an experienced Polish team in 2007, the Irish team lost 12–6 overall; Fury, however, was victorious in both his fights in Rzeszów and Białystok. In another Irish match against the US, Fury won his bout by knockout. He won bronze at the AIBA Youth World Boxing Championships in 2006.\n\nIn England, while representing Jimmy Egan's Boxing Academy in Wythenshawe, Manchester, he participated in the senior national championships in 2006 but was beaten by David Price 22–8. In May 2007, he won the EU Junior Championship, defeating Istvan Bernath in the final. In July 2007 he won silver at the European Junior Championship, losing to Maxim Babanin in the final.\n\nAs a junior, Fury was ranked number three in the world behind the Russians Maxim Babanin and Andrey Volkov, but did not get the chance to represent Great Britain at the 2008 Olympics because each country is restricted to one boxer per weight division and David Price was selected. Price came up through the amateur Olympic programme. Fury also unsuccessfully tried to qualify for Ireland. Speaking in 2011, Fury said \"I should have gone to the Olympic games in 2008 and won a gold medal for Ireland, but I was denied the chance to go and do it\" He was also forced to withdraw from the Irish national championships after officials from the Holy Trinity Boxing Club in West Belfast, the club of the then Irish amateur heavyweight champion, submitted a protest regarding his eligibility as he was not born in Ireland.\n\nFury won the ABA super-heavyweight title in 2008 by defeating Damien Campbell 19:1. He turned professional later that year. Feeling disillusioned with amateur boxing, he decided not to wait for the 2012 Olympics. He finished with an amateur record of 31–4 (26 KOs).",
            "output": "He finished with an amateur record of 31–4 (26 KOs",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.95,
            "s_out_div": 0.3,
            "s_out_dens": 0.9,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.7,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.708448886871338,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4125,
            "strategy_details": {
                "raw": 0.825,
                "normalized": 0.825,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.15000000000000002,
                    "output": 0.49499999999999994
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.7,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 618,
        "original_data": {
            "instruction": "Can you give me a summary of the story of the Vidhivilasa movie?",
            "input": "Vidhivilasa is a 1962 Kannada language swashbuckler film directed by S. V. Mahesh. The film stars Rajkumar, Leelavathi and K. S. Ashwath. The film is based on the story of a King who confronts destiny (fate), which appears to him in the form of a young lady, by challenging it on how he would die. The King tries in vain all means at his disposal to prevent events that destiny foretold would happen. Finally, destiny wins, employing a twisted turn of events. The film has musical score by T. Padman. The story, screenplay, dialogues and lyrics were written by H. L. Narayana Rao who happens to be the father of actor Vishnuvardhan.",
            "output": "A king is confronted by destiny in person of a young lady and tries by any means possible to avoid his fate.  However the king is not able to prevent his destiny.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.036561965942383,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.49249999999999994,
            "strategy_details": {
                "raw": 0.9849999999999999,
                "normalized": 0.9849999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 619,
        "original_data": {
            "instruction": "Given this text about Heracles' twelve labors, construct a list of characters that either help him in his quest, how, and why.",
            "input": "1. Slay the Nemean Lion\nHeracles defeated a lion that was attacking the city of Nemea with his bare hands. After he succeeded he wore the skin as a cloak to demonstrate his power over the opponent he had defeated.\n2. Slay the nine-headed Lernaean Hydra\nA fire-breathing monster with multiple serpent heads. When one head was cut off, two would grow in its place. It lived in a swamp near Lerna. Hera had sent it in hope it would destroy Heracles' home city because she thought it was invincible. With help from his nephew Iolaus, he defeated the monster and dipped his arrows in its poisoned blood, thus envenomizing them.\n3. Capture the Golden Hind of Artemis\nNot to kill, but to catch, this hind that was sacred to Artemis. A different, but still difficult, task for a hero. It cost time but, having chased it for a year, Heracles wore out the Hind. Artemis intervened, but as soon as Heracles explained the situation to her, she allowed him to take it, and he presented it alive to Eurystheus.\n4. Capture the Erymanthian Boar\nA fearsome marauding boar on the loose. Eurystheus set Heracles the Labour of catching it, and bringing it to Mycenae. Again, a time-consuming task, but the tireless hero found the beast, captured it, and brought it to its final spot. Patience is the heroic quality in the third and fourth Labours.\n5. Clean the Augean stables in a single day\nThe Augean stables were the home of 3,000 cattle with poisoned faeces which Augeas had been given by his father Helios. Heracles was given the near impossible task of cleaning the stables of the diseased faeces. He accomplished it by digging ditches on both sides of the stables, moving them into the ditches, and then diverting the rivers Alpheios and Pineios to wash the ditches clean.\n6. Slay the Stymphalian Birds\nThese aggressive man-eating birds were terrorizing a forest near Lake Stymphalia in northern Arcadia. Heracles scared them with a rattle given to him by Athena, to frighten them into flight away from the forest, allowing him to shoot many of them with his bow and arrow and bring back this proof of his success to Eurystheus.\n7. Capture the Cretan Bull\nThe harmful bull, father of the Minotaur, was laying waste to the lands round Knossos on Crete. It embodied the rage of Poseidon at having his gift (the Bull) to Minos diverted from the intention to sacrifice it to himself. Heracles captured it, and carried it on his shoulders to Eurystheus in Tiryns. Eurystheus released it, when it wandered to Marathon which it then terrorized, until killed by Theseus.\n8. Steal the Mares of Diomedes\nStealing the horses from Diomedes' stables that had been trained by their owner to feed on human flesh was his next challenge. Heracles' task was to capture them and hand them over to Eurystheus. He accomplished this task by feeding King Diomedes to the animals before binding their mouths shut.\n9. Obtain the girdle of Hippolyta, Queen of the Amazons\nHippolyta was an Amazon queen and she had a girdle given to her by her father Ares. Heracles had to retrieve the girdle and return it to Eurystheus. He and his band of companions received a rough welcome because, ordered by Hera, the Amazons were supposed to attack them; however, against all odds, Heracles completed the task and secured the girdle for Eurystheus.\n10. Obtain the cattle of the monster Geryon\nThe next challenge was to capture the herd guarded by a two-headed dog called Orthrus, which belonged to Geryon; a giant with three heads and six arms who lived in Erytheia. While travelling to Erytheia, he passed through the Libyan desert and was so annoyed by the heat he shot an arrow at Helios, the sun. Helios, impressed, lent him his giant cup which Heracles used to find Orthrus, the herdsman Erytion and the owner, Geryon. He killed the first two with his club and the third with a poisoned arrow. Heracles then herded the cattle and, with difficulty, took them to Eurystheus.\n11. Steal the golden apples of the Hesperides\nHercules stealing the golden apples from the Garden of the Hesperides\nThese sacred fruits were protected by Hera who had set Ladon, a fearsome hundred-headed dragon as the guardian. Heracles had to first find where the garden was; he asked Nereus for help. He came across Prometheus on his journey. Heracles shot the eagle eating at his liver, and in return he helped Heracles with knowledge that his brother would know where the garden was. His brother Atlas offered him help with the apples if he would hold up the heavens while he was gone. Atlas tricked him and did not return. Heracles returned the trickery and managed to get Atlas taking the burden of the heavens once again, and returned the apples to Mycenae.\n12. Capture and bring back Cerberus\nHis last labour and undoubtedly the riskiest. Eurystheus was so frustrated that Heracles was completing all the tasks that he had given him that he imposed one he believed to be impossible: Heracles had to go down into the underworld of Hades and capture the ferocious three-headed dog Cerberus who guarded the gates. He used the souls to help convince Hades to hand over the dog. He agreed to give him the dog if he used no weapons to obtain him. Heracles succeeded and took the creature back to Mycenae, causing Eurystheus to be fearful of the power and strength of this hero.",
            "output": "- Iolaus, who helps him kill the Lernaean Hydra, since he is his nephew\n- Artemis, who allows him to take the Golden Hind, after Heracles explains his situation\n- Prometheus, who tells him that his brother Atlas knows the location of the garden of the Hesperides, in return for being saved by Heracles",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.7414209842681885,
            "normalized": 0.9261202812194824
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 620,
        "original_data": {
            "instruction": "What are the key features of a Galleon",
            "input": "Galleons were large, multi-decked sailing ships first used as armed cargo carriers by European states from the 16th to 18th centuries during the age of sail and were the principal vessels drafted for use as warships until the Anglo-Dutch Wars of the mid-1600s. Galleons generally carried three or more masts with a lateen fore-and-aft rig on the rear masts, were carvel built with a prominent squared off raised stern, and used square-rigged sail plans on their fore-mast and main-masts.",
            "output": "Galleons can be identified by the large size, multiple decks and tall masts with big sails. They typically has big raised sterns as well.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.922923564910889,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.435,
            "strategy_details": {
                "raw": 0.87,
                "normalized": 0.87,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.29999999999999993,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 621,
        "original_data": {
            "instruction": "Do you know anything about the writer Joshua Braff",
            "input": "Joshua Braff (born October 11, 1967) is an American writer.\n\nBraff's first novel, The Unthinkable Thoughts of Jacob Green, described as a Jewish coming of age tale, was published in 2004 by Algonquin Books of Chapel Hill. The Unthinkable Thoughts of Jacob Green was an ALA Notable Book, named to Booklist's Top 10 1st Novels List, and chosen for the Barnes & Noble Discover Great New Writers program. His second novel Peep Show was published by Algonquin in 2010. In a four-star review of Peep Show, People said, “Braff skillfully illuminates the failures and charms of a broken family. That teen longing for adults to act their age haunts long after the final page.”",
            "output": "Joshua Braff (born October 11, 1967) is an American writer. Braff's first novel, The Unthinkable Thoughts of Jacob Green, described as a Jewish coming of age tale, was published in 2004 by Algonquin Books of Chapel Hill.\n\nThe Unthinkable Thoughts of Jacob Green was an ALA Notable Book, named to Booklist's Top 10 1st Novels List, and chosen for the Barnes & Noble Discover Great New Writers program. His second novel Peep Show was published by Algonquin in 2010. \n\nIn a four-star review of Peep Show, People said, “Braff skillfully illuminates the failures and charms of a broken family. That teen longing for adults to act their age haunts long after the final page.”",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 3.1832878589630127,
            "normalized": 0.7666536739894322
        },
        "potential_entropy": {
            "strategy_component": 0.4174999999999999,
            "strategy_details": {
                "raw": 0.8349999999999999,
                "normalized": 0.8349999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.19499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 622,
        "original_data": {
            "instruction": "What was the SS Robert Coryndon?",
            "input": "SS Robert Coryndon was a British twin-screw passenger and cargo ferry on Lake Albert in central Africa.\n\nShe was built in England in 1929 and reassembled in kit form at Lake Albert in 1930. She ran aground in 1962 and lay derelict until she was broken up for scrap between 2009 and 2012.\n\nJohn I. Thornycroft & Company at Woolston, Hampshire built the ship for Kenya and Uganda Railways and Harbours in 1929–30. She was named after the South African Sir Robert Coryndon, who was Governor of Uganda 1918–22.\n\nShe was a \"knock-down\" ship. She was assembled in 1929 at Woolston. All her parts were marked with numbers, she was disassembled into many hundreds of parts, and transported in kit form to Africa, where she was reassembled on the shore of Lake Albert in 1930. Part of the overland journey to Lake Albert was by lorry, which severely limited the maximum size and weight of her parts.\n\nRobert Coryndon was part of a plan for a network of railway, river steamer and lake steamer services linking British interests in Egypt, East Africa and southern Africa. Sir Winston Churchill described her as \"the best library afloat\".\n\nFate\nRobert Coryndon sank in 1962, around the time of Ugandan independence from Britain. She was not refloated.\n\nIn 1967 the East African Railways and Harbours Corporation (EAR&H) offered her wreck for sale, but she remained largely intact in 2009.\n\nBy the beginning of 2012 her wreck had been taken away \"in bits and pieces by cutting all the metal remains for scrap\" and only her aft king posts were still visible above the water.",
            "output": "The SS Coryndon was a passenger and cargo ferry on Lake Albert in central Africa. It was built in 1929-30 and named after Sir Robert Coryndon, who was Governor of Uganda from 1918-22. It sank in 1962 and has been nearly entirely salvaged for scrap as of 2012.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9002175331115723,
            "normalized": 0.6857764380318778
        },
        "potential_entropy": {
            "strategy_component": 0.49249999999999994,
            "strategy_details": {
                "raw": 0.9849999999999999,
                "normalized": 0.9849999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 623,
        "original_data": {
            "instruction": "What is Perseverance doing on Mars?",
            "input": "",
            "output": "Perseverance is studying Mars' habitability, seeking signs of past microbial life, collecting and storing samples of selected rock and soil, and preparing for future human missions.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.3164310455322266,
            "normalized": 0.804694584437779
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 624,
        "original_data": {
            "instruction": "Provide a bulleted list of the different shapes of lenses",
            "input": "Construction of Simple Lenses\n\nMost lenses are spherical lenses: their two surfaces are parts of the surfaces of spheres. Each surface can be convex (bulging outwards from the lens), concave (depressed into the lens), or planar (flat). The line joining the centres of the spheres making up the lens surfaces is called the axis of the lens. Typically the lens axis passes through the physical centre of the lens, because of the way they are manufactured. Lenses may be cut or ground after manufacturing to give them a different shape or size. The lens axis may then not pass through the physical centre of the lens.\n\nToric or sphero-cylindrical lenses have surfaces with two different radii of curvature in two orthogonal planes. They have a different focal power in different meridians. This forms an astigmatic lens. An example is eyeglass lenses that are used to correct astigmatism in someone's eye.",
            "output": "The following are the different shapes of lenses:\n- Convex: Curves out from the middle of the lens\n- Concave: Curves into the middle of the lens\n- Planar: Flat lens",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.4825470447540283,
            "normalized": 0.5664420127868652
        },
        "potential_entropy": {
            "strategy_component": 0.48500000000000004,
            "strategy_details": {
                "raw": 0.9700000000000001,
                "normalized": 0.9700000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.23999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 625,
        "original_data": {
            "instruction": "What are some physical activities I could do on a sunny day?",
            "input": "",
            "output": "On a sunny day, you can take advantage of the nice weather by doing outdoor activities. For solo activities, you can go on a run, a hike or a bike ride. For group activities, consider playing frisbee or playing tennis.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.752761125564575,
            "normalized": 0.6436460358755929
        },
        "potential_entropy": {
            "strategy_component": 0.7175,
            "strategy_details": {
                "raw": 1.435,
                "normalized": 1.435,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.0,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 626,
        "original_data": {
            "instruction": "What is a snook?",
            "input": "",
            "output": "A snook is an inshore fish species.  They cannot be commercially fished.  If you want to eat a snook, you will need to catch it yourself.  Snook catch limits are highly regulated by Florida fish and wildlife.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.6351940631866455,
            "normalized": 0.8957697323390416
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 627,
        "original_data": {
            "instruction": "Is the language spoken in Kerala a palindrome?",
            "input": "Malayalam (/ˌmæləˈjɑːləm/; Malayalam: മലയാളം, Malayāḷam, [mɐlɐjaːɭɐm] (listen)) is a Dravidian language spoken in the Indian state of Kerala and the union territories of Lakshadweep and Puducherry (Mahé district) by the Malayali people. It is one of 22 scheduled languages of India. Malayalam was designated a \"Classical Language of India\" in 2013. Malayalam has official language status in Kerala and Puducherry (Mahé), and is also the primary spoken language of Lakshadweep and is spoken by 34 million people in India. Malayalam is also spoken by linguistic minorities in the neighbouring states; with a significant number of speakers in the Kodagu and Dakshina Kannada districts of Karnataka, and Kanyakumari, district of Tamil Nadu. It is also spoken by the Malayali Diaspora worldwide, especially in the Persian Gulf countries, due to the large populations of Malayali expatriates there. They are a significant population in each city in India including Mumbai, Bengaluru, Delhi, Kolkata, Pune etc.",
            "output": "Yes, malayalam is the language spoken in Kerala and it is a palindrome",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.981693983078003,
            "normalized": 0.994769709450858
        },
        "potential_entropy": {
            "strategy_component": 0.365,
            "strategy_details": {
                "raw": 0.73,
                "normalized": 0.73,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 628,
        "original_data": {
            "instruction": "Where was the 2018 UCI Track Cycling World Championships held?",
            "input": "The 2018 UCI Track Cycling World Championships were the World Championships for track cycling in 2018. They took place in the Netherlands at the Omnisport Apeldoorn from 28 February to 4 March 2018.",
            "output": "The UCI Track Cycling World Championships were held in Omnisport Apeldoorn velodrome, in the Netherlands.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.576186418533325,
            "normalized": 0.8789104052952358
        },
        "potential_entropy": {
            "strategy_component": 0.375,
            "strategy_details": {
                "raw": 0.75,
                "normalized": 0.75,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 629,
        "original_data": {
            "instruction": "Given this reference text, how old was Trey Anastasio when he co-founded Phish?",
            "input": "Ernest Joseph \"Trey\" Anastasio III (born September 30, 1964) is an American guitarist, singer, and songwriter, best known as the lead guitarist of the rock band Phish, which he co-founded in 1983. He is credited by name as composer of 152 Phish original songs, 141 of them as a solo credit, in addition to 41 credits attributed to the band as a whole. In addition to his work with Phish, Anastasio has released 11 solo albums, and been part of several side projects including the Trey Anastasio Band, Oysterhead, Ghosts of the Forest, Phil Lesh and Friends, and Surrender to the Air.",
            "output": "Based on this passage, Trey was most likely 19 years old. He was born at the end of September in 1964 so there is a 25% chance that he was 20 years old when he co-founded Phish.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.014805793762207,
            "normalized": 0.7185159410749163
        },
        "potential_entropy": {
            "strategy_component": 0.3975000000000001,
            "strategy_details": {
                "raw": 0.7950000000000002,
                "normalized": 0.7950000000000002,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 630,
        "original_data": {
            "instruction": "What are some sports that are played with a round ball?",
            "input": "",
            "output": "Cricket, Baseball, Basketball, Volleyball, Tennis, Pickleball, Table Tennis, Soccer, Racquetball, Hockey, Squash, handball",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.6,
            "s_out_div": 0.7,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.4,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.504317283630371,
            "normalized": 0.5726620810372489
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.4,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 631,
        "original_data": {
            "instruction": "When was Suntory established?",
            "input": "Suntory Holdings Limited (サントリーホールディングス株式会社, Santorī Hōrudingusu Kabushiki-gaisha) (commonly referred to as simply Suntory) is a Japanese multinational brewing and distilling company group. Established in 1899, it is one of the oldest companies in the distribution of alcoholic beverages in Japan, and makes Japanese whisky. Its business has expanded to other fields, and the company now also makes soft drinks and operates sandwich chains. With its 2014 acquisition of Beam, Inc., it has diversified internationally and become the third largest maker of distilled beverages in the world. Suntory is headquartered in Dojimahama 2-chome, Kita-ku, Osaka, Osaka Prefecture. Suntory Beverage & Food Limited (サントリー食品インターナショナル株式会社, Santorī shokuhin",
            "output": "Suntory was established in 1899",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.3225297927856445,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 632,
        "original_data": {
            "instruction": "Given this reference text about Neonatology, tell me which specialty it is related to, and where it is typically practiced.",
            "input": "Neonatology is a subspecialty of pediatrics that consists of the medical care of newborn infants, especially the ill or premature newborn. It is a hospital-based specialty, and is usually practiced in neonatal intensive care units (NICUs). The principal patients of neonatologists are newborn infants who are ill or require special medical care due to prematurity, low birth weight, intrauterine growth restriction, congenital malformations (birth defects), sepsis, pulmonary hypoplasia or birth asphyxia.",
            "output": "Neonatology is a subspecialty of pediatrics, which is typically practiced in a hospital's Neonatal Intensive Care Unit, often referred to as a NICU.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.018447160720825,
            "normalized": 0.4338420459202358
        },
        "potential_entropy": {
            "strategy_component": 0.23000000000000004,
            "strategy_details": {
                "raw": 0.4600000000000001,
                "normalized": 0.4600000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 633,
        "original_data": {
            "instruction": "Can animals like dog and cat get Kidney Stones ?",
            "input": "Bladder stones or uroliths are a common occurrence in animals, especially in domestic animals such as dogs and cats. Occurrence in other species, including tortoises, has been reported as well. The stones form in the urinary bladder in varying size and numbers secondary to infection, dietary influences, and genetics. Stones can form in any part of the urinary tract in dogs and cats, but unlike in humans, stones of the kidney are less common and do not often cause significant disease, although they can contribute to pyelonephritis and chronic kidney disease. Types of stones include struvite, calcium oxalate, urate, cystine, calcium phosphate, and silicate. Struvite and calcium oxalate stones are by far the most common. Bladder stones are not the same as bladder crystals but if the crystals coalesce unchecked in the bladder they can become stones.",
            "output": "Yes animals like humans can get Kidney Stones. Bladder stones or uroliths are a common occurrence in animals, especially in domestic animals such as dogs and cats. Types of stones include struvite,  cystine, calcium oxalate, urate, calcium phosphate, and silicate. Struvite and calcium oxalate stones are by far the most common.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.7,
            "s_out_div": 0.65,
            "s_out_dens": 0.6,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.35,
            "s_out_dens": 0.4,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.5588066577911377,
            "normalized": 0.5882304736546108
        },
        "potential_entropy": {
            "strategy_component": 0.5449999999999999,
            "strategy_details": {
                "raw": 1.0899999999999999,
                "normalized": 1.0899999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.35,
                    "out_dens": 0.4,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 634,
        "original_data": {
            "instruction": "What started World War 1?",
            "input": "",
            "output": "Many historians consider the assassination of Archduke Franz Ferdinand of Austria the start of World War 1. Soon after the assassination Austria declared war on Serbia. Within four weeks their allies declared war on one another.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.3243136405944824,
            "normalized": 0.8069467544555664
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 635,
        "original_data": {
            "instruction": "How many cycling world championships has Peter Sagan won?",
            "input": "Sagan is considered one of cycling's greatest talents, having earned many prestigious victories, including three consecutive World Championships, one European Championship, two Paris–Nice stages, seven Tirreno–Adriatico stages, one in the Tour de Romandie, three and the overall classification in the Tour de Pologne, a record seventeen stages and the overall in the Tour of California, and another fifteen in the Tour de Suisse.",
            "output": "Sagan has won three consecutive cycling world championships",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.85,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 6.986029624938965,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5675,
            "strategy_details": {
                "raw": 1.135,
                "normalized": 1.135,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 636,
        "original_data": {
            "instruction": "Extract the names of the companies that built the Pacific Railroad",
            "input": "A transcontinental railroad in the United States is any continuous rail line connecting a location on the U.S. Pacific coast with one or more of the railroads of the nation's eastern trunk line rail systems operating between the Missouri or Mississippi Rivers and the U.S. Atlantic coast. The first concrete plan for a transcontinental railroad in the United States was presented to Congress by Asa Whitney in 1845.\n\nA series of transcontinental railroads built over the last third of the 19th century created a nationwide transportation network that united the country by rail. The first of these, the 3,103 km (1,928 mi) \"Pacific Railroad\", was built by the Central Pacific Railroad and Union Pacific Railroad to link the San Francisco Bay at Alameda, California, with the nation's existing eastern railroad network at Council Bluffs, Iowa/Omaha, Nebraska, thereby creating the world's first transcontinental railroad when it opened in 1869. Its construction was made possible by the US government under Pacific Railroad Acts of 1862, 1864, and 1867. Its original course was very close to current Interstate 80.",
            "output": "The \"Pacific Railroad\" was built by the Central Pacific Railroad and Union Pacific Railroad.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2152044773101807,
            "normalized": 0.7757727078029087
        },
        "potential_entropy": {
            "strategy_component": 0.3625,
            "strategy_details": {
                "raw": 0.725,
                "normalized": 0.725,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 637,
        "original_data": {
            "instruction": "From the passage provided, extract the what were some of the key years and discoveries of portuguese maritime exploration?",
            "input": "Portuguese maritime exploration resulted in the numerous territories and maritime routes recorded by the Portuguese as a result of their intensive maritime journeys during the 15th and 16th centuries. Portuguese sailors were at the vanguard of European exploration, chronicling and mapping the coasts of Africa and Asia, then known as the East Indies, and Canada and Brazil (the West Indies), in what came to be known as the Age of Discovery.\n\nMethodical expeditions started in 1419 along West Africa's coast under the sponsorship of prince Henry the Navigator, with Bartolomeu Dias reaching the Cape of Good Hope and entering the Indian Ocean in 1488. Ten years later, in 1498, Vasco da Gama led the first fleet around Africa to India, arriving in Calicut and starting a maritime route from Portugal to India. Portuguese explorations then proceeded to southeast Asia, where they reached Japan in 1542, forty-four years after their first arrival in India. In 1500, the Portuguese nobleman Pedro Álvares Cabral became the first European to discover Brazil.",
            "output": "In 1419, the portuguese started their maritime explorations along the West Africa's coast. The goal was to reach India by sea, by going around Africa. In 1488, Bartolomeu Dias reaching the Cape of Good Hope and entered the Indian Ocean. In 1498, Vasco da Gama finally reached India.\nBy 1500 also reached discovered Brazil and reached Japan in 1542.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.449613571166992,
            "normalized": 0.5570324489048549
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 638,
        "original_data": {
            "instruction": "Based on the passage below, summarize Beethoven's contributions to western music.",
            "input": "Beethoven expanded the formal and emotional scope – not to mention length – of nearly every genre in which he wrote. While he is most famous for his heightening of the symphonic form, Beethoven also had a dramatic influence on the piano sonata, violin sonata, string quartet and piano concerto, among several others. Only in the realm of vocal composition – opera and the mass – was his effect on later generations muted.\n\nBeethoven's stylistic innovations bridge the Classical and Romantic periods. The works of his early period brought the Classical form to its highest expressive level, expanding in formal, structural, and harmonic terms the musical idiom developed by predecessors such as Mozart and Haydn. The works of his middle period were more forward-looking, contributing to the musical language and thinking of the Romantic era, inspiring composers such as Franz Schubert, Felix Mendelssohn, Robert Schumann, Franz Liszt, Richard Wagner, and Johannes Brahms. His late period works were characterized by formal, harmonic, and structural experimentation at the highest level, often pointing toward contrapuntal tendencies and microscopic textures, as well as an increasingly introverted compositional outlook. Though rightly credited as a major harbinger of the Romantic era in music that followed, Beethoven never abandoned the fundamental formal paradigms and generally objective artistic philosophy characterizing musical Classicism to the same extent that later composers such as Berlioz or even Schubert did.",
            "output": "- Heightened the symphonic form and had a dramatic influence on the piano sonata, violin sonata, string quartet, piano concertos and other genres\n- Contributed to the musical language and thinking of the romantic era\n- Formal, harmonic and structural experimentation in his later works",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.3855154514312744,
            "normalized": 0.8244329861232212
        },
        "potential_entropy": {
            "strategy_component": 0.27,
            "strategy_details": {
                "raw": 0.54,
                "normalized": 0.54,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.15000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 639,
        "original_data": {
            "instruction": "What should I do if my laptop has been hacked?",
            "input": "",
            "output": "If your laptop has been hacked, you can be in a big trouble. Attackers may have stolen all your confidential data and passwords. It is better to ask for helps from security experts. It is also important to reset all your passwords. Do not turn on your laptop and if it is a personal laptop make sure to re-install your operating system. Check your recent bank account activities to ensure that you have not lost any money.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.800300121307373,
            "normalized": 0.6572286060878209
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 640,
        "original_data": {
            "instruction": "What is Python?",
            "input": "",
            "output": "Python is a programming language that has a design emphasis on code readability and is strongly typed.  You can interact with this programming language through writing 'python code' using proper coding syntax and semantics.  The code that is written can be used to build tools, applications, websites, and streaming services.  You can also automate tasks with python, analyze data, and perform machine learning and data science.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.197080373764038,
            "normalized": 0.7705943925040108
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 641,
        "original_data": {
            "instruction": "What Breaking Bad actor guest starred on Season 6, Episode 2 \"Drive\" of The X-Files?",
            "input": "",
            "output": "Bryan Cranston guest starred as Patrick Crump.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 6.141315460205078,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3150000000000002,
            "strategy_details": {
                "raw": 2.6300000000000003,
                "normalized": 2.6300000000000003,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.85,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 642,
        "original_data": {
            "instruction": "What is the project management approach that have a serious of distinct steps?",
            "input": "There are five phases to a project lifecycle; known as process groups. Each process group represents a series of inter-related processes to manage the work through a series of distinct steps to be completed. This type of project approach is often referred to as \"traditional\" or \"waterfall\". The five process groups are:\n\nTypical development phases of an engineering project\nInitiating\nPlanning\nExecuting\nMonitoring and Controlling\nClosing\n\nSome industries may use variations of these project stages and rename them to better suit the organization. For example, when working on a brick-and-mortar design and construction, projects will typically progress through stages like pre-planning, conceptual design, schematic design, design development, construction drawings (or contract documents), and construction administration.\n\nWhile the phased approach works well for small, well-defined projects, it often results in challenge or failure on larger projects, or those that are more complex or have more ambiguities, issues, and risks",
            "output": "The waterfall, sometimes called traditional, approach is the project management approach that has a series of related processes.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.577008247375488,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.32000000000000006,
            "strategy_details": {
                "raw": 0.6400000000000001,
                "normalized": 0.6400000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 643,
        "original_data": {
            "instruction": "What's the game engine used by Bethesda before Creation Engine? And what's the last game make with it?",
            "input": "After using the Gamebryo engine to create The Elder Scrolls III: Morrowind, The Elder Scrolls IV: Oblivion, and Fallout 3, Bethesda decided that Gamebryo's capabilities were becoming too outdated and began work on the Creation Engine for their next game, The Elder Scrolls V: Skyrim, by forking the codebase used for Fallout 3.\n\nFollowing the completion of Skyrim, Bethesda set out to enhance the graphical core of the Creation Engine by first adding a physically based deferred renderer to allow for more dynamic lighting and to paint materials object surfaces with realistic materials. Bethesda worked with technology company Nvidia to implement volumetric lighting through a technique that makes use of hardware tesselation. Additionally the updated version of the Creation Engine powering Bethesda's Fallout 4 offers more advanced character generation.",
            "output": "Gamebryo was the engine used by Bethesda before Creation Engine. It is developed by Gamebase Cot., Ltd and Gamebase USA. Gamebryo is used by numerous companies within the gaming industry.\n\nBethesda decided that Gamebryo's capabilities were becoming too outdated and began work on the Creation Engine for their next game, The Elder Scrolls V: Skyrim, by forking the codebase used for Fallout 3.\n\nThe last game from Bethesda made with Gamebryo engine is Fallout 3 (2008).",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.8,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.6170077323913574,
            "normalized": 0.8905736378261021
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 644,
        "original_data": {
            "instruction": "Which singer’s real name is Stefani Joanne Angelina Germanotta?",
            "input": "",
            "output": "Lady Gaga",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            0,
            1,
            1
        ],
        "entropy": {
            "raw": 15.482068061828613,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.125,
            "strategy_details": {
                "raw": 2.25,
                "normalized": 2.25,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.8,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.9,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 645,
        "original_data": {
            "instruction": "How many times Lewis Hamilton won the F1 Championship ?",
            "input": "",
            "output": "7 times",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 11.667837142944336,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1275,
            "strategy_details": {
                "raw": 2.255,
                "normalized": 2.255,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.7000000000000002,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 646,
        "original_data": {
            "instruction": "What is a Gross domestic product (GDP)?",
            "input": "",
            "output": "Gross domestic product (GDP) is a monetary measure of the market value of all the final goods and services produced and sold in a specific time period by a country or countries, generally \"without double counting the intermediate goods and services used up to produce them\". GDP is most often used by the government of a single country to measure its economic health. Due to its complex and subjective nature, this measure is often revised before being considered a reliable indicator. GDP (nominal) per capita does not, however, reflect differences in the cost of living and the inflation rates of the countries; therefore, using a basis of GDP per capita at purchasing power parity (PPP) may be more useful when comparing living standards between nations, while nominal GDP is more useful comparing national economies on the international market. Total GDP can also be broken down into the contribution of each industry or sector of the economy. The ratio of GDP to the total population of the region is the per capita GDP (also called the Mean Standard of Living).",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.5852484703063965,
            "normalized": 0.595785277230399
        },
        "potential_entropy": {
            "strategy_component": 0.303,
            "strategy_details": {
                "raw": 0.606,
                "normalized": 0.606,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.29999999999999993,
                    "output": 0.126
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 647,
        "original_data": {
            "instruction": "When was Lure of the Swamp released?",
            "input": "Lure of the Swamp is a 1957 American drama film directed by Hubert Cornfield and written by William George, based on the 1953 novel of the same name by Gil Brewer. The film stars Marshall Thompson, Willard Parker, Joan Vohs, Jack Elam, Leo Gordon and Joan Lora. The film was released in May 1957 by 20th Century-Fox.",
            "output": "Lure of the Swamp was released by 20th Century-Fox in May 1957 and is an American drama film directed by Hubert Cornfield and written by William George. Lure of the Swamp is based on the 1953 novel of the same name by Gil Brewer.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.490446090698242,
            "normalized": 0.5686988830566406
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 648,
        "original_data": {
            "instruction": "Which are the best movie sequels in the past century?",
            "input": "",
            "output": "The distinction for the best movie sequels in the past 50 years goes to The Godfather Part 2, Terminator 2 Judgement Day, Empire Strikes Back and The DarK Night. These movie elevated the original source material and achieved cinematic excellence. All these movies were also more financially successful when compared to their predecessors.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.8441860675811768,
            "normalized": 0.955481733594622
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 649,
        "original_data": {
            "instruction": "Which epic poem discussed the Trojan Horse?",
            "input": "The Trojan Horse was a wooden horse said to have been used by the Greeks during the Trojan War to enter the city of Troy and win the war. The Trojan Horse is not mentioned in Homer's Iliad, with the poem ending before the war is concluded, and it is only briefly mentioned in the Odyssey. But in the Aeneid by Virgil, after a fruitless 10-year siege, the Greeks constructed a huge wooden horse at the behest of Odysseus, and hid a select force of men inside, including Odysseus himself. The Greeks pretended to sail away, and the Trojans pulled the horse into their city as a victory trophy. That night, the Greek force crept out of the horse and opened the gates for the rest of the Greek army, which had sailed back under the cover of darkness. The Greeks entered and destroyed the city, ending the war.",
            "output": "Odyssey",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.6,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.4,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 17.162582397460938,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.4,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 650,
        "original_data": {
            "instruction": "What are microservices?",
            "input": "",
            "output": "Microservices are an architectural and organizational approach to software development where software is composed of small independent services that communicate over well-defined APIs. These services are owned by small, self-contained teams.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.6868395805358887,
            "normalized": 0.6248113087245396
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 651,
        "original_data": {
            "instruction": "Describe the video game crash of 1983 in 2 sentences.",
            "input": "The video game crash of 1983 (known as the Atari shock in Japan) was a large-scale recession in the video game industry that occurred from 1983 to 1985, primarily in the United States. The crash was attributed to several factors, including market saturation in the number of video game consoles and available games, many of which were of poor quality, as well as waning interest in console games in favor of personal computers. Home video game revenues peaked at around $3.2 billion in 1983, then fell to around $100 million by 1985 (a drop of almost 97 percent). The crash abruptly ended what is retrospectively considered the second generation of console video gaming in North America. To a lesser extent, the arcade video game market also weakened as the golden age of arcade video games came to an end.\n\nLasting about two years, the crash shook a then-booming video game industry and led to the bankruptcy of several companies producing home computers and video game consoles. Analysts of the time expressed doubts about the long-term viability of video game consoles and software.\n\nThe North American video game console industry recovered a few years later, mostly due to the widespread success of Nintendo's Western branding for its Famicom console, the Nintendo Entertainment System (NES), released in 1985. The NES was designed to avoid the missteps that caused the 1983 crash and the stigma associated with video games at that time.",
            "output": "Home video game revenues in North America declined from $3.2 billion in 1983 to around $100 million in 1985. This decline – which preceded the success of the Nintendo Entertainment System – is referred to as the video game crash of 1983.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.264601469039917,
            "normalized": 0.5041718482971191
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 652,
        "original_data": {
            "instruction": "How do you make an iced matcha latter?",
            "input": "",
            "output": "You will need 1-2 teaspoons of matcha powder, milk of your choice, 1 cup of hot water, and ice. In a cup or bowl, you will add your match powder and pour your hot water into it and use a whisk until the matcha powder is well incorporated, which should create a paste like consistency. Then you will take a glass and pour in your ice and milk and your matcha paste on top and stir.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.7619807720184326,
            "normalized": 0.646280220576695
        },
        "potential_entropy": {
            "strategy_component": 1.1125,
            "strategy_details": {
                "raw": 2.225,
                "normalized": 2.225,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 653,
        "original_data": {
            "instruction": "What was the primary mode of transportation across the San Francisco bay prior to the Golden Gate Bridge?",
            "input": "Before the bridge was built, the only practical short route between San Francisco and what is now Marin County was by boat across a section of San Francisco Bay. A ferry service began as early as 1820, with a regularly scheduled service beginning in the 1840s for the purpose of transporting water to San Francisco.\n\nThe Sausalito Land and Ferry Company service, launched in 1867, eventually became the Golden Gate Ferry Company, a Southern Pacific Railroad subsidiary, the largest ferry operation in the world by the late 1920s. Once for railroad passengers and customers only, Southern Pacific's automobile ferries became very profitable and important to the regional economy. The ferry crossing between the Hyde Street Pier in San Francisco and Sausalito Ferry Terminal in Marin County took approximately 20 minutes and cost $1.00 per vehicle,[when?] a price later reduced to compete with the new bridge. The trip from the San Francisco Ferry Building took 27 minutes.",
            "output": "The only (practical) way to get across the San Francisco Bay prior to 1937 was by Ferry. The Golden Gate Ferry Company, which became the largest ferry operation by the late 1920's, was established in 1867 as the Sausalito Land & Ferry Company. The ferry would take approximately 20 minutes and cost $1.00 per vehicle.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.016179084777832,
            "normalized": 0.7189083099365234
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 654,
        "original_data": {
            "instruction": "How many letters there are in the China language?",
            "input": "",
            "output": "However, since there is no Chinese alphabet, there is no definite answer. But there are some statistics that will probably give you an idea. A modern Chinese dictionary has around 20,000 characters.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.8,
            "s_out_div": 0.5,
            "s_out_dens": 0.6,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.5,
            "s_out_dens": 0.4,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.8144731521606445,
            "normalized": 0.6612780434744698
        },
        "potential_entropy": {
            "strategy_component": 1.11,
            "strategy_details": {
                "raw": 2.22,
                "normalized": 2.22,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.5,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.5,
                    "out_dens": 0.4,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 655,
        "original_data": {
            "instruction": "In relation to working out, what is a bench press, and how do you perform it?",
            "input": "The bench press, or chest press, is a weight training exercise where the trainee presses a weight upwards while lying on a weight training bench. Although the bench press is a full-body exercise, the muscles primarily used are the pectoralis major, the anterior deltoids, and the triceps, among other stabilizing muscles. A barbell is generally used to hold the weight, but a pair of dumbbells can also be used",
            "output": "The bench press is a weight training exercise targeting the chest and triceps. The trainee begins by lying down on a flat bench with a straight barbell directly on top of them. The trainee lifts the bar to extend directly above the head, and then slowly descends the bar until it touches their chest. The bar is then pressed upward until the arms are once again fully extended.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.8,
            "s_out_div": 0.65,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.35,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.609495162963867,
            "normalized": 0.602712903703962
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.35,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 656,
        "original_data": {
            "instruction": "Which of the following are nuts: pecan, hazelnut, butternut squash, walnut, peanut, corn nut, macadamia nut.",
            "input": "",
            "output": "The following are nuts: pecan, hazelnut, walnut, peanut, macadamia nut. \n\nThe following are not nuts: butternut squash, corn nut.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.0023560523986816,
            "normalized": 0.7149588721139091
        },
        "potential_entropy": {
            "strategy_component": 0.5974999999999999,
            "strategy_details": {
                "raw": 1.1949999999999998,
                "normalized": 1.1949999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 657,
        "original_data": {
            "instruction": "Who is responsible for deportation of Ukrainian children to Russia?",
            "input": "On 17 March 2023, the International Criminal Court (ICC) issued arrest warrants for Vladimir Putin, the President of Russia, and Maria Lvova-Belova, Russian Commissioner for Children's Rights, alleging responsibility for the unlawful deportation and transfer of children during the Russian invasion of Ukraine. The warrant against Putin is the first against the leader of a permanent member of the United Nations Security Council.\n\nThe 123 member states of the ICC are obliged to detain and transfer Putin and Lvova-Belova if either sets foot on their territory.",
            "output": "President of Russian Federation Vladimir Putin and Maria Lvova-Belova, Russian Commissioner for Children's Rights are directly responsible the unlawful deportation of Ukrainian children to Russia. This crime against humanity along with many other atrocities (like Bucha massacre) has been committed by Russians during the Russian invasion of Ukraine that started on February 24, 2022.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.479341745376587,
            "normalized": 0.8512404986790248
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 658,
        "original_data": {
            "instruction": "Please give me a short summary of the misinformed public announcements leading to the fall of the Berlin Wall",
            "input": "The announcement of the regulations which brought down the Wall took place at an hour-long press conference led by Günter Schabowski, the party leader in East Berlin and the top government spokesman, beginning at 18:00 CET on 9 November and broadcast live on East German television and radio. Schabowski was joined by Minister of Foreign Trade Gerhard Beil and Central Committee members Helga Labs and Manfred Banaschak.: 352 \n\nSchabowski had not been involved in the discussions about the new regulations and had not been fully updated. Shortly before the press conference, he was handed a note from Krenz announcing the changes, but given no further instructions on how to handle the information. The text stipulated that East German citizens could apply for permission to travel abroad without having to meet the previous requirements for those trips, and also allowed for permanent emigration between all border crossings—including those between East and West Berlin.\n\nAt 18:53, near the end of the press conference, ANSA's Riccardo Ehrman asked if the draft travel law of 6 November was a mistake. Schabowski gave a confusing answer that asserted it was necessary because West Germany had exhausted its capacity to accept fleeing East Germans, then remembered the note he had been given and added that a new regulation had been drafted to allow permanent emigration at any border crossing. This caused a stir in the room; amid several questions at once, Schabowski expressed surprise that the reporters had not yet seen this regulation, and started reading from the note. After this, a reporter, either Ehrman or Bild-Zeitung reporter Peter Brinkmann, both of whom were sitting in the front row at the press conference, asked when the regulations would take effect. After a few seconds' hesitation, Schabowski replied, \"As far as I know, it takes effect immediately, without delay\" (German: Das tritt nach meiner Kenntnis ... ist das sofort ... unverzüglich).: 352  This was an apparent assumption based on the note's opening paragraph; as Beil attempted to interject that it was up to the Council of Ministers to decide when it took effect, Schabowski proceeded to read this clause, which stated it was in effect until a law on the matter was passed by the Volkskammer. Crucially, a journalist then asked if the regulation also applied to the crossings to West Berlin. Schabowski shrugged and read item 3 of the note, which confirmed that it did.\n\nAfter this exchange, Daniel Johnson of The Daily Telegraph asked what this law meant for the Berlin Wall. Schabowski sat frozen before giving a rambling statement about the Wall being tied to the larger disarmament question. He then ended the press conference promptly at 19:00 as journalists hurried from the room.\n\nAfter the press conference, Schabowski sat for an interview with NBC News anchor Tom Brokaw in which he repeated that East Germans would be able to emigrate through the border and the regulations would go into effect immediately.",
            "output": "The announcement of the new laws which brought down the Berlin Wall took place at 6 pm on 9 November 1989,  at a live press conference led by Günter Schabowski, the party leader in East Berlin. Schabowski had not been involved in the discussions about the new regulations and consequently was not adequately informed. Shortly before the press conference, he was handed a note outlining the changes to be announced, but given no further instructions on how to handle the content. The note specified that East German citizens could apply for permission to travel abroad, and permanent emigration between all border crossings—including those between East and West Berlin – would be allowed.\nAt 6:53 pm, near the end of the press conference, when asked if the draft travel law of 6 November was a mistake, Schabowski gave a confusing answer that it was necessary because West Germany had exhausted its capacity to accept fleeing East Germans, then remembered the note he had been handed and went on to add that a new regulation had been drafted to allow permanent emigration at any border crossing. This caused a stir in the room, several questions came flying at once. When asked when the regulations would take effect, Schabowski replied hesitantly, \"As far as I know, it takes effect immediately, without delay\". And after some confused interjections between party members, when asked what this law meant for the Berlin Wall, Schabowski froze for a few moments, before blurting out something to the effect of the Wall being tied to the larger disarmament question. He then ended the press conference promptly at 7 pm, as journalists hurried out of the room.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1623053550720215,
            "normalized": 0.7606586728777204
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 659,
        "original_data": {
            "instruction": "What is caramelization?",
            "input": "",
            "output": "Caramelization is a chemical process that happens to carbohydrates and sugars when they are heated to certain temperatures. At these temperatures new compounds are formed and browning happens. For example during caramelization, complex sugars degrade to simple sugars. Caramelization is an important process in cooking and shows ups in many common recipes",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.045645236968994,
            "normalized": 0.7273272105625698
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 660,
        "original_data": {
            "instruction": "Based on the paragraph below, what bone was used to estimate the height of William the Conquerer?",
            "input": "No authentic portrait of William has been found; the contemporary depictions of him on the Bayeux Tapestry and on his seals and coins are conventional representations designed to assert his authority. There are some written descriptions of a burly and robust appearance, with a guttural voice. He enjoyed excellent health until old age, although he became quite fat in later life. He was strong enough to draw bows that others were unable to pull and had great stamina. Geoffrey Martel described him as without equal as a fighter and as a horseman. Examination of William's femur, the only bone to survive when the rest of his remains were destroyed, showed he was approximately 5 feet 10 inches (1.78 m) in height.",
            "output": "His femur.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 6.998698711395264,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.36250000000000004,
            "strategy_details": {
                "raw": 0.7250000000000001,
                "normalized": 0.7250000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 661,
        "original_data": {
            "instruction": "Extract the ways to spell \"Muscovy duck\" using the sample as a reference, and place them in a list.",
            "input": "The Muscovy duck has been domesticated for centuries, and is widely traded as \"Barbary duck\". Muscovy breeds are popular because they have stronger-tasting meat — sometimes compared to roasted beef — than that of the domestic duck. The meat is lean when compared to the fatty meat of mallard-derived ducks, its leanness and tenderness being often compared to veal. Muscovy ducks are also less noisy, and sometimes marketed as a \"quackless\" duck; even though they are not completely silent, they do not actually quack (except in cases of extreme stress). The carcass of a Muscovy duck is also much heavier than most other domestic ducks, which makes it ideal for the dinner table.",
            "output": "- Muscovy duck\n- Barbary duck\n- Quackless duck",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.9116811752319336,
            "normalized": 0.9747660500662667
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 662,
        "original_data": {
            "instruction": "Identify the first run broadcast network (ABC, NBC, or CBS) of the following TV shows:\nCastle, Chicago Fire, CSI, ER, Grey's Anatomy, Scandal, Seinfeld, Magnum PI",
            "input": "",
            "output": "In the US, Castle, Grey's Anatomy, and Scandal were first broadcast on ABC\nChicago Fire, ER and Seinfeld were first broadcast on NBC. CSI and Magnum PI were first broadcast on CBS, though Magnum PI switched to NBC for the 5th season of the rebooted series.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.9677541255950928,
            "normalized": 0.9907868930271694
        },
        "potential_entropy": {
            "strategy_component": 0.335,
            "strategy_details": {
                "raw": 0.67,
                "normalized": 0.67,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.4,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 663,
        "original_data": {
            "instruction": "What countries have won the rugby world cup?",
            "input": "The most important competition in rugby union is the Rugby World Cup, a men's tournament that has taken place every four years since the inaugural event in 1987. South Africa are the reigning champions, having defeated England in the final of the 2019 Rugby World Cup in Yokohama. New Zealand and South Africa have each won the title three times (New Zealand: 1987, 2011, 2015; South Africa: 1995, 2007, 2019), Australia have won twice (1991 and 1999), and England once (2003). England is the only team from the Northern Hemisphere to have won the Rugby World Cup.",
            "output": "- New Zealand\n- Australia\n- South Africa\n- England",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.928936243057251,
            "normalized": 0.6939817837306431
        },
        "potential_entropy": {
            "strategy_component": 0.375,
            "strategy_details": {
                "raw": 0.75,
                "normalized": 0.75,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 664,
        "original_data": {
            "instruction": "Tell me three key things to do to drive a car faster on a track",
            "input": "",
            "output": "Driving a car on a track can be an exhilarating experience, but there a few rules to keep in mind.  Here are three key things to remember:\n1. Eyes up - Many drivers have a tendency to look directly at the end of the hood or at the next cone or apex.  Keep your eyes up and to be able to assess where you want to go.\n2. Look where you want to go - Your hands on the steering wheel follow your eyes, so look where you want to go, not what you want to avoid or navigate.  For example. you should be looking at the exit of a turn as you enter the turn (or the apex if it's a wide turn)\n3. Slow in, fast out - Cornering can be particularly tricky, because we tend to turn in too fast and too early, which can set up for a slow exit.  Brake hard, take it slow, and wait to turn into a corner to best position you and the car to accelerate swiftly out of the turn.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.9298720359802246,
            "normalized": 0.694249153137207
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 665,
        "original_data": {
            "instruction": "What foods are high in antioxidants?",
            "input": "",
            "output": "Vegetables are foods that are high in antioxidants.  For example, kale, cabbage, broccoli and onions are foods that are high in antioxidants.  When you consume foods that are high in antioxidants they helps to remove acidity out of the body created by physical stress and daily activity.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1081509590148926,
            "normalized": 0.7451859882899693
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 666,
        "original_data": {
            "instruction": "Which one of the following objects is least like the others, and why?\nLemon, Car, Grass, Lime",
            "input": "",
            "output": "A car is least like the others, because it is a man-made non-living object, while the rest are plants.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.75,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.25,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.070401668548584,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0474999999999999,
            "strategy_details": {
                "raw": 2.0949999999999998,
                "normalized": 2.0949999999999998,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.6149999999999999
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.25,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 667,
        "original_data": {
            "instruction": "Who wrote Vande Mataram poem?",
            "input": "",
            "output": "Vande Mataram is a poem written in sanskritised Bengali by Bankim Chandra Chatterjee. The poem was first published in 1882 as part of Chatterjee's Bengali novel Anandmath.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.3,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.7,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.9386980533599854,
            "normalized": 0.6967708723885673
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.7,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 668,
        "original_data": {
            "instruction": "Who are co-founders of Google?",
            "input": "",
            "output": "Larry Page and ‎Sergey Brin",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.275290489196777,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 669,
        "original_data": {
            "instruction": "Does Amazon use doors for desks?",
            "input": "",
            "output": "Jeff Bezos initially used doors purchased from the local hardware store and added legs, and door desk was born.  He did that because funds were limited, and this was the least expensive way for him to purchase a desk.  Today, Amazon still uses wooden doors with four pieces of wood attached for legs.  New employees receive directions to calculate the optimum desk height, then fill out a form, and maintenance cuts the wooden legs to achieve the correct size.  The company does provide adjustable desks by request only.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.262811183929443,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 670,
        "original_data": {
            "instruction": "Tell me in which country are the following cities: Zaragoza, Albufeira, Bari, Amfissa and Targoviste.",
            "input": "",
            "output": "Zaragoza is a city in Spain, Albufeira is in Portugal, Bari is located in Italy, Amfissa is part of Greece and Targoviste is in Romania.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.24003529548645,
            "normalized": 0.7828672272818429
        },
        "potential_entropy": {
            "strategy_component": 0.7775,
            "strategy_details": {
                "raw": 1.555,
                "normalized": 1.555,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 671,
        "original_data": {
            "instruction": "In which movie or plot the character vedimuthu was established",
            "input": "London is a 2005 Indian Tamil-language comedy-drama film written by Badri and directed by Sundar C.  Bhaskaran hides the money before getting arrested, and the place is known only to his lawyer Vedimuthu. Aishwarya pretends to be in love with Vedimuthu to find where the money is hidden.",
            "output": "Vedimuthu character was potrayed by Vadivelu and become famous for his comic potrayal of act of love with mistress Aishwarya (potrayed by mumtaz) and getting caught in the act to his wife Baby(potrayed by Nalini).",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.435239315032959,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 672,
        "original_data": {
            "instruction": "Given this paragraph, what key crops were not available in Europe before the colonization of the New World?",
            "input": "Food historian Lois Ellen Frank calls potatoes, tomatoes, corn, beans, squash, chili, cacao, and vanilla the \"magic eight\" ingredients that were found and used only in the Americas before 1492 and were taken via the Columbian Exchange back to the Old World, dramatically transforming the cuisine there. According to Frank,\nIf we deconstruct that these foods were inherently native, then that means that the Italians didn't have the tomato, the Irish didn't have the potato, half the British National Dish—Fish and Chips—didn't exist. The Russians didn't have the potato, nor did they have vodka from the potato. There were no chiles in any Asian cuisine anywhere in the world, nor were there any chiles in any East Indian cuisine dishes, including curries. And the French had no confection using either vanilla or chocolate. So the Old World was a completely different place.",
            "output": "Potatoes, tomatoes, corn, beans, squash, chili, cacao, and vanilla were not available in the Old World prior to the European discovery of the Americas",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.217938184738159,
            "normalized": 0.7765537670680455
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 673,
        "original_data": {
            "instruction": "Give me a list of things to do in Washington, DC",
            "input": "",
            "output": "Go see the White House, Washington Monument, Lincoln Memorial, and some of the other great landmarks. If that doesn't interest you, you can head anywhere on U-street for some great food.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.6849067211151123,
            "normalized": 0.9099733488900321
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 674,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Dollu, Tidinet",
            "input": "",
            "output": "Tidinet is string, Dollu is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.981286525726318,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 675,
        "original_data": {
            "instruction": "Pulitzer Prize 2022 was given to  4 Indian photographers from Reuters news agency. Name them",
            "input": "",
            "output": "Danish Siddiqui(slain), Adnan Abidi, SannaIrshad Matto & Amit Dave",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.2208757400512695,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 676,
        "original_data": {
            "instruction": "Explain the word taboo",
            "input": "Word taboo, also called taboo language, language taboo or linguistic taboo is a kind of taboo that involves restricting the use of words or other parts of language due to social constraints. This may be due to a taboo on specific parts of the language itself (such as certain words, or sounds), or due to the need to avoid a taboo topic. The taboo against naming the dead in parts of the world is an example. Taboo words are commonly avoided with euphemisms, such as the English euphemism pass away, meaning \"die\". It is a common source of neologisms and lexical replacement.",
            "output": "Word taboo, also called taboo language, language taboo or linguistic taboo is a kind of taboo that involves restricting the use of words or other parts of language due to social constraints. This may be due to a taboo on specific parts of the language itself (such as certain words, or sounds), or due to the need to avoid a taboo topic. The taboo against naming the dead in parts of the world is an example.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.8163657188415527,
            "normalized": 0.6618187768118722
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 677,
        "original_data": {
            "instruction": "What are some good body weight exercises for shoulders?",
            "input": "",
            "output": "Good body weight exercises for shoulders include plank front arm reaches, pike pushups, plank shoulder taps, arm circles, and dolphin presses.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.912563323974609,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5675000000000001,
            "strategy_details": {
                "raw": 1.1350000000000002,
                "normalized": 1.1350000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 678,
        "original_data": {
            "instruction": "What are five totally distinct, creative ways to describe the usefulness of empty time?",
            "input": "",
            "output": "1. \"Empty time is like an open window, allowing the possibilities of the future to unfold.\" \n2. \"Empty time is like a blank canvas, ready to be filled with new ideas and opportunities.\" \n3. \"Empty time is like a fresh start, allowing us to make the most of the present moment.\" \n4. \"Empty time is like a blank page, waiting to be written upon.\" \n5. \"Empty time is like a vacuum, opening up a space for new ideas to come in.\"",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 1.8281086683273315,
            "normalized": 0.3794596195220947
        },
        "potential_entropy": {
            "strategy_component": 1.24,
            "strategy_details": {
                "raw": 2.48,
                "normalized": 2.48,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 679,
        "original_data": {
            "instruction": "What are the 5 largest ski and snowboard resorts in North America?",
            "input": "",
            "output": "1.) Whistler Blackcomb, 2.) Park City Resort, 3.) Big Sky, 4.) Vail, and 5.) Heavenly",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.289086103439331,
            "normalized": 0.7968817438398089
        },
        "potential_entropy": {
            "strategy_component": 0.5150000000000001,
            "strategy_details": {
                "raw": 1.0300000000000002,
                "normalized": 1.0300000000000002,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.7000000000000001,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 680,
        "original_data": {
            "instruction": "How do I make a perfect grilled cheese sandwich?",
            "input": "",
            "output": "Step 1. Select a cheese that will melt well. Parmesan, for example, is not ideal for a grilled cheese sandwich. Cheddar, Havarti, and Colby Jack are all cheeses that taste great and will melt.\n\nStep 2. Get two slices of bread, preferably not too thick; typical store-bought sliced bread like sourdough, potato, and white make adequate grilled bread.\n\nStep 3. While you are assembling the sandwich, heat a shallow pan over medium-low heat on your stovetop.\n\nStep 4. Spread some mayonnaise on the inside of each slice of bread. Sprinkle some garlic powder over the mayonnaise -- this gives a little extra flavor that one might not expect from a grilled cheese. \n\nStep 5. Layer the cheese between the slices of bread. Don't put too much cheese on it, or it will leak out of the sandwich when you cut it!\n\nStep 6. Assemble the sandwich and put a thin layer of mayo on the outside of each slice. \n\nStep 7. Lay the sandwich inside your heated pan. It should sizzle immediately. Wait two minutes and check to see if the cheese is melting. If the bread is browning too quickly, turn the heat down. When it is adequately browned on one side, use a spatula to flip it over to grill the other side.\n\nStep 8. Once your sandwich is grilled to your liking, turn off the stove and remove your sandwich from the pan. Slice diagonally and serve with a bowl of hot tomato soup. Enjoy!",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.4424898624420166,
            "normalized": 0.5549971035548619
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 681,
        "original_data": {
            "instruction": "Who is Bryan Adams",
            "input": "Bryan Guy Adams OC OBC FRPS (born November 5, 1959) is a Canadian musician, singer, songwriter, composer, and photographer. He has been cited as one of the best-selling music artists of all time, and is estimated to have sold between 75 million and more than 100 million records and singles worldwide. Adams was the most played artist on Canadian radio in the 2010s and has had 25 top-15 singles in Canada and a dozen or more in each of the US, UK, and Australia.",
            "output": "Bryan Guy Adams was born on November 5, 1959 and is a Canadian musician, singer, songwriter, composer, and photographer. He has been cited as one of the best-selling music artists of all time, and is estimated to have sold between 75 million and more than 100 million records and singles worldwide.\n\nAdams was the most played artist on Canadian radio in the 2010s and has had 25 top-15 singles in Canada and a dozen or more in each of the US, UK, and Australia.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.318039655685425,
            "normalized": 0.5194399016244071
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 682,
        "original_data": {
            "instruction": "What's the appeal of a vacation rental on the Big Island of Hawaii?",
            "input": "",
            "output": "The Big Island of Hawaii is a bit quieter than the island of Oahu, where Waikiki Beach is. While Waikiki is a beautiful beach and place, the Big Island of Hawaii has so much to offer that is unique only to this island. First, you can visit black sand beaches. This is due to all of the volcanic activity on the island. There are still active volcanoes on the island. You can also take tours around them should you wish to get closer. You still get all the beautiful beaches Hawaii offers, but with features, you can't find on the other islands. Also, if you want a slightly less touristy destination, the Big Island is for you!",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.9153034687042236,
            "normalized": 0.6900867053440639
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 683,
        "original_data": {
            "instruction": "Who is best Michael Schumacher or Hamilton",
            "input": "",
            "output": "Michael Schumacher is the best player in the world.The time he drove the car is more dangerous and was not regulated like the modern F1",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.3,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "metric_gaps": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 4.410826206207275,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.5550000000000002,
            "strategy_details": {
                "raw": 3.1100000000000003,
                "normalized": 3.1100000000000003,
                "components": {
                    "instruction": 0.6,
                    "input": 1.7000000000000002,
                    "output": 0.8099999999999999
                },
                "gaps": {
                    "ins_tone": 0.5,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.7,
                    "out_div": 0.6,
                    "out_dens": 0.75,
                    "out_bg": 0.65
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 684,
        "original_data": {
            "instruction": "What is the largest pollutant?",
            "input": "",
            "output": "Carbon dioxide (CO2) - a greenhouse gas emitted by burning fossil fuels, deforestation, and other human activities that contribute to climate change.\n\nParticulate matter - tiny particles of pollutants such as dust, smoke, and soot that can be inhaled and cause respiratory problems.\n\nNitrogen oxides (NOx) - gases emitted by vehicle exhausts and industrial processes that contribute to air pollution and can also react with other pollutants to form ozone.\n\nSulfur dioxide (SO2) - a gas emitted by burning fossil fuels and industrial processes that can cause respiratory problems and acid rain.\n\nMethane (CH4) - a potent greenhouse gas emitted by agriculture, waste management, and natural gas production.\n\nChlorofluorocarbons (CFCs) - chemicals used in refrigeration, air conditioning, and industrial processes that contribute to the depletion of the ozone layer.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 1.2491390705108643,
            "normalized": 0.2140397344316755
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 685,
        "original_data": {
            "instruction": "Tell me about NGC 7314",
            "input": "NGC 7314 is a spiral galaxy located in the southern constellation of Piscis Austrinus. It was discovered by English astronomer John Herschel on July 29, 1834. This is a nearby Seyfert (active) galaxy, located at a distance of approximately 54.6 megalight-years from the Milky Way. Since it appears to have detached spiral arm segments (either from dust lanes or bright star clusters), it was listed in Halton Arp's Atlas of Peculiar Galaxies.\n\nWalter Scott Houston describes its appearance in small telescopes:\n\nDo not let its photographic magnitude of 11.6 scare you off, for it can be seen in a 6-inch telescope as a curiously fuzzy object. But it is small, appearing only 4' by 2'.\n\nThe morphological classification of this galaxy is SAB(rs)bc, indicating a spiral galaxy with a weak central bar (SAB), an incomplete ring structure around the bar (rs), and moderately–wound arms (bc). The plane of the galactic disk is inclined by 64° to the line of sight from the Earth, with the major axis aligned along a position angle of 178°. Within the galaxy's core is an active galactic nucleus tentatively classified as a type I Seyfert. The central supermassive black hole has a relatively low mass, estimated as (0.87±0.45)×106 M☉. The core is a source for X-ray emission that is seen to vary dramatically on time scales as low as hours.",
            "output": "NGC 7314 is a spiral galaxy located in the southern constellation of Piscis Austrinus. It was discovered by English astronomer John Herschel on July 29, 1834",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.6,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.4,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 1.6390950679779053,
            "normalized": 0.3254557337079729
        },
        "potential_entropy": {
            "strategy_component": 0.565,
            "strategy_details": {
                "raw": 1.13,
                "normalized": 1.13,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.4,
                    "out_div": 0.6,
                    "out_dens": 0.5,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 686,
        "original_data": {
            "instruction": "What type of babies did Regalado say Jiankui was making?",
            "input": "On 25 November 2018, Regalado posted on MIT Technology Review website that He Jiankui was making CRIPSR babies. As it was publicised, He was prompted to announce his experiment and posted the news of the birth of twins, nicknamed Lulu and Nana, on YouTube in five videos the same day. He formally presented the experiment at the Second International Summit on Human Genome Editing organized at the University of Hong Kong on 28 November 2018. He explained that the experiment was to make the babies resistant to HIV infection as they were (as embryos) obtained from an HIV-positive father. He specifically used a mutant gene named CCR5-Δ32 that is known to confer innate resistance to HIV. The twins were born in secrecy in October 2018, and a third baby (revealed in 2022 as Amy) was then almost born, as He reported.",
            "output": "Regalado said that Jiankui was making CRIPSR babies.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 7.299168109893799,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 687,
        "original_data": {
            "instruction": "Provide a bullet point list of all primary matter phase transitions, with the name of the process following by the matter states involved  Use the format: {process name}: {initial state} -> {final state}",
            "input": "",
            "output": "- melting: solid -> liquid\n- sublimation: solid -> gas\n- freezing: liquid -> solid\n- vaporization: liquid -> gas\n- ionization: gas -> plasma\n- condensation: gas -> liquid\n- deposition: gas -> solid\n- deionization: plasma -> gas",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 1.358965277671814,
            "normalized": 0.2454186507633754
        },
        "potential_entropy": {
            "strategy_component": 0.36250000000000004,
            "strategy_details": {
                "raw": 0.7250000000000001,
                "normalized": 0.7250000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 688,
        "original_data": {
            "instruction": "Extract the highest point in the city of Paris from this paragraph.",
            "input": "Paris is located in northern central France, in a north-bending arc of the river Seine whose crest includes two islands, the Île Saint-Louis and the larger Île de la Cité, which form the oldest part of the city. The river's mouth on the English Channel (La Manche) is about 233 mi (375 km) downstream from the city. The city is spread widely on both banks of the river. Overall, the city is relatively flat, and the lowest point is 35 m (115 ft) above sea level. Paris has several prominent hills, the highest of which is Montmartre at 130 m (427 ft).",
            "output": "The highest points is Montmartre at 130m or 427ft.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.463129043579102,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 689,
        "original_data": {
            "instruction": "Where is the largest living thing located?",
            "input": "",
            "output": "Oregon, USA",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.1,
            "s_out_dens": 0.5,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.9,
            "s_out_dens": 0.5,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.2660346031188965,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.33,
            "strategy_details": {
                "raw": 2.66,
                "normalized": 2.66,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.7200000000000001
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.9,
                    "out_dens": 0.5,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 690,
        "original_data": {
            "instruction": "On what date was the country of Belize granted independence?",
            "input": "",
            "output": "Belize was granted independence from the British Empire on September 21, 1981.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.652641773223877,
            "normalized": 0.6150405066353934
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 691,
        "original_data": {
            "instruction": "Steps to choose right colour for occasion",
            "input": "",
            "output": "Blue is  the most recommended interview outfit\nWarm colours like yellow, orange, or red are good for Company events\nBold colour like red or blue are good for parties\nFor court and legal the colours  are blue, grey, and brown.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.280693054199219,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 692,
        "original_data": {
            "instruction": "Why do people like to ski?",
            "input": "",
            "output": "Skiing is a great way to get out into the open air in the winter months.  It also provides a great way to stay active, challenge yourself, be with family members, and enjoy the great outdoors.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.465472459793091,
            "normalized": 0.5615635599408831
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 693,
        "original_data": {
            "instruction": "What is the answer to life, the universe, and everything?",
            "input": "",
            "output": "42",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 8.544265747070312,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 694,
        "original_data": {
            "instruction": "Who is Richard Wayne Bogosian?",
            "input": "Richard Wayne Bogosian (born July 18, 1937) is an American diplomat.",
            "output": "Richard Wayne Bogosian (born July 18, 1937) is an American diplomat. He was the United States Ambassador to Chad from 1990 to 1993 and Niger from 1985 to 1988.\nHe graduated from Tufts College with an A.B. in 1959 and the University of Chicago with a J.D. in 1962. He joined the U.S. Foreign Service in 1962. He oversaw many diplomatic posts, including at the Foreign Service Institute from 1962 to 1963, in the Bureau of Near Eastern and South Asian Affairs at the Department of State in 1963, at the U.S. Embassy in Baghdad, Iraq from 1963 to 1965, at the Foreign Service Institute again in 1965, as vice consul for the U.S. Embassy in Paris, France, from 1966 to 1968, in the Bureau of Near East and South Asian Affairs at the Department of State from 1968 to 1969, in the Bureau for Intelligence and Research at the Department of State from 1969 to 1971, and in the Foreign Service Institute for Economics in 1972.\n\nHe was chief of the economic section at the U.S. Embassy in Kuwait from 1972 to 1976, and Deputy Chief of Mission at the U.S. Embassy in Khartoum, Sudan, from 1976 to 1979. From 1976 to 1979, Bogosian was the chief of the Aviation Negotiations Division. He was the Director of East African Affairs from 1982 to 1985, and was nominated to be the United States Ambassador to Niger on August 1, 1985, by President Ronald Reagan.\nFrom 1988 to 1990 he was the Director of the Office of Monetary Affairs.In 1990, Bogosian became the United States Ambassador to Chad from 1990 to 1993. From 1993 to 1994, he was the Coordinator at the U.S. Liaison Office in Mogadishu, Somalia, and Coordinator for Rwanda and Burundi from 1996 to 1997, after the Rwandan genocide.\n\nBogosian speaks Arabic and French. He is married to Claire Marie Bogosian, and has three children.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            2,
            0
        ],
        "entropy": {
            "raw": 1.453230857849121,
            "normalized": 0.27235167367117746
        },
        "potential_entropy": {
            "strategy_component": 0.8849999999999999,
            "strategy_details": {
                "raw": 1.7699999999999998,
                "normalized": 1.7699999999999998,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.5,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 695,
        "original_data": {
            "instruction": "What time of year does school normally start in the United States?",
            "input": "",
            "output": "School normally starts in early Fall, in either August or September",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.781468391418457,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 696,
        "original_data": {
            "instruction": "Who is the star of the movie Breakfast at Tiffany's?",
            "input": "",
            "output": "Audrey Hepburn starred in Breakfast at Tiffany's",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.130354881286621,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 697,
        "original_data": {
            "instruction": "Given this paragraph, who wrote and directed Heads I Win, Tails You Lose",
            "input": "Heads I Win, Tails You Lose (Italian: Testa o Croce, also known as Heads or Tails) is a 1982 Italian comedy film written and directed by Nanni Loy.\n\nThe film consists in two back-to-back stories that deals with two \"taboo\" themes, the celibacy of the clergy in the episode of Renato Pozzetto and the homosexuality in the one with Nino Manfredi.",
            "output": "Based on this passage of text, Nanni Loy wrote and directed the movie Heads I Win, Tails You Lose.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 1.0,
            "s_out_div": 0.6,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.0,
            "s_out_div": 0.4,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 6.0671706199646,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.36750000000000005,
            "strategy_details": {
                "raw": 0.7350000000000001,
                "normalized": 0.7350000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.0,
                    "out_div": 0.4,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 698,
        "original_data": {
            "instruction": "which of these cities are in asia: cardiff, amsterdam, mumbai, bangalore, delhi?",
            "input": "",
            "output": "The following cities are in Asia: Mumbai, Bangalore( new name Bengalooru), Delhi. Cardiff is in the U.K. and Amsterdam in Europe.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.544908046722412,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.9124999999999999,
            "strategy_details": {
                "raw": 1.8249999999999997,
                "normalized": 1.8249999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 699,
        "original_data": {
            "instruction": "Name a winner of the Fields Medal in mathematics.",
            "input": "Maryam Mirzakhani was an Iranian mathematician and a professor of mathematics at Stanford University. Her research topics included Teichmüller theory, hyperbolic geometry, ergodic theory, and symplectic geometry. In 2005, as a result of her research, she was honored in Popular Science's fourth annual \"Brilliant 10\" in which she was acknowledged as one of the top 10 young minds who have pushed their fields in innovative directions. Mirzakhani was honored with the Fields Medal, the most prestigious award in mathematics, becoming the first Iranian to be honored with the award and the first of only two women to date. The award committee cited her work in \"the dynamics and geometry of Riemann surfaces and their moduli spaces\". Mirzakhani died of breast cancer at the age of 40 in 2017.",
            "output": "Maryam Mirzakhani, an Iranian mathematician, received the Fields Medal in mathematics in 2014 before her untimely death from breast cancer in 2017.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.0833911895751953,
            "normalized": 0.4523974827357701
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 700,
        "original_data": {
            "instruction": "What are some common characteristics of Maine Coon cats?",
            "input": "",
            "output": "Maine Coon cats are a large breed that tends to have long-haired cats and exceptionally fluffy tails. They have distinct facial characteristics, including larger head, more pronounced chins, and tufted ear tips. They are also perceived to be loyal and more intelligent than average cats.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.332059144973755,
            "normalized": 0.8091597557067871
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 701,
        "original_data": {
            "instruction": "What is a hunting dog?",
            "input": "",
            "output": "Hunting dogs are usually paired with a human to help in the sport of hunting.  They have been used for hundreds of years and are considered to improve the hunters abilities.  Hunting dogs are typically pointers or retrievers.  There are many breeds, such as German Shorthaired Pointers and labradors just to name a couple.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.125303268432617,
            "normalized": 0.7500866481236049
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 702,
        "original_data": {
            "instruction": "How long has Dataminr been operating and who are their customers?",
            "input": "Dataminr was founded in 2009 by Yale University graduates Ted Bailey, Sam Hendel and Jeff Kinsey. Dataminr came to wider notice when it issued an alert that Osama bin Laden had been killed 23 minutes faster than major news organizations.\n\nIn 2014, Datamnr entered into a partnership with CNN and Twitter, resulting in Dataminr for News, a tool to \"alert journalists to information that’s emerging on Twitter in real time.\"\n\nOn December 30, 2019, Dataminr claimed to have detected the first signals of the COVID-19 outbreak within public social media posts.[citation needed] The company went on to detect clusters indicating future spikes in 14 different US states. Seven days later, all 14 states were hit hard by the coronavirus. Dataminr partnered with the UN in May 2019 to equip thousands of UN personnel with Dataminr’s First Alert product for the public sector.\n\nDataminr's social media intelligence contract for the FBI was taken over by Zerofox at the end of 2020.\n\nOn the morning of January 5, 2021, Dataminr warned Capitol security officials of troubling online public chatter that would soon become the January 6 riot.\n\nIn July 2021, Dataminr conducted its first M&A transaction when it acquired WatchKeeper, a UK-based geovisualization platform. In the acquisition, Dataminr combined WatchKeeper’s geovisualized data layers with its Pulse platform to provide context around events. A few months later, in October 2021, Dataminr acquired Krizo, a real-time crisis response platform based in Copenhagen, Denmark.\n\nThe company ranked #5 on the Forbes AI 50 List in 2019 and was the winner of an 2019 AI Breakthrough Award for Best Overall AI Solution. In 2020, Dataminr was named to Forbes Cloud 100, and Deloitte Technology Fast 500, ‘Most Innovative Use of AI’ at the 2020 AI & Machine Learning Awards.\n\nIn 2020, Dataminr’s AI team had published several papers including, “Unsupervised Detection of Sub-Events in Large Scale Disasters,” for the Association for the Advancement of Artificial Intelligence quarterly journal; “Multimodal Categorization of Crisis Events in Social Media,” for Computer Vision and Pattern Recognition 2020; “Clustering of Social Media Messages for Humanitarian Aid Response during Crisis,” for AI for Social Good, and “The ApposCorpus: A new multilingual, multi-domain dataset for factual appositive generation,” for COLING 2020.\n\nIn late 2021, the company announced the appointment of Cristina Bita to its board of directors and as chairwoman of the audit committee.",
            "output": "Dataminr was founded in 2009 so it has been operating for 14 years. Dataminr sells to variety of organisations in the public and private sector such as the UN and journalists from new media organisations.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.4273695945739746,
            "normalized": 0.8363913127354213
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 703,
        "original_data": {
            "instruction": "ways to reduce solid waste",
            "input": "",
            "output": "Use biodegradable plates\nUse reusable personal bottles\nUse less plastic products\nUse less detergent\nUse more biodegradable FMCG products",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.65,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.35,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 4.029647350311279,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.35,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 704,
        "original_data": {
            "instruction": "My dog is a 100lb pitbull purebreed. She's always barking at people when we walk her on the street. She's friendly, however, when outside with me or the kids, her bark is loud and deep. How can I get my dog to stop barking at strangers and remove her feeling of \"threat\"?",
            "input": "",
            "output": "Ensure that when you walk her to use a prong collar. Ensure the collar is not loose on her neck, but high up and tight (cannot move easily) under her ear. This will make sure she doesn't take sudden movements and allow her to ensure that you will direct her whether the oncoming distraction is a threat.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.4,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.6,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.249111175537109,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.87,
            "strategy_details": {
                "raw": 1.74,
                "normalized": 1.74,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 0.9,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.6,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 705,
        "original_data": {
            "instruction": "Does Jukebox contain many original songs?",
            "input": "Jukebox is the eleventh studio album by Australian pop vocal group Human Nature released in October 2014. It contains covers from the 1950s and 1960s, with one original song, \"End of Days\"",
            "output": "No, it contains covers from the 1950s and 1960s, with only one original song.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.7793004512786865,
            "normalized": 0.651228700365339
        },
        "potential_entropy": {
            "strategy_component": 0.47250000000000003,
            "strategy_details": {
                "raw": 0.9450000000000001,
                "normalized": 0.9450000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.6,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 706,
        "original_data": {
            "instruction": "Classify each of the following as either a natural hair color or a grocery store bakery brand: Hostess, blonde, brunette, Betty Crocker, Pilsbury, red, black, Entenmann's",
            "input": "",
            "output": "Natural hair color: blonde, brunette, red, black\nGrocery store bakery brands: Hostess, Betty Crocker, Pilsbury, Entenmann's",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 4.559666633605957,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.265,
            "strategy_details": {
                "raw": 0.53,
                "normalized": 0.53,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 707,
        "original_data": {
            "instruction": "Who is credited with the recognition of electromagnetism?",
            "input": "Long before any knowledge of electricity existed, people were aware of shocks from electric fish. Ancient Egyptian texts dating from 2750 BCE referred to these fish as the \"Thunderer of the Nile\", and described them as the \"protectors\" of all other fish. Electric fish were again reported millennia later by ancient Greek, Roman and Arabic naturalists and physicians. Several ancient writers, such as Pliny the Elder and Scribonius Largus, attested to the numbing effect of electric shocks delivered by electric catfish and electric rays, and knew that such shocks could travel along conducting objects. Patients with ailments such as gout or headache were directed to touch electric fish in the hope that the powerful jolt might cure them.\nAncient cultures around the Mediterranean knew that certain objects, such as rods of amber, could be rubbed with cat's fur to attract light objects like feathers. Thales of Miletus made a series of observations on static electricity around 600 BCE, from which he believed that friction rendered amber magnetic, in contrast to minerals such as magnetite, which needed no rubbing.\nThales was incorrect in believing the attraction was due to a magnetic effect, but later science would prove a link between magnetism and electricity. According to a controversial theory, the Parthians may have had knowledge of electroplating, based on the 1936 discovery of the Baghdad Battery, which resembles a galvanic cell, though it is uncertain whether the artifact was electrical in nature.\nElectricity would remain little more than an intellectual curiosity for millennia until 1600, when the English scientist William Gilbert wrote De Magnete, in which he made a careful study of electricity and magnetism, distinguishing the lodestone effect from static electricity produced by rubbing amber. He coined the New Latin word electricus (\"of amber\" or \"like amber\",, elektron, the Greek word for \"amber\") to refer to the property of attracting small objects after being rubbed. This association gave rise to the English words \"electric\" and \"electricity\", which made their first appearance in print in Thomas Browne's Pseudodoxia Epidemica of 1646.\nFurther work was conducted in the 17th and early 18th centuries by Otto von Guericke, Robert Boyle, Stephen Gray and C. F. du Fay. Later in the 18th century, Benjamin Franklin conducted extensive research in electricity, selling his possessions to fund his work. In June 1752 he is reputed to have attached a metal key to the bottom of a dampened kite string and flown the kite in a storm-threatened sky. A succession of sparks jumping from the key to the back of his hand showed that lightning was indeed electrical in nature. He also explained the apparently paradoxical behavior of the Leyden jar as a device for storing large amounts of electrical charge in terms of electricity consisting of both positive and negative charges\nIn 1775, Hugh Williamson reported a series of experiments to the Royal Society on the shocks delivered by the electric eel; that same year the surgeon and anatomist John Hunter described the structure of the fish's electric organs. In 1791, Luigi Galvani published his discovery of bioelectromagnetics, demonstrating that electricity was the medium by which neurons passed signals to the muscles. Alessandro Volta's battery, or voltaic pile, of 1800, made from alternating layers of zinc and copper, provided scientists with a more reliable source of electrical energy than the electrostatic machines previously used. The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820. Michael Faraday invented the electric motor in 1821, and Georg Ohm mathematically analysed the electrical circuit in 1827. Electricity and magnetism (and light) were definitively linked by James Clerk Maxwell, in particular in his \"On Physical Lines of Force\" in 1861 and 1862. \nWhile the early 19th century had seen rapid progress in electrical science, the late 19th century would see the greatest progress in electrical engineering. Through such people as Alexander Graham Bell, Ottó Bláthy, Thomas Edison, Galileo Ferraris, Oliver Heaviside, Ányos Jedlik, William Thomson, 1st Baron Kelvin, Charles Algernon Parsons, Werner von Siemens, Joseph Swan, Reginald Fessenden, Nikola Tesla and George Westinghouse, electricity turned from a scientific curiosity into an essential tool for modern life.\nIn 1887, Heinrich Hertz discovered that electrodes illuminated with ultraviolet light create electric sparks more easily. In 1905, Albert Einstein published a paper that explained experimental data from the photoelectric effect as being the result of light energy being carried in discrete quantized packets, energising electrons. This discovery led to the quantum revolution. Einstein was awarded the Nobel Prize in Physics in 1921 for \"his discovery of the law of the photoelectric effect\". The photoelectric effect is also employed in photocells such as can be found in solar panels.\nThe first solid-state device was the \"cat's-whisker detector\" first used in the 1900s in radio receivers. A whisker-like wire is placed lightly in contact with a solid crystal (such as a germanium crystal) to detect a radio signal by the contact junction effect. In a solid-state component, the current is confined to solid elements and compounds engineered specifically to switch and amplify it. Current flow can be understood in two forms: as negatively charged electrons, and as positively charged electron deficiencies called holes. These charges and holes are understood in terms of quantum physics. The building material is most often a crystalline semiconductor.\nSolid-state electronics came into its own with the emergence of transistor technology. The first working transistor, a germanium-based point-contact transistor, was invented by John Bardeen and Walter Houser Brattain at Bell Labs in 1947, followed by the bipolar junction transistor in 1948.",
            "output": "The recognition of electromagnetism, the unity of electric and magnetic phenomena, is due to Hans Christian Ørsted and André-Marie Ampère in 1819–1820.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.3966610431671143,
            "normalized": 0.5419031551906041
        },
        "potential_entropy": {
            "strategy_component": 0.23500000000000004,
            "strategy_details": {
                "raw": 0.4700000000000001,
                "normalized": 0.4700000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 708,
        "original_data": {
            "instruction": "How do you do meal prep efficiently and make cooking easy?",
            "input": "",
            "output": "Doing meal prep efficiently is mainly about getting things prepared before your cooking time and use the right kitchen appliances to free you up. You will want to prepare the ingratiations for multiple meals together. Bulk cutting and seasoning your vegetables and meats first. Put them in containers with the same dimensions like silicon bags in your freezer or refrigerator. Then, when you need to cook, you can start cooking right away. Also, use kitchen appliances like Instant Pot and oven can help you free up your hands to do other things. There are lots of Instant Pot or one pan recipes online to help you get inspired.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.636387348175049,
            "normalized": 0.8961106709071568
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 709,
        "original_data": {
            "instruction": "What is the difference between the Scala Future onComplete and andThen functions?",
            "input": "",
            "output": "Both `andThen` and `onComplete` allow you to observe the result of some future via a callback function. However, `andThen` can be chained, and unlike `onComplete`, it guarantees the order in which callback functions will be applied. In addition, the callback given to `andThen` may be a partial function, handling only a subset of outcomes (e.g., only `Success` or only `Failure`).\n\nConsider the following example using `andThen` and `map`:\n\n```\ndef demoAndThen(fut: Future[Int]): Future[Int] = {\n  fut.andThen { case Success(value) => println(s\"andThen1: $value\") }\n    .andThen { case Success(value) => println(s\"andThen2: $value\") }\n    .map { i =>\n      println(s\"map: $i\")\n      i\n    }\n}\n```\n\nWhen `fut` completes successfully, \"andthen1: ...\", \"andThen2: ...\" and \"map: ...\" will be printed in that order.\n\nConsider the analogous example using `onComplete` and `map`:\n\n```\ndef demoOnComplete(fut: Future[Int]): Future[Int] = {\n  fut.onComplete { t: Try[Int] => println(s\"onComplete1: $t\") }\n  fut.onComplete { t: Try[Int] => println(s\"onComplete2: $t\") }\n  fut.map { i =>\n    println(s\"map: $i\")\n    i\n  }\n}\n```\n\nIn this case, when `fut` completes successfully, \"onComplete1: ...\", \"onComplete2: ...\" and \"map: ...\" will be printed in some unspecified order.\n\nAlthough `andThen` is chained, the callback function does not generally affect the outcome of the resulting `Future`. But be aware: if the callback throws some fatal errors, the resulting future will never complete.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 1.5568617582321167,
            "normalized": 0.30196050235203337
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 710,
        "original_data": {
            "instruction": "What is one of your favorite color?",
            "input": "",
            "output": "My favorite color is purple. Purple is a great color for many reasons. The hue and tone are quite pleasing to the eye. Additionally, purple is complementary to many other colors such as green, yellow, and gray. The color purple is often a color symbolizing richness and celebration, while it is a mature color it is also lively.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.086336135864258,
            "normalized": 0.7389531816755023
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 711,
        "original_data": {
            "instruction": "What nationality and percentage were forced into internment camps?",
            "input": "",
            "output": "Japanese Americans ethnic group were forced into internment camps. Around 70% of Japanese Americans were impacted by this.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.4,
            "s_out_dens": 0.6,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.6,
            "s_out_dens": 0.4,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.489790439605713,
            "normalized": 0.8542258398873466
        },
        "potential_entropy": {
            "strategy_component": 0.8025,
            "strategy_details": {
                "raw": 1.605,
                "normalized": 1.605,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.6,
                    "out_dens": 0.4,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 712,
        "original_data": {
            "instruction": "Where is Meerut?",
            "input": "Meerut (pronunciation (help·info), IAST: Meraṭh) is a city in Meerut district of the western part of the Indian state of Uttar Pradesh. The city lies 80 km (50 mi) northeast of the national capital New Delhi, within the National Capital Region and 480 km (300 mi) west of the state capital Lucknow.",
            "output": "Meerut is a city in India and part of Uttar Pradesh state close to new Delhi",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.4228262901306152,
            "normalized": 0.8350932257516044
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 713,
        "original_data": {
            "instruction": "Who is the rbr formula 1 team principal?",
            "input": "",
            "output": "The Red Bull Racing team principal is Christian Horner",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.965907096862793,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 714,
        "original_data": {
            "instruction": "Who is Lee Jung-sik",
            "input": "",
            "output": "Lee Jung-sik (born July 6, 1995) is a South Korean actor and model. He is best known for his starring roles in the web series I Have Three Boyfriends (2019) and Summer Guys (2021).",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.057666778564453,
            "normalized": 0.4450476510184152
        },
        "potential_entropy": {
            "strategy_component": 1.2625,
            "strategy_details": {
                "raw": 2.525,
                "normalized": 2.525,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 715,
        "original_data": {
            "instruction": "Tell me whether these teams have ever won the Champions League: Real Madrid, Manchester United, Manchester City, Liverpool, Paris Saint-Germain",
            "input": "",
            "output": "Winners;\nReal Madrid, Liverpool, Manchester United\nDespite the recent success in their national leagues, Manchester City and Paris Saint-Germain are still competing to win their first UEFA Champions League title.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.676042318344116,
            "normalized": 0.9074406623840332
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 716,
        "original_data": {
            "instruction": "classify the following as living vs non-living: plants, pants, bees, beer, mug, goat, pen, night lamp, cow, pig, phone, dog, leaf, candle, mosquito",
            "input": "",
            "output": "living: plants, bees, goat, cow, dog, mosquito, leaf\nnon-living: pants, beer, mug, pen, night lamp, phone, candle",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 4.0914154052734375,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 717,
        "original_data": {
            "instruction": "Give me a summary of how the citric acid cycle works",
            "input": "The citric acid cycle (CAC)—also known as the Krebs cycle, Szent-Györgyi-Krebs cycle or the TCA cycle (tricarboxylic acid cycle)—is a series of chemical reactions to release stored energy through the oxidation of acetyl-CoA derived from carbohydrates, fats, and proteins. The Krebs cycle is used by organisms that respire (as opposed to organisms that ferment) to generate energy, either by anaerobic respiration or aerobic respiration. In addition, the cycle provides precursors of certain amino acids, as well as the reducing agent NADH, that are used in numerous other reactions. Its central importance to many biochemical pathways suggests that it was one of the earliest components of metabolism and may have originated abiogenically. Even though it is branded as a 'cycle', it is not necessary for metabolites to follow only one specific route; at least three alternative segments of the citric acid cycle have been recognized.\n\nThe name of this metabolic pathway is derived from the citric acid (a tricarboxylic acid, often called citrate, as the ionized form predominates at biological pH) that is consumed and then regenerated by this sequence of reactions to complete the cycle. The cycle consumes acetate (in the form of acetyl-CoA) and water, reduces NAD+ to NADH, releasing carbon dioxide. The NADH generated by the citric acid cycle is fed into the oxidative phosphorylation (electron transport) pathway. The net result of these two closely linked pathways is the oxidation of nutrients to produce usable chemical energy in the form of ATP.\n\nIn eukaryotic cells, the citric acid cycle occurs in the matrix of the mitochondrion. In prokaryotic cells, such as bacteria, which lack mitochondria, the citric acid cycle reaction sequence is performed in the cytosol with the proton gradient for ATP production being across the cell's surface (plasma membrane) rather than the inner membrane of the mitochondrion.\n\nFor each pyruvate molecule (from glycolysis), the overall yield of energy-containing compounds from the citric acid cycle is three NADH, one FADH2, and one GTP.",
            "output": "The citric acid cycle is used by organisms to generate energy. This is achieved through a series of chemical reactions that occur in mitochondria to release stored energy through the oxidation of acetyl-CoA derived from fats, proteins, and carbohydrates.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.243283987045288,
            "normalized": 0.4980811391557966
        },
        "potential_entropy": {
            "strategy_component": 0.3925,
            "strategy_details": {
                "raw": 0.785,
                "normalized": 0.785,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 718,
        "original_data": {
            "instruction": "what is fish oil?",
            "input": "",
            "output": "Fish oil is produced from the tissues of fish.  Fish that contain omega-3 fatty acids are the ones that are used to produce fish oil supplements.  These fatty acids have been show to reduce inflammation in the body and may reduce your risk for heart disease, high blood pressure, and rheumatoid arthritis.  Fish oil can be consumed daily as a morning or nightly supplement for support or enhance your immune health.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.6895740032196045,
            "normalized": 0.6255925723484584
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 719,
        "original_data": {
            "instruction": "What are the benefits of in-person learning versus online?",
            "input": "",
            "output": "In-person learning allows for a more engaged and personalized learning experience. Whereas, online learning allows reaching a broader audience of learners who cannot attend in person for geographical reasons. \n\nIn-person learning provides a positive employee experience which allows learners opportunities to interact with colleagues or meet new colleagues. It is not non-common for employees to meet team members for the first time in years during in-person learning. \n\nThe company culture usually makes its strongest display during these events and fosters a feeling of belonging within the company. Events like this build or strengthen much-needed skills for employees and fosters a culture of innovation that is required to remain competitive. Therefore, in-person learning is a win-win for both employees and the company.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.29382586479187,
            "normalized": 0.7982359613691058
        },
        "potential_entropy": {
            "strategy_component": 0.58,
            "strategy_details": {
                "raw": 1.16,
                "normalized": 1.16,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 720,
        "original_data": {
            "instruction": "Based on this paragraph, what does Briony mean when she says at the conclusion: \"\"I like to think that it isn't weakness or evasion, but a final act of kindness, a stand against oblivion and despair, to let my lovers live and to unite them at the end.\"",
            "input": "The final section, titled \"London 1999\", is narrated by Briony herself in the form of a diary entry. Now 77, she is a successful novelist who has recently been diagnosed with vascular dementia, so she is facing rapid mental decline. The reader learns that Briony is the author of the preceding sections of the novel. Briony attends a party in her honor at the Tallis family home, where the extended Tallis children perform The Trials of Arabella, the play that 13-year-old Briony had written and unsuccessfully attempted to stage with her cousins in the summer of 1935. Leon and Pierrot are in attendance, Jackson is fifteen years deceased, and Lola is alive but does not attend. Finally, Briony reveals to the reader that Robbie Turner died of septicaemia on the beaches of Dunkirk, that Cecilia was killed several months later when a bomb destroyed Balham Underground station during the Blitz, and that Briony's story of seeing them together in 1940 was a fabrication. Briony did attend Lola's wedding to Paul Marshall, but confesses she was too \"cowardly\" to visit the recently bereaved Cecilia to make amends. The novel, which she says is factually true apart from Robbie and Cecilia being reunited, is her lifelong attempt at \"atonement\" for what she did to them. Briony justifies her invented happy ending by saying she does not see what purpose it would serve to give readers a \"pitiless\" story. She writes, \"I like to think that it isn't weakness or evasion, but a final act of kindness, a stand against oblivion and despair, to let my lovers live and to unite them at the end.\"",
            "output": "Based on this passage, Briony is explaining, as the narrator of the novel, why she fabricated the portion of the story involving the reunion of Robbie and Cecilia.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.145949363708496,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 721,
        "original_data": {
            "instruction": "Write a haiku about how much I love pickles.",
            "input": "",
            "output": "Caressed by sweet brine,\nyou are crispy and crunchy.\nOh, won’t you be mine?",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 1.0,
            "s_out_div": 0.5,
            "s_out_dens": 0.9,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.0,
            "s_out_div": 0.5,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.026976585388184,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.0,
                    "out_div": 0.5,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 722,
        "original_data": {
            "instruction": "Provide a possible jurisdiction section for a federal civil rights complaint leaving blanks for the location.",
            "input": "",
            "output": "This is a civil rights complaint arising out of the county of _______ within the state of _______. This complaint sets forth causes of action under Title 42 SS 1981, 1982, 1983, 1985, 1986. The Plaintiff will show that her ability to engage in a contractual agreement was infringed upon by a private corporation conspiring with state actors acting under color of state law in violation of the aforementioned federal statutes. These facts give this honorable court jurisdiction over the matter.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9401843547821045,
            "normalized": 0.6971955299377441
        },
        "potential_entropy": {
            "strategy_component": 0.4275,
            "strategy_details": {
                "raw": 0.855,
                "normalized": 0.855,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.44999999999999996,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 723,
        "original_data": {
            "instruction": "At the concert last night, identify each of these musicians as singer, drummer, guitarist, or bassist: Steve Harris, Bruce Dickinson, George Lynch, Nicko McBrain, Angus Young",
            "input": "",
            "output": "Steve Harris - bassist\nBruce Dickinson - singer\nGeorge Lynch - guitarist\nNicko McBrain - drummer\nAngus Young - guitarist",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9724929332733154,
            "normalized": 0.9921408380780902
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 724,
        "original_data": {
            "instruction": "Where is the Black See?",
            "input": "The Black Sea is a marginal mediterranean sea of the Atlantic Ocean lying between Europe and Asia, east of the Balkans, south of the East European Plain, west of the Caucasus, and north of Anatolia. It is bounded by Bulgaria, Georgia, Romania, Russia, Turkey, and Ukraine. The Black Sea is supplied by major rivers, principally the Danube, Dnieper, and Don. Consequently, while six countries have a coastline on the sea, its drainage basin includes parts of 24 countries in Europe.",
            "output": "The Black Sea is lying between Europe and Asia, east of the Balkans, south of the East European Plain, west of the Caucasus, and north of Anatolia. It is bounded by Bulgaria, Georgia, Romania, Russia, Turkey, and Ukraine.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 1.9802398681640625,
            "normalized": 0.42292567661830355
        },
        "potential_entropy": {
            "strategy_component": 0.365,
            "strategy_details": {
                "raw": 0.73,
                "normalized": 0.73,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 725,
        "original_data": {
            "instruction": "What are five totally distinct, creative ways to describe the usefulness of the police?",
            "input": "",
            "output": "1. The police serve as a \"watchful guardian\" to keep us safe.\n2. The police are a \"protector of the law\" who protects us from harm.\n3. The police are an \"impartial enforcer\" of the law.\n4. The police are an \"intermediary of justice\" who helps us get justice.\n5. The police are a \"counselor of the law\" who helps us understand the law.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 1.78847074508667,
            "normalized": 0.3681344985961914
        },
        "potential_entropy": {
            "strategy_component": 0.505,
            "strategy_details": {
                "raw": 1.01,
                "normalized": 1.01,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 726,
        "original_data": {
            "instruction": "Given this article about Operation Aurora, Which nation was beleived to be behund the attacks?",
            "input": "Operation Aurora was a series of cyber attacks conducted by advanced persistent threats such as the Elderwood Group based in Beijing, China, with ties to the People's Liberation Army. First publicly disclosed by Google on January 12, 2010, in a blog post, the attacks began in mid-2009 and continued through December 2009.\n\nThe attack was aimed at dozens of other organizations, of which Adobe Systems, Akamai Technologies, Juniper Networks, and Rackspace have publicly confirmed that they were targeted. According to media reports, Yahoo, Symantec, Northrop Grumman, Morgan Stanley, and Dow Chemical were also among the targets.\n\nAs a result of the attack, Google stated in its blog that it plans to operate a completely uncensored version of its search engine in China \"within the law, if at all,\" and acknowledged that if this is not possible, it may leave China and close its Chinese offices. Official Chinese sources claimed this was part of a strategy developed by the U.S. government.\n\nThe attack was named \"Operation Aurora\" by Dmitri Alperovitch, Vice President of Threat Research at cybersecurity company McAfee. Research by McAfee Labs discovered that \"Aurora\" was part of the file path on the attacker's machine that was included in two of the malware binaries McAfee said were associated with the attack. \"We believe the name was the internal name the attacker(s) gave to this operation,\" McAfee Chief Technology Officer George Kurtz said in a blog post.\n\nAccording to McAfee, the primary goal of the attack was to gain access to and potentially modify source code repositories at these high-tech, security, and defense contractor companies. \"[The SCMs] were wide open,\" says Alperovitch. \"No one ever thought about securing them, yet these were the crown jewels of most of these companies in many ways—much more valuable than any financial or personally identifiable data that they may have and spend so much time and effort protecting.\"\n\nHistory\n\nFlowers left outside Google China's headquarters after its announcement it might leave the country\nOn January 12, 2010, Google revealed on its blog that it had been the victim of a cyber attack. The company said the attack occurred in mid-December and originated from China. Google stated that over 20 other companies had been attacked; other sources have since cited that more than 34 organizations were targeted. As a result of the attack, Google said it was reviewing its business in China. On the same day, United States Secretary of State Hillary Clinton issued a brief statement condemning the attacks and requesting a response from China.\n\nOn January 13, 2010, the news agency All Headline News reported that the United States Congress plans to investigate Google's allegations that the Chinese government used the company's service to spy on human rights activists.\n\nIn Beijing, visitors left flowers outside of Google's office. However, these were later removed, with a Chinese security guard stating that this was an \"illegal flower tribute\". The Chinese government has yet to issue a formal response, although an anonymous official stated that China was seeking more information on Google's intentions.\n\nAttackers involved\nFurther information: Cyberwarfare by China\nTechnical evidence including IP addresses, domain names, malware signatures, and other factors, show Elderwood was behind the Operation Aurora attack. The \"Elderwood\" group was named by Symantec (after a source-code variable used by the attackers), and is referred to as the \"Beijing Group\" by Dell Secureworks. The group obtained some of Google's source code, as well as access to information about Chinese activists. Elderwood also targeted numerous other companies in the shipping, aeronautics, arms, energy, manufacturing, engineering, electronics, financial, and software sectors.\n\nThe \"APT\" designation for the Chinese threat actors responsible for attacking Google is APT17.\n\nElderwood specializes in attacking and infiltrating second-tier defense industry suppliers that make electronic or mechanical components for top defense companies. Those firms then become a cyber \"stepping stone\" to gain access to top-tier defense contractors. One attack procedure used by Elderwood is to infect legitimate websites frequented by employees of the target company – a so-called \"water hole\" attack, just as lions stake out a watering hole for their prey. Elderwood infects these less-secure sites with malware that downloads to a computer that clicks on the site. After that, the group searches inside the network to which the infected computer is connected, finding and then downloading executives' e-mails and critical documents on company plans, decisions, acquisitions, and product designs.\n\nAttack analysis\nIn its blog posting, Google stated that some of its intellectual property had been stolen. It suggested that the attackers were interested in accessing Gmail accounts of Chinese dissidents. According to the Financial Times, two accounts used by Ai Weiwei had been attacked, their contents read and copied; his bank accounts were investigated by state security agents who claimed he was under investigation for \"unspecified suspected crimes\". However, the attackers were only able to view details on two accounts and those details were limited to things such as the subject line and the accounts' creation date.\n\nSecurity experts immediately noted the sophistication of the attack. Two days after the attack became public, McAfee reported that the attackers had exploited purported zero-day vulnerabilities (unfixed and previously unknown to the target system developers) in Internet Explorer and dubbed the attack \"Operation Aurora\". A week after the report by McAfee, Microsoft issued a fix for the issue, and admitted that they had known about the security hole used since September. Additional vulnerabilities were found in Perforce, the source code revision software used by Google to manage their source code.\n\nVeriSign's iDefense Labs claimed that the attacks were perpetrated by \"agents of the Chinese state or proxies thereof\".\n\nAccording to a diplomatic cable from the U.S. Embassy in Beijing, a Chinese source reported that the Chinese Politburo directed the intrusion into Google's computer systems. The cable suggested that the attack was part of a coordinated campaign executed by \"government operatives, public security experts and Internet outlaws recruited by the Chinese government.\" The report suggested that it was part of an ongoing campaign in which attackers have \"broken into American government computers and those of Western allies, the Dalai Lama and American businesses since 2002.\" According to The Guardian's reporting on the leak, the attacks were \"orchestrated by a senior member of the Politburo who typed his own name into the global version of the search engine and found articles criticising him personally.\"\n\nOnce a victim's system was compromised, a backdoor connection that masqueraded as an SSL connection made connections to command and control servers running in Illinois, Texas, and Taiwan, including machines that were running under stolen Rackspace customer accounts. The victim's machine then began exploring the protected corporate intranet that it was a part of, searching for other vulnerable systems as well as sources of intellectual property, specifically the contents of source code repositories.\n\nThe attacks were thought to have definitively ended on Jan 4 when the command and control servers were taken down, although it is not known at this point whether or not the attackers intentionally shut them down. However, the attacks were still occurring as of February 2010.\n\nResponse and aftermath\nThe German, Australian, and French governments publicly issued warnings to users of Internet Explorer after the attack, advising them to use alternative browsers at least until a fix for the security hole was made. The German, Australian, and French governments considered all versions of Internet Explorer vulnerable or potentially vulnerable.\n\nIn an advisory on January 14, 2010, Microsoft said that attackers targeting Google and other U.S. companies used software that exploits a hole in Internet Explorer. The vulnerability affects Internet Explorer versions 6, 7, and 8 on Windows 7, Vista, Windows XP, Server 2003, Server 2008 R2, as well as IE 6 Service Pack 1 on Windows 2000 Service Pack 4.\n\nThe Internet Explorer exploit code used in the attack has been released into the public domain, and has been incorporated into the Metasploit Framework penetration testing tool. A copy of the exploit was uploaded to Wepawet, a service for detecting and analyzing web-based malware operated by the computer security group at the University of California, Santa Barbara. \"The public release of the exploit code increases the possibility of widespread attacks using the Internet Explorer vulnerability,\" said George Kurtz, CTO of McAfee, of the attack. \"The now public computer code may help cybercriminals craft attacks that use the vulnerability to compromise Windows systems.\"\n\nSecurity company Websense said it identified \"limited public use\" of the unpatched IE vulnerability in drive-by attacks against users who strayed onto malicious Web sites. According to Websense, the attack code it spotted is the same as the exploit that went public last week.[clarification needed] \"Internet Explorer users currently face a real and present danger due to the public disclosure of the vulnerability and release of attack code, increasing the possibility of widespread attacks,\" said George Kurtz, chief technology officer of McAfee, in a blog update. Confirming this speculation, Websense Security Labs identified additional sites using the exploit on January 19. According to reports from Ahnlab, the second URL was spread through the Instant Messenger network Misslee Messenger, a popular IM client in South Korea.\n\nResearchers have created attack code that exploits the vulnerability in Internet Explorer 7 (IE7) and IE8—even when Microsoft's recommended defensive measure (Data Execution Prevention (DEP)) is turned on.[dubious – discuss] According to Dino Dai Zovi, a security vulnerability researcher, \"even the newest IE8 isn't safe from attack if it's running on Windows XP Service Pack 2 (SP2) or earlier, or on Windows Vista RTM (release to manufacturing), the version Microsoft shipped in January 2007.\"\n\nMicrosoft admitted that the security hole used had been known to them since September. Work on an update was prioritized and on Thursday, January 21, 2010, Microsoft released a security patch aiming to counter this weakness, the published exploits based on it and a number of other privately reported vulnerabilities. They did not state if any of the latter had been used or published by exploiters or whether these had any particular relation to the Aurora operation, but the entire cumulative update was termed critical for most versions of Windows, including Windows 7.\n\nSecurity researchers continued to investigate the attacks. HBGary, a security firm, released a report in which they claimed to have found some significant markers that might help identify the code developer. The firm also said that the code was Chinese language based but could not be specifically tied to any government entity.\n\nOn February 19, 2010, a security expert investigating the cyber-attack on Google, has claimed that the people behind the attack were also responsible for the cyber-attacks made on several Fortune 100 companies in the past one and a half years. They have also tracked the attack back to its point of origin, which seems to be two Chinese schools, Shanghai Jiao Tong University and Lanxiang Vocational School. As highlighted by The New York Times, both of these schools have ties with the Chinese search engine Baidu, a rival of Google China. Both Lanxiang Vocational and Jiaotong University have denied the allegation.\n\nIn March 2010, Symantec, which was helping investigate the attack for Google, identified Shaoxing as the source of 21.3% of all (12 billion) malicious emails sent throughout the world.\n\nGoogle retrospective\nOn October 3, 2022, Google on YouTube released a six-episode series covering the events that occurred during Operation Aurora, with commentary from insiders who dealt with the attack, though the series primary focus was to reassure the Google-using public that measures are in place to counter hacking attempts.",
            "output": "China's People's Liberation Army (PLA) is believed to be behind this operation.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.956824779510498,
            "normalized": 0.7019499370029995
        },
        "potential_entropy": {
            "strategy_component": 0.32000000000000006,
            "strategy_details": {
                "raw": 0.6400000000000001,
                "normalized": 0.6400000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 727,
        "original_data": {
            "instruction": "Why does pollution in Delhi increase during the months of Nov-Dec each year?",
            "input": "",
            "output": "Delhi is the capital of India and one of India's largest city.  It has over 30 million residents and an estimated 100+ million vehicles.  Delhi is surrounded by the agricultural states of Punjab and Haryana.  During the period Sep-Nov, the states of Punjab and Haryana harvest rice they have grown during the summer months.  Post the harvest, in order to prepare the fields for sowing again they burn the left over rice husk and dried plants.  The month of November marks the onset of winter in the north Indian plains and during this time the north westerly winds stop.  The combination of high vehicle number, large scale burning of post harvest leftover and onset of winter with no winds results in the months of Nov-Dec being the most polluted in the year.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.018385648727417,
            "normalized": 0.719538756779262
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 728,
        "original_data": {
            "instruction": "Given this paragraph about the United States Marine Corps, tell me when it was created in which city?",
            "input": "The history of the Marine Corps began when two battalions of Continental Marines were formed on 10 November 1775 in Philadelphia as a service branch of infantry troops capable of fighting both at sea and on shore. In the Pacific theater of World War II the Corps took the lead in a massive campaign of amphibious warfare, advancing from island to island. As of 2022, the USMC has around 177,200 active duty members and some 32,400 personnel in reserve.",
            "output": "The Marine Corps was established on the 10th of November in 1775 in Philadelphia, PA.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.3971686363220215,
            "normalized": 0.5420481818062919
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 729,
        "original_data": {
            "instruction": "Can foreign nationals get an Aadhaar in India?",
            "input": "Aadhaar is a 12-digit unique identity number that can be obtained voluntarily by the citizens of India and resident foreign nationals who have spent over 182 days in twelve months immediately preceding the date of application for enrolment, based on their biometric and demographic data. The data is collected by the Unique Identification Authority of India (UIDAI), a statutory authority established in January 2009 by the Government of India, under the jurisdiction of the Ministry of Electronics and Information Technology, following the provisions of the Aadhaar (Targeted Delivery of Financial and other Subsidies, benefits and services) Act, 2016.\nAadhaar is the world's largest biometric ID system. World Bank Chief Economist Paul Romer described Aadhaar as \"the most sophisticated ID programme in the world\". Considered a proof of residence and not a proof of citizenship, Aadhaar does not itself grant any rights to domicile in India. In June 2017, the Home Ministry clarified that Aadhaar is not a valid identification document for Indians travelling to Nepal and Bhutan.\nPrior to the enactment of the Act, the UIDAI had functioned, since 28 January 2009, as an attached office of the Planning Commission (now NITI Aayog). On 3 March 2016, a money bill was introduced in the Parliament to give legislative backing to Aadhaar. On 11 March 2016, the Aadhaar (Targeted Delivery of Financial and other Subsidies, benefits and services) Act, 2016, was passed in the Lok Sabha.",
            "output": "Resident foreign nationals who have spent over 182 days in twelve months immediately preceding the date of application for enrolment, based on their biometric and demographic data get an Aadhaar.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.327230930328369,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 730,
        "original_data": {
            "instruction": "What is the oldest type of flight technology?",
            "input": "",
            "output": "Hot air balloons",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 8.237037658691406,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 731,
        "original_data": {
            "instruction": "What are the biggest cities in Germany?",
            "input": "",
            "output": "The biggest cities in Germany are Berlin, Hamburg, Munich and Cologne. Berlin is the biggest city with almost 3.7 million inhabitants. The second biggest city is Hamburg with 1.9 million inhabitants.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 1.8972223997116089,
            "normalized": 0.39920639991760254
        },
        "potential_entropy": {
            "strategy_component": 0.9649999999999999,
            "strategy_details": {
                "raw": 1.9299999999999997,
                "normalized": 1.9299999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 732,
        "original_data": {
            "instruction": "How many syllables are in the word cat?",
            "input": "",
            "output": "The word cat has only one syllable.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.270879745483398,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 733,
        "original_data": {
            "instruction": "How much music theory knowledge is required to compose music?",
            "input": "",
            "output": "Technically speaking, very little music theory knowledge is “required” to compose music. Music theory is, essentially, what musicians have learned over the course of several hundred years about how to write music that is interesting and sounds good. In other words, music theory is there to help you, not hold you back. \n\nWhen you learn music theory, what often happens is that you expand your broader understanding of music, which can be a very helpful safety net. The more music theory you know, the easier you’ll be able to distinguish between the good ideas and the not-so-good. You need to know the rules before you can break them. You need to be able to write by the book before you throw the book away. If not, you run the risk of writing music that’s not nearly as clever as you think it is, that tries to be daring and experimental but ends up just being dull and vanilla. And frankly, I just don’t see the value in ignoring all the work that centuries worth of musicians have done into knowing how music works. Putting the time into learning and really understanding it can only make you a better musician and a better composer.\n\nSo, that’s my answer. Learn at least as much theory as you need to know to understand the rules you want to break.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.8296256065368652,
            "normalized": 0.6656073161533901
        },
        "potential_entropy": {
            "strategy_component": 1.0750000000000002,
            "strategy_details": {
                "raw": 2.1500000000000004,
                "normalized": 2.1500000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 734,
        "original_data": {
            "instruction": "What are some of the main themes of Jane Eyre",
            "input": "",
            "output": "Some of the main themes of Jane Eyre are social class, love, family, dependence and independence, personal discovery and growth.  There are also themes of religion, pride, duty to society and personal morality",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.4667797088623047,
            "normalized": 0.8476513453892299
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 735,
        "original_data": {
            "instruction": "What's the difference between metrics and logs?",
            "input": "",
            "output": "Logs have finer granularity and are therefore slower to query. Metrics are generally aggregated and faster to query. For example logs could be: \"server 1 is running\", \"server 2 is running\", and the corresponding metric would be \"there are 2 servers running\".",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.008249044418335,
            "normalized": 0.7166425841195243
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 736,
        "original_data": {
            "instruction": "What is the most invaded country in history?",
            "input": "",
            "output": "The most invaded country in history is difficult to determine definitively as there are many variables to consider. However, one country that has been invaded numerous times throughout its history is Poland. Poland has a long history of being invaded and occupied by various neighboring powers. In the 18th century, it was partitioned three times by Russia, Prussia, and Austria, and ceased to exist as an independent state for over a century\n\nDuring World War II, Poland was invaded and occupied by Nazi Germany, resulting in the deaths of over six million Polish citizens, including three million Jews who were murdered in the Holocaust. Following the war, Poland became a satellite state of the Soviet Union and was occupied by Soviet forces until the fall of the Soviet Union in 1989.\n\nThroughout its history, Poland has also been invaded by other powers, including the Mongols, Teutonic Knights, and Ottoman Empire, among others. The country's location in central Europe, as well as its fertile land and valuable resources, have made it a target for invasion throughout history.\n\nOverall, while there is no definitive answer to which country has been the most invaded in history, Poland's history of being invaded and occupied by numerous neighboring powers makes it a strong contender for this title.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 1.8898077011108398,
            "normalized": 0.3970879146030971
        },
        "potential_entropy": {
            "strategy_component": 1.0525,
            "strategy_details": {
                "raw": 2.105,
                "normalized": 2.105,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.8,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 737,
        "original_data": {
            "instruction": "List of Cat Breeds",
            "input": "The following list of cat breeds includes only domestic cat breeds and domestic and wild hybrids. The list includes established breeds recognized by various cat registries, new and experimental breeds, landraces being established as standardized breeds, distinct domestic populations not being actively developed and lapsed (extinct) breeds.\n\nAs of 2023, The International Cat Association (TICA) recognizes 73 standardized breeds, the Cat Fanciers' Association (CFA) recognizes 45, the Fédération Internationale Féline (FIFe) recognizes 50, the Governing Council of the Cat Fancy (GCCF) recognizes 45, and the World Cat Federation (WCF) recognizes 69.\n\nInconsistency in a breed classification and naming among registries means that an individual animal may be considered different breeds by different registries (though not necessarily eligible for registry in them all, depending on its exact ancestry). For example, TICA's Himalayan is considered a colorpoint variety of the Persian by the CFA, while the Javanese (or Colorpoint Longhair) is a color variation of the Balinese in both the TICA and the CFA; both breeds are merged (along with the Colorpoint Shorthair) into a single \"mega-breed\", the Colourpoint, by the World Cat Federation (WCF), who have repurposed the name \"Javanese\" for the Oriental Longhair. Also, \"Colorpoint Longhair\" refers to different breeds in other registries. There are many examples of nomenclatural overlap and differences of this sort. Furthermore, many geographical and cultural names for cat breeds are fanciful selections made by Western breeders to be exotic sounding and bear no relationship to the actual origin of the breeds; the Balinese, Javanese, and Himalayan are all examples of this trend.\n\nThe domestic short-haired and domestic long-haired cat types are not breeds, but terms used (with various spellings) in the cat fancy to describe \"mongrel\" or \"bicolor\" cats by coat length, ones that do not belong to a particular breed. Some registries permit them to be pedigreed and they have been used as foundation stock in the establishment of some breeds.[citation needed] They should not be confused with standardized breeds with similar names, such as the British Shorthair and Oriental Longhair.",
            "output": "Base on the examples taken from the text, give me the total # of The International Cat Association (TICA) recognizes standardized breeds",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 1.0,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.0,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.863306045532227,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.275,
            "strategy_details": {
                "raw": 0.55,
                "normalized": 0.55,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.23999999999999996
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.0,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 738,
        "original_data": {
            "instruction": "What are some adventurous activities we could do during a bachelors party?",
            "input": "",
            "output": "Adventurous activities to do on a bachelors party include mountain biking, sky diving, canyoning, rafting, bungee jumping, laser tag, archery, and paintball.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.956040382385254,
            "normalized": 0.701725823538644
        },
        "potential_entropy": {
            "strategy_component": 0.7274999999999999,
            "strategy_details": {
                "raw": 1.4549999999999998,
                "normalized": 1.4549999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 739,
        "original_data": {
            "instruction": "Who is the 'sole Survivor' on the TV Show Survivor?",
            "input": "Survivor is a reality-competition television franchise produced in many countries around the world. The show features a group of contestants deliberately marooned in an isolated location, where they must provide basic survival necessities for themselves. The contestants compete in challenges for rewards and immunity from elimination. The contestants are progressively eliminated from the game as they are voted out by their fellow contestants until only one remains to be awarded the grand prize and named the \"Sole Survivor\".",
            "output": "The contestants are progressively eliminated from the game as they are voted out by their fellow contestants until only one remains to be awarded the grand prize and named the \"Sole Survivor\".",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.9,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.5619287490844727,
            "normalized": 0.8748367854527065
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 740,
        "original_data": {
            "instruction": "What is Assassin's Creed?",
            "input": "Assassin's Creed is an open-world, action-adventure, and stealth game franchise published by Ubisoft and developed mainly by its studio Ubisoft Montreal using the game engine Anvil and its more advanced derivatives. Created by Patrice Désilets, Jade Raymond, and Corey May, the Assassin's Creed video game series depicts a fictional millennia-old struggle between the Order of Assassins, who fight for peace and free will, and the Knights Templar, who desire peace through order and control. The series features historical fiction, science fiction, and fictional characters intertwined with real-world historical events and historical figures. In most games, players control a historical Assassin while also playing as an Assassin Initiate or someone caught in the Assassin–Templar conflict in the present-day framing story. Considered a spiritual successor to the Prince of Persia series, Assassin's Creed took inspiration from the novel Alamut by the Slovenian writer Vladimir Bartol, based on the historical Hashashin sect of the medieval Middle East.",
            "output": "Assassin's Creed is a game franchise about a mythical group of assassin's fighting against the antagonists, the Templars. While the Assassin's fight for Freedom, the Templars fight for control and World domination. The game is hugely popular amongst the people who play video games. The series has historical characters which makes it even more popular amongst people who play video games. It was created by Ubisoft and is considered built off of Prince of Persia.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.4241104125976562,
            "normalized": 0.8354601178850446
        },
        "potential_entropy": {
            "strategy_component": 0.33,
            "strategy_details": {
                "raw": 0.66,
                "normalized": 0.66,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.15000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 741,
        "original_data": {
            "instruction": "Think of a number of different ways you can cook eggs",
            "input": "",
            "output": "Eggs are versatile and can be cooked using a number of methods including boiling, scrambling, poaching, frying and baking. They can also be beaten and augmented with other ingredients to make an Omelette or Frittata",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.9,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.534803867340088,
            "normalized": 0.5813725335257394
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 742,
        "original_data": {
            "instruction": "What is the Summary of this page?",
            "input": "Franklin Edson (April 5, 1832 – September 24, 1904) was an American merchant who served as the 85th Mayor of New York from 1883 to 1884.\n\nEarly life\nEdson was born in Chester, Vermont on April 5, 1832, where his father had a farm. A descendant of the Puritans, he was the son of Soviah (née Wilson) Edson and Opher Edson.\n\nHe was educated at the local schools and at the Chester Academy in Vermont.\n\nCareer\nBusiness\nAt age twenty, Edson moved to Albany to work in his brother Cyrus' distillery, becoming a partner three years later.\n\nHe left the distillery after his brother's death and started a produce business, which he relocated to New York City in 1866. His venture proved successful during the American Civil War, making Edson wealthy and enabling him to engage in civic, religious and charitable causes. He was an active Episcopalian and a member of Saint James Church, Fordham, in the Bronx.\n\nIn 1873, he became one of the city's most important business leaders when he was appointed President of the New York Produce Exchange.\n\nPolitics\nAn anti-Tammany Democrat, in 1882 he was nominated for Mayor through the efforts of Tammany Hall boss John Murphy to avoid a Democratic Party split between organization loyalists and reformers. Upon taking office in 1883, he angered reformers by appointing Tammany men to key jobs, but he soon embraced civil service reform and other honest government measures.\n\nDuring his term the Brooklyn Bridge was dedicated, the Manhattan Municipal Building was constructed, and work was completed on the city's new water supply, the Croton Aqueduct. He appointed the commission responsible for the selection and location of public lands for parks in the Bronx, which came to include Van Cortlandt, Bronx, Pelham Bay, Crotona, Claremont and St. Mary's Parks, and the Mosholu, Bronx River, Pelham, and Crotona Parkways.\n\nAfter Edson split with Tammany the 1884 Democratic nomination for Mayor went to William Russell Grace, who had also preceded Edson as Mayor, and Edson retired from politics at the completion of his term in 1885.\n\nAfter leaving the mayor's office, Edson returned to his business interests and continued his philanthropic activities.\n\nPersonal life\nIn 1856, Edson was married to Frances Cameron \"Fannie\" Wood (1835–1893), the daughter of Benjamin Howland Wood. Fannie was the granddaughter of Jethro Wood, inventor of the cast-iron moldboard plow. They owned a homestead in Morris Heights, Bronx that consisted of three acres, a stately residence, stable and barn, which he exchanged for 247 Central Park West, two doors south of the corner of 85th Street, in 1893. Together, Fannie and Franklin were the parents of:\n\nCyrus Edson (1857–1903), a doctor who married Virginia Churchill Paige (1853–1891). He served as the President of the Board of Health.\nFranklin (1859-1926), who married Elsie Squier.\nDavid Orr Edson (1862–1923), a doctor.\nHenry Townsend Edson (1864–1903), who married Margarita Diehl. In 1903, Henry Edson murdered his friend's wife and then committed suicide.\nEdith Edson (b. 1870), who married Willis Benner.\nRobert S. (1873-1941), who married Fanny Ropes.\nEthel Townsend Edson (b. 1877), who married Arthur Hoffman Van Brunt (b. 1865).\nHe died at his home in Manhattan on September 24, 1904. He was buried in Section 15, Lot 16 at Albany Rural Cemetery, Menands, New York.\n\nLegacy\nEdson Avenue in The Bronx is named for him.",
            "output": "Franklin Edson is a former Mayor of New York. He started a produce business and became one of the city's most important business leaders. Later, he joined politics as an anti-Tammany Democrat and became the mayor of New York in 1883. He led the effort for public lands for parks in the Bronx. He was married to Frances Cameron \"Fannie\" Wood and they had 7 children.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.248912811279297,
            "normalized": 0.7854036603655133
        },
        "potential_entropy": {
            "strategy_component": 0.41,
            "strategy_details": {
                "raw": 0.82,
                "normalized": 0.82,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 743,
        "original_data": {
            "instruction": "Where is the Lighthouse Point, Bahamas",
            "input": "Lighthouse Point, Bahamas, or simply Lighthouse Point, is a private peninsula in The Bahamas which serves as an exclusive port for the Disney Cruise Line ships. It is located in the south-eastern region of Bannerman Town, Eleuthera. In March 2019, The Walt Disney Company purchased the peninsula from the Bahamian government, giving the company control over the area.",
            "output": "The Lighthouse Point, Bahamas, or simply Lighthouse Point, is a private peninsula in the Bahamas which serves as an exclusive port for the Disney Cruise Line ships. It is located in the south-eastern region of Bannerman Town, Eleuthera.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.272113800048828,
            "normalized": 0.7920325142996651
        },
        "potential_entropy": {
            "strategy_component": 0.3425,
            "strategy_details": {
                "raw": 0.685,
                "normalized": 0.685,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 744,
        "original_data": {
            "instruction": "Extract the key bulletpoints I need to know about University of Oxford",
            "input": "The University of Oxford is a collegiate research university in Oxford, England. There is evidence of teaching as early as 1096, making it the oldest university in the English-speaking world and the world's second-oldest university in continuous operation. It grew rapidly from 1167 when Henry II banned English students from attending the University of Paris. After disputes between students and Oxford townsfolk in 1209, some academics fled north-east to Cambridge where they established what became the University of Cambridge. The two English ancient universities share many common features and are jointly referred to as Oxbridge.\n\nThe university is made up of thirty-nine semi-autonomous constituent colleges, five permanent private halls, and a range of academic departments which are organised into four divisions. All the colleges are self-governing institutions within the university, each controlling its own membership and with its own internal structure and activities. All students are members of a college. It does not have a main campus, and its buildings and facilities are scattered throughout the city centre. Undergraduate teaching at Oxford consists of lectures, small-group tutorials at the colleges and halls, seminars, laboratory work and occasionally further tutorials provided by the central university faculties and departments. Postgraduate teaching is provided predominantly centrally.\n\nOxford operates the world's oldest university museum, as well as the largest university press in the world and the largest academic library system nationwide. In the fiscal year ending 31 July 2019, the university had a total income of £2.45 billion, of which £624.8 million was from research grants and contracts.\n\nOxford has educated a wide range of notable alumni, including 30 prime ministers of the United Kingdom and many heads of state and government around the world. As of October 2022, 73 Nobel Prize laureates, 4 Fields Medalists, and 6 Turing Award winners have studied, worked, or held visiting fellowships at the University of Oxford, while its alumni have won 160 Olympic medals. Oxford is the home of numerous scholarships, including the Rhodes Scholarship, one of the oldest international graduate scholarship programmes.",
            "output": "The university of Oxford is the worlds second oldest university. It's made up of 39 semi-autonomous colleges which are self-governing institutions within the university + a range of academic departments organised in four divisions. Oxford has educated more than 70 nobel laureates, multiple Fields Medalists and Turing Award winners, as well as 30 UK prime ministers. The university has an income larger than 2 billion pounds, it operates the world's oldest university museum and the largest university press.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.22292160987854,
            "normalized": 0.77797760282244
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 745,
        "original_data": {
            "instruction": "Find the four eye shapes on Maitum anthropomorphic pottery and separate them with a comma",
            "input": "Since the eyes are believed to be expressive, the eyes portrayed in Maitum jars are given high regard in describing the dead. The eyes are classified to four major shapes: (1) almond, (2) ovaloid, (3) round and (4) rectangular.",
            "output": "almond, ovaloid, round, rectangular",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 5.6315155029296875,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.49,
            "strategy_details": {
                "raw": 0.98,
                "normalized": 0.98,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.65,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 746,
        "original_data": {
            "instruction": "Imagine you're seeing colors for the first time. Explain what you see and how you feel.",
            "input": "",
            "output": "I honestly never thought I would experience colors in my lifetime. I'm overwhelmed with the amount of new colors I see. I now see more than just two different options when viewing the world. Although I'm aware, It's still unclear to me how to match the colors I see to their proper names. It will be very helpful to match what I'm seeing with their proper names. What a wonderful day; I'm wiping the tears from my eyes in pure joy!",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2808897495269775,
            "normalized": 0.7945399284362793
        },
        "potential_entropy": {
            "strategy_component": 0.47000000000000003,
            "strategy_details": {
                "raw": 0.9400000000000001,
                "normalized": 0.9400000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 747,
        "original_data": {
            "instruction": "What is Spot rate in finance?",
            "input": "",
            "output": "This is the exchange rate for foreign−exchange transactions which are being done straightaway.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.5,
            "s_out_dens": 0.6,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.5,
            "s_out_dens": 0.4,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.80867862701416,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.04,
            "strategy_details": {
                "raw": 2.08,
                "normalized": 2.08,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.5,
                    "out_dens": 0.4,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 748,
        "original_data": {
            "instruction": "Why is soccer so popular?",
            "input": "",
            "output": "Soccer, or football, is played in every corner of the globe, from world-class pitches to back streets. All one needs to play is a round ball and some markers to signify the goals. The same cannot be said for other sports which commonly require specialized equipment or venues. Due to this accessibility, the world has seen many great footballing talents come from diverse socio-economic backgrounds and nations. \n\nIt is a team-based sport that is incredibly unpredictable. Ironically, underdogs have frequently overcome defying odds to win. It is a beautiful sport to watch as moments of individual skill or team play can completely change the outcome of a game. \n\nFor these reasons, among many others, is why football is the world's most popular sport and is commonly referred to as \"the beautiful game.\"",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.2020859718322754,
            "normalized": 0.7720245633806501
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 749,
        "original_data": {
            "instruction": "Who is David Oliver Siegmund",
            "input": "David Oliver Siegmund (born November 15, 1941) is an American statistician who has worked extensively on sequential analysis.\n\nBiography\nSiegmund grew up in Webster Groves, Missouri. He received his baccalaureate degree, in mathematics, from Southern Methodist University in 1963, and a doctorate in statistics from Columbia University in 1966. His Ph.D. advisor was Herbert Robbins. After being an assistant and then a full professor at Columbia, he went to Stanford University in 1976, where he is currently a professor of statistics. He has served twice as the chair of Stanford's statistics department. He has also held visiting positions at Hebrew University of Jerusalem, the University of Zurich, the University of Oxford, and the University of Cambridge.",
            "output": "David Oliver Siegmund (born November 15, 1941) is an American statistician who has worked extensively on sequential analysis.\n\nSiegmund grew up in Webster Groves, Missouri. He received his baccalaureate degree, in mathematics, from Southern Methodist University in 1963, and a doctorate in statistics from Columbia University in 1966. His Ph.D. advisor was Herbert Robbins. After being an assistant and then a full professor at Columbia, he went to Stanford University in 1976, where he is currently a professor of statistics. \n\nHe has served twice as the chair of Stanford's statistics department. He has also held visiting positions at Hebrew University of Jerusalem, the University of Zurich, the University of Oxford, and the University of Cambridge.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.1714184284210205,
            "normalized": 0.47754812240600586
        },
        "potential_entropy": {
            "strategy_component": 0.39499999999999996,
            "strategy_details": {
                "raw": 0.7899999999999999,
                "normalized": 0.7899999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 750,
        "original_data": {
            "instruction": "What is multipass distillation?",
            "input": "",
            "output": "Multipass distillation is taking water and distilled it hundreds of times per gallon made by ultra heating and cooling the water many times per minute.  This action itself will change the bond angle of water from 104 degrees which is traditional tap water to 114 degrees.  This bond angle change is a permanent change to the produced multi-pass distilled water.  It has properties that can heal the body similar to stream water coming down a mountain which usually has a bond angle around 108 degrees.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.5,
            "s_out_div": 0.4,
            "s_out_dens": 0.6,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.5,
            "s_out_div": 0.6,
            "s_out_dens": 0.4,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.8272764682769775,
            "normalized": 0.9506504195077079
        },
        "potential_entropy": {
            "strategy_component": 1.2,
            "strategy_details": {
                "raw": 2.4,
                "normalized": 2.4,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.7,
                    "out_cot": 0.5,
                    "out_div": 0.6,
                    "out_dens": 0.4,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 751,
        "original_data": {
            "instruction": "Based on this text, would it be correct to say a peanut is a nut?",
            "input": "The peanut (Arachis hypogaea), also known as the groundnut, goober, pindar or monkey nut (UK), is a legume crop grown mainly for its edible seeds. It is widely grown in the tropics and subtropics, important to both small and large commercial producers. It is classified as both a grain legume and, due to its high oil content, an oil crop. World annual production of shelled peanuts was 44 million tonnes in 2016, led by China with 38% of the world total. Atypically among legume crop plants, peanut pods develop underground (geocarpy) rather than above ground. With this characteristic in mind, the botanist Carl Linnaeus gave peanuts the specific epithet hypogaea, which means \"under the earth\".\n\nThe peanut belongs to the botanical family Fabaceae (or Leguminosae), commonly known as the legume, bean, or pea family. Like most other legumes, peanuts harbor symbiotic nitrogen-fixing bacteria in root nodules. The capacity to fix nitrogen means peanuts require less nitrogen-containing fertilizer and improve soil fertility, making them valuable in crop rotations.\n\nPeanuts are similar in taste and nutritional profile to tree nuts such as walnuts and almonds, and, as a culinary nut, are often served in similar ways in Western cuisines. The botanical definition of a nut is \"a fruit whose ovary wall becomes hard at maturity.\" Using this criterion, the peanut is not a nut. However, peanuts are usually categorized as nuts for culinary purposes and in common English more generally.",
            "output": "This article describes different ways to classify a peanut. In a culinary or colloquial regard, yes it can be considered a nut. In the botanical or biological definition, it is not a nut but rather a legume.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.241478681564331,
            "normalized": 0.7832796233040946
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 752,
        "original_data": {
            "instruction": "What conference does the Spokane Braves belong to?",
            "input": "The Spokane Braves are a Junior \"B\" Ice Hockey team based in Spokane, Washington, United States. They are members of the Neil Murdoch Division of the Kootenay Conference of the Kootenay International Junior Hockey League (KIJHL). They play their home games at Eagles Ice-A-Rena. The Braves are the only team in the KIJHL to play in the United States. They were forced to go on hiatus at the beginning of the 20/21 season due to the US/Canada border closure.",
            "output": "The Spokane Braves are members of the Kootenay Conference of the Kootenay International Junior Hockey League (KIJHL).",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.0989553928375244,
            "normalized": 0.7425586836678642
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 753,
        "original_data": {
            "instruction": "How do I turn on my Tesla Y?",
            "input": "",
            "output": "When you open a door to enter Model Y, the touchscreen powers on and you can operate all controls. To drive Model Y: Press the brake pedal - Model Y powers on and is ready to drive.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.270127296447754,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 754,
        "original_data": {
            "instruction": "How to optimize space usage in garage",
            "input": "",
            "output": "Based on the size of the garage the most left over space you can use to anything else will be the space left after the cars are parked. And that space is usually by the wall. However; there is one interesting aspect that can be considered and that's the vertical space. Usually the space close to the ceiling. One of the things you can do is look for study heavy duty hooks that can be put high up on the garage walls. These hooks than can be used to hold Bicycles, Ladders, Backyard equipments and several other things. Get creative !!",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.84533953666687,
            "normalized": 0.9558112961905343
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 755,
        "original_data": {
            "instruction": "What is dhollywood?",
            "input": "Gujarati cinema, also known as Dhollywood, is the Gujarati language film industry. It is one of the major regional and vernacular film industries of the cinema of India, having produced more than one thousand films since its inception.",
            "output": "Dhollywood is a nickname for movies made by Gujarati cinema in India.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.5,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.5,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.635406017303467,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6575,
            "strategy_details": {
                "raw": 1.315,
                "normalized": 1.315,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.5,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 756,
        "original_data": {
            "instruction": "From the passage provided, extract the movie that did well across the globe but received underwhelming traction in Matt Damon's home country.",
            "input": "In 2014, he starred in George Clooney's The Monuments Men, and played the minor role of scientist Dr. Mann, in Christopher Nolan's Interstellar. That same year, Damon appeared as a celebrity correspondent for Years of Living Dangerously In 2015, Damon portrayed the main character, astronaut Mark Watney, in Ridley Scott's The Martian, based on Andy Weir's best-selling novel of the same name, a role that earned him the Golden Globe Award for Best Actor – Motion Picture Musical or Comedy and his second Academy Award nomination for Best Actor. Having not returned for the fourth film in the Bourne film series, Damon reprised his role in 2016's Jason Bourne, reuniting with Paul Greengrass. In 2017, Damon played the lead role in Zhang Yimou's The Great Wall, a hit internationally and a disappointment at the domestic box office. The film, and Damon's casting, were not well received by critics. Later in 2017, he starred in two satires, George Clooney's 1950s-set Suburbicon, which was released in October, and Alexander Payne's comedy Downsizing, which was released in December In 2019, Damon portrayed Carroll Shelby in the action biographical drama Ford v Ferrari, directed by James Mangold.",
            "output": "Zhang Yimou's The Great Wall performed well globally but did not meet domestic box office expectations.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.485102653503418,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 757,
        "original_data": {
            "instruction": "Classify the following people as primarily authors or primarily movie directors: Steven King, Steven Spielberg, N. K. Jemisin, Martin Scorsese, Jane Austen",
            "input": "",
            "output": "Steven King, N. K. Jemisin, and Jane Austen are authors. Steven Spielberg and Martin Scorsese are movie directors.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.132625102996826,
            "normalized": 0.752178600856236
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 758,
        "original_data": {
            "instruction": "Where is Hawaii?",
            "input": "Hawaii (/həˈwaɪi/ (listen) hə-WY-ee; Hawaiian: Hawaiʻi [həˈvɐjʔi] or [həˈwɐjʔi]) is a state in the Western United States, about 2,000 miles (3,200 km) from the U.S. mainland in the Pacific Ocean. It is the only U.S. state outside North America, the only state that is an archipelago, and the only state in the tropics.",
            "output": "Hawaii is located in the Pacific Ocean, about two thousand miles from the west coast of North America",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.7932910919189453,
            "normalized": 0.6552260262625558
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 759,
        "original_data": {
            "instruction": "At what age should I get a shingles vaccine?",
            "input": "",
            "output": "The US CDC recommends adults at the age of 50 or older receive a singles vaccine, but adults at the age of 19 or older are also recommended to get one if they have a weakened immune system.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.656291961669922,
            "normalized": 0.6160834176199776
        },
        "potential_entropy": {
            "strategy_component": 0.99,
            "strategy_details": {
                "raw": 1.98,
                "normalized": 1.98,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 760,
        "original_data": {
            "instruction": "Tell me an idea for a film based off of the cartoon movie Cars but incorporate self-driving cars.",
            "input": "",
            "output": "A self-driving car named Walter (voiced by Will Arnett) is stuck driving drunk passengers around San Francisco at all hours of the night trying to make ends meet. He frequently has pedestrians staring at him because he doesn't have a driver, kids drawing graffiti on him, and passengers giving him a bad review for being too cautious while driving.\n\nWalter dreams of being a race car like the rich cars he sees on Tik Tok. They get to drive fast as Sonoma Raceway and race each other in front of stadiums full of people while Walter has to follow the 30mph speed limit on SF streets.\n\nWalter finds out that the race car life isn't all it's cracked up to be, and realizes over time to be happy with what he has and that at least he isn't one of those ride-share vehicles that can't drive themselves around town.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.55,
            "s_out_cot": 0.7,
            "s_out_div": 0.65,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.44999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.35,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.7640113830566406,
            "normalized": 0.9325746808733258
        },
        "potential_entropy": {
            "strategy_component": 0.8074999999999999,
            "strategy_details": {
                "raw": 1.6149999999999998,
                "normalized": 1.6149999999999998,
                "components": {
                    "instruction": 0.3,
                    "input": 0.85,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.44999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.35,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 761,
        "original_data": {
            "instruction": "What is the nickname for West Ham United?",
            "input": "",
            "output": "West Ham United, a UK Premier League soccer team based in East London, are known as both the Irons and the Hammers. Both names refer to the origins of the club as the team of the Thames Ironworks. The club's badge includes a pair of crossed hammers. Whilst many know West Ham as the Hammers most fans tend to refer to them as the Irons particularly in the chant \"Come on You Irons\".",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 3.109097957611084,
            "normalized": 0.7454565593174526
        },
        "potential_entropy": {
            "strategy_component": 1.2025,
            "strategy_details": {
                "raw": 2.405,
                "normalized": 2.405,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 762,
        "original_data": {
            "instruction": "Who are some of the best guitarists of all time?",
            "input": "",
            "output": "Jimi Hendrix\nDuane Allman\nB.B. King\nEric Clapton\nRobert Johnson\nChuck Berry\nStevie Ray Vaughan\nRy Cooder\nJimmy Page\nKeith Richards\nKirk Hammett\nKurt Cobain\nJerry Garcia\nJeff Beck\nCarlos Santana\nJohnny Ramone\nJack White\nJohn Frusciante\nRichard Thompson\nJames Burton\nGeorge Harrison\nMike Bloomfield\nWarren Haynes\nThe Edge\nFreddie King\nTom Morello\nMark Knopfler\nStephen Stills\nRon Asheton\nBuddy Guy\nDick Dale\nJohn Cipollina\nLee Ranaldo\nThurston Moore\nJohn Fahey\nSteve Cropper\nBo Diddley\nPeter Green\nBrian May\nJohn Fogerty\nClarence White\nRobert Fripp\nEddie Hazel\nScotty Moore\nFrank Zappa\nLes Paul\nT-Bone Walker\nJoe Perry\nJohn McLaughlin\nPete Townshend\nPaul Kossoff\nLou Reed\nMickey Baker\nJorma Kaukonen\nRitchie Blackmore\nTom Verlaine\nRoy Buchanan\nDickey Betts\nJonny Greenwood\nEd O'Brien\nIke Turner\nZoot Horn Rollo\nDanny Gatton\nMick Ronson\nHubert Sumlin\nVernon Reid\nLink Wray\nJerry Miller\nSteve Howe\nEddie Van Halen\nLightnin' Hopkins\nJoni Mitchell\nTrey Anastasio\nJohnny Winter\nAdam Jones\nAli Farka Toure\nHenry Vestine\nRobbie Robertson\nCliff Gallup\nRobert Quine\nDerek Trucks\nDavid Gilmour\nNeil Young\nEddie Cochran\nRandy Rhoads\nTony Iommi\nJoan Jett\nDave Davies\nD Boon\nGlen Buxton\nRobby Krieger\nFred \"Sonic\" Smith\nWayne Kramer\nBert Jansch\nKevin Shields\nAngus Young\nRobert Randolph\nLeigh Stephens\nGreg Ginn\nKim Thayil",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.6,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.2,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.4,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.8,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            0
        ],
        "entropy": {
            "raw": 3.1086084842681885,
            "normalized": 0.745316709790911
        },
        "potential_entropy": {
            "strategy_component": 1.4425000000000001,
            "strategy_details": {
                "raw": 2.8850000000000002,
                "normalized": 2.8850000000000002,
                "components": {
                    "instruction": 0.48,
                    "input": 1.85,
                    "output": 0.555
                },
                "gaps": {
                    "ins_tone": 0.4,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.8,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 763,
        "original_data": {
            "instruction": "Classify each of the following as a planet or a star: Neptune, Saturn, Pollux, Sun, Jupiter, Mercury",
            "input": "",
            "output": "Neptune: Planet\nSaturn: Planet\nPollux: Star\nSun: Star\nJupiter: Planet\nMercury: Planet",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.50301456451416,
            "normalized": 0.5722898755754743
        },
        "potential_entropy": {
            "strategy_component": 0.5075000000000001,
            "strategy_details": {
                "raw": 1.0150000000000001,
                "normalized": 1.0150000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 764,
        "original_data": {
            "instruction": "What were all the earlier names for the city presently known as Chennai?",
            "input": "Chennai (/ˈtʃɛnaɪ/ (listen), Tamil: [ˈt͡ɕenːaɪ̯]), formerly known as Madras,[A] is the capital city of Tamil Nadu, the southernmost Indian state. It is the state's primate city both in area and population and is located on the Coromandel Coast of the Bay of Bengal. According to the 2011 Indian census, Chennai is the sixth-most populous city in India and forms the fourth-most populous urban agglomeration. The Greater Chennai Corporation is the civic body responsible for the city; it is the oldest city corporation of India, established in 1688—the second oldest in the world after London.\n\nHistorically, Chennai and surroundings were part of the Chola, Pandya, Pallava and Vijayanagara kingdoms for many centuries. The coastal land, which then contained the fishing village Madrasapattinam, was purchased by the British East India Company from the Nayak ruler Damarla Chennapa Nayaka, in the 17th century. The British garrison established the Madras city and port, and built Fort St. George—the first British fortress in India—which the French won over briefly in 1746, before becoming the winter capital of the Madras Presidency, a colonial province of the British Raj in the Indian subcontinent. After India gained its independence in 1947, Madras continued as the capital city of the Madras State and present-day Tamil Nadu. The city was officially renamed as Chennai in 1996.\n\nThe city is coterminous with Chennai district, which together with the adjoining suburbs constitutes the Chennai Metropolitan Area,[note 1] the 36th-largest urban area in the world by population and one of the largest metropolitan economies of India. As the traditional and de facto gateway of South India, Chennai is among the most-visited Indian cities by foreigners. It was ranked the 43rd most-visited city in the world in 2015 and 36th in 2019. The Quality of Living Survey rates it as India's safest city. Chennai attracts 45 percent of health tourists visiting India, and 30 to 40 percent of domestic health tourists. As such, it is termed \"India's health capital\". Chennai has the fifth-largest urban economy, and had the third-largest expatriate population in India.",
            "output": "Chennai traces its roots back to a fishing village called Madrasapattinam. The British officially named it Madras when they purchased the land in the 17th century. The name of the city was changed to Chennai in 1996.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8102476596832275,
            "normalized": 0.6600707599094936
        },
        "potential_entropy": {
            "strategy_component": 0.355,
            "strategy_details": {
                "raw": 0.71,
                "normalized": 0.71,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 765,
        "original_data": {
            "instruction": "Tell me whether the following animals are part of the cat family: ocelot, lion, jackal, caracal, leopard, fox, tiger, lynx.",
            "input": "",
            "output": "Ocelot, lion, caracal, leopard, tiger, lynx are all species that are part of the Felidae family (commonly known as cats). Jackals and foxes are part of the Canidae (or dog) family. While the two groups have physical similarities, they also have distinctive physical and behavioral differences.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.699824333190918,
            "normalized": 0.628521238054548
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 766,
        "original_data": {
            "instruction": "Extract which animals Waddell played with and which ones he fought with from the text. Separate them with a comma, putting the ones he played with first.",
            "input": "Waddell's career wound through a number of teams. He was notably unpredictable; early in his career, he once left in the middle of a game to go fishing. He also had a longstanding fascination with fire trucks and ran off the field to chase after them during games on multiple occasions. He would disappear for months at a time during the offseason, and it was not known where he went until it was discovered that he was wrestling alligators in a circus. He was easily distracted by opposing fans who held up puppies, which caused him to run over to play with them, and shiny objects, which seemed to put him in a trance. An alcoholic for much of his short life, he reportedly spent his entire first signing bonus on a drinking binge; as a pun of the baseball term \"southpaw\" denoting a left-handed pitcher, the Sporting News dubbed him a \"sousepaw\". His eccentric behavior led to constant battles with his managers and scuffles with bad-tempered teammates.",
            "output": "puppies, alligators",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.88,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.92,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.12,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.07999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 6.20442533493042,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.242,
            "strategy_details": {
                "raw": 0.484,
                "normalized": 0.484,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.21999999999999997,
                    "output": 0.204
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.12,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.07999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 767,
        "original_data": {
            "instruction": "Identify which animal species is alive or extinct: Cave Lion, Sumatran Elephant",
            "input": "",
            "output": "Sumatran Elephant is alive, Cave Lion is extinct.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.4,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.6,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.653027057647705,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.765,
            "strategy_details": {
                "raw": 1.53,
                "normalized": 1.53,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.9,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.6,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 768,
        "original_data": {
            "instruction": "From the passage, extract the list of movies that Pedro Almodovar produced. Include the year and the name. Place them in a bullet list.",
            "input": "Pedro Almodóvar Caballero (Spanish pronunciation: [ˈpeðɾo almoˈðoβaɾ kaβaˈʝeɾo]; born 25 September 1949) is a Spanish filmmaker. His films are marked by melodrama, irreverent humour, bold colour, glossy décor, quotations from popular culture, and complex narratives. Desire, passion, family, and identity are among Almodóvar's most prevalent subjects in his films. Acclaimed as one of the most internationally successful Spanish filmmakers, Almodóvar and his films have gained worldwide interest and developed a cult following.\n\nAlmodóvar's career came to during La Movida Madrileña, a cultural renaissance that followed after the end of Francoist Spain. His early films characterised the sense of sexual and political freedom of the period. In 1986, he established his own film production company, El Deseo, with his younger brother Agustín Almodóvar, who has been responsible for producing all of his films since Law of Desire (1987). His breakthrough film was Women on the Verge of a Nervous Breakdown (1988), which was nominated for the Academy Award for Best Foreign Language Film.\n\nHe achieved further success often collaborating with actors Antonio Banderas and Penélope Cruz. He directed Tie Me Up! Tie Me Down! (1989), High Heels (1991), and Live Flesh (1997). His next two films, All About My Mother (1999) and Talk to Her (2002), earned him an Academy Award each for Best International Feature Film and Best Original Screenplay, respectively. His later films include Volver (2006), Broken Embraces (2009), The Skin I Live In (2011), Julieta (2016), Pain and Glory (2019), and Parallel Mothers (2021).\n\nAlmodóvar has received numerous accolades including two Academy Awards, five BAFTA Awards, two Emmy Awards, two Golden Globe Awards, nine Goya Awards. He has also received the French Legion of Honour in 1997, the Gold Medal of Merit in the Fine Arts in 1999, and the European Film Academy Achievement in World Cinema Award in 2013 and was awarded the Golden Lion in 2019. He's also received an honorary doctoral degrees from Harvard University in 2009 and from University of Oxford in 2016.",
            "output": "1982 - Labyrinth of Passion\n1988 - Women on the Verge of a Nervous Breakdown\n2004 - Bad Education\n2021 - Parallel Mothers",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.0304903984069824,
            "normalized": 0.7229972566877093
        },
        "potential_entropy": {
            "strategy_component": 0.3875,
            "strategy_details": {
                "raw": 0.775,
                "normalized": 0.775,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 769,
        "original_data": {
            "instruction": "Is double checked locking safe?",
            "input": "In software engineering, double-checked locking (also known as \"double-checked locking optimization\") is a software design pattern used to reduce the overhead of acquiring a lock by testing the locking criterion (the \"lock hint\") before acquiring the lock. Locking occurs only if the locking criterion check indicates that locking is required.\n\nThe pattern, when implemented in some language/hardware combinations, can be unsafe. At times, it can be considered an anti-pattern.\n\nIt is typically used to reduce locking overhead when implementing \"lazy initialization\" in a multi-threaded environment, especially as part of the Singleton pattern. Lazy initialization avoids initializing a value until the first time it is accessed.",
            "output": "Double checked locking can be unsafe in some situations and is sometimes considered an anti-pattern. However, it can be implemented in a safe way.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5904643535614014,
            "normalized": 0.8829898153032575
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 770,
        "original_data": {
            "instruction": "From the passage provided, extract the year Apache Software Foundation was incorporated on.",
            "input": "The Apache Software Foundation /əˈpætʃi/ (ASF) is an American nonprofit corporation (classified as a 501(c)(3) organization in the United States) to support a number of open source software projects. The ASF was formed from a group of developers of the Apache HTTP Server, and incorporated on March 25, 1999. As of 2021, it includes approximately 1000 members.\n\nThe Apache Software Foundation is a decentralized open source community of developers. The software they produce is distributed under the terms of the Apache License, a permissive open-source license for free and open-source software (FOSS). The Apache projects are characterized by a collaborative, consensus-based development process and an open and pragmatic software license, which is to say that it allows developers who receive the software freely, to re-distribute it under nonfree terms. Each project is managed by a self-selected team of technical experts who are active contributors to the project. The ASF is a meritocracy, implying that membership of the foundation is granted only to volunteers who have actively contributed to Apache projects. The ASF is considered a second generation open-source organization,[citation needed] in that commercial support is provided without the risk of platform lock-in.\n\nAmong the ASF's objectives are: to provide legal protection to volunteers working on Apache projects; to prevent the Apache brand name from being used by other organizations without permission.\n\nThe ASF also holds several ApacheCon conferences each year, highlighting Apache projects and related technology.",
            "output": "Apache Software Foundation was incorporated on March 25, 1999",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.525848388671875,
            "normalized": 0.5788138253348214
        },
        "potential_entropy": {
            "strategy_component": 0.38,
            "strategy_details": {
                "raw": 0.76,
                "normalized": 0.76,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.4,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 771,
        "original_data": {
            "instruction": "Based on this summary of Lebron James career, what are the key reasons he is considered the greatest basketball player of all time?",
            "input": "LeBron Raymone James Sr. (/ləˈbrɒn/; born December 30, 1984) is an American professional basketball player for the Los Angeles Lakers in the National Basketball Association (NBA). Nicknamed \"King James\", he is considered to be one of the greatest basketball players in history and is often compared to Michael Jordan in debates over the greatest basketball player of all time. James is the all-time leading scorer in NBA history and ranks fourth in career assists. He has won four NBA championships (two with the Miami Heat, one each with the Lakers and Cleveland Cavaliers), and has competed in 10 NBA Finals. He has four MVP awards, four Finals MVP awards, and two Olympic gold medals. He has been named an All-Star 19 times, selected to the All-NBA Team 18 times (including 13 First Team selections) and the All-Defensive Team six times, and was a runner-up for the NBA Defensive Player of the Year Award twice in his career.\n\nJames grew up playing basketball for St. Vincent–St. Mary High School in his hometown of Akron, Ohio. He was heavily touted by the national media as a future NBA superstar. A prep-to-pro, he was selected by the Cleveland Cavaliers with the first overall pick of the 2003 NBA draft. Named the 2004 NBA Rookie of the Year, he soon established himself as one of the league's premier players, leading the Cavaliers to their first NBA Finals appearance in 2007 and winning the NBA MVP award in 2009 and 2010. After failing to win a championship with Cleveland, James left in 2010 as a free agent to join the Miami Heat; this was announced in a nationally televised special titled The Decision and is among the most controversial free agency moves in sports history.\n\nJames won his first two NBA championships while playing for the Heat in 2012 and 2013; in both of these years, he also earned the league's MVP and Finals MVP awards. After his fourth season with the Heat in 2014, James opted out of his contract and re-signed with the Cavaliers. In 2016, he led the Cavaliers to victory over the Golden State Warriors in the Finals by coming back from a 3–1 deficit, delivering the team's first championship and ending the Cleveland sports curse. In 2018, James exercised his contract option to leave the Cavaliers and signed with the Lakers, where he won the 2020 NBA championship and his fourth Finals MVP. James is the first player in NBA history to accumulate $1 billion in earnings as an active player. On February 7, 2023, James surpassed Kareem Abdul-Jabbar to become the all-time leading scorer in NBA history.\n\nOff the court, James has accumulated more wealth and fame from numerous endorsement contracts. He has been featured in books, documentaries (including winning two Sports Emmy Awards as an executive producer), and television commercials. He has won 19 ESPY Awards, hosted Saturday Night Live, and starred in the sports film Space Jam: A New Legacy (2021). James has been a part-owner of Liverpool F.C. since 2011 and leads the LeBron James Family Foundation, which has opened an elementary school, housing complex, retail plaza, and medical center in Akron.",
            "output": "\"King\" James is considered one of basketball's all-time greats because of the dominance and longevity displayed over his career. Lebron has personal accolades for days, including: 4 MVPs, 4 Finals MVPs, 19 All-Star seasons, 1st Team All-NBA 13 times, All-Defensives Team 6 times, and he's still going. When you sum up Lebron's career totals, they are equaling mind-blowing, as the NBA's all-time leading scorer, he is also 4th in career assists. Lebron's greatest achievement on the court is commonly viewed as the time he led the Cleveland Cavaliers back to championship glory, coming back from a 3-1 deficit to the Golden State Warriors in 2016.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.88,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.82,
            "s_out_bg": 0.78
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.12,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.18000000000000005,
            "s_out_bg": 0.21999999999999997
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.9679195880889893,
            "normalized": 0.7051198823111398
        },
        "potential_entropy": {
            "strategy_component": 0.3275,
            "strategy_details": {
                "raw": 0.655,
                "normalized": 0.655,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.21999999999999997,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.12,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.18000000000000005,
                    "out_bg": 0.21999999999999997
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 772,
        "original_data": {
            "instruction": "How many years has it been since the city of Calcutta was officially renamed Kolkata?",
            "input": "Kolkata also known as Calcutta (the official name until 2001) is the capital of the Indian state of West Bengal, on the eastern bank of the Hooghly River 80 km (50 mi) west of the border with Bangladesh. It is the primary business, commercial, and financial hub of Eastern India and the main port of communication for North-East India. According to the 2011 Indian census, Kolkata is the seventh-most populous city in India, with a population of 45 lakh (4.5 million) residents within the city. It is a part of Kolkata Metropolitan Area (or known as Greater Kolkata) which has a population of over 1.41 crore (14.1 million) residents making it the third-most populous metropolitan area in India. In 2021, the Kolkata metropolitan area crossed 1.5 crore (15 million) registered voters. The Port of Kolkata is India's oldest operating port and its sole major riverine port. Kolkata is regarded as the cultural capital of India. Kolkata is the second largest Bengali-speaking city after Dhaka. It has the highest number of nobel laureates among all cities in India.",
            "output": "It has been 22 years since Calcutta was renamed Kolkata.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.8873469829559326,
            "normalized": 0.967813423701695
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 773,
        "original_data": {
            "instruction": "Who was the first actor to play the Joker on the Batman television series?",
            "input": "César Julio Romero Jr. (February 15, 1907 – January 1, 1994) was an American actor and activist. He was active in film, radio, and television for almost sixty years.\n\nHis wide range of screen roles included Latin lovers, historical figures in costume dramas, characters in light domestic comedies, and the Joker on the Batman television series, which was included in TV Guide's 2013 list of The 60 Nastiest Villains of All Time. He was the first actor to play the character.",
            "output": "César Julio Romero Jr. played was the first actor to play the Joker.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.120786666870117,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 774,
        "original_data": {
            "instruction": "Categorize each of the following as a grocery store or restaurant: Publix, Texas Roadhouse, Bonefish, Costco, Aldi, McDonalds, KFC, Whole Foods",
            "input": "",
            "output": "Publix: Grocery Store\nTexas Roadhouse: Restaurant\nBonefish: Restaurant\nCostco: Grocery Store\nAldi: Grocery Store\nMcDonalds: Restaurant\nKFC: Restaurant\nWhole Foods: Grocery Store",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 2.537637948989868,
            "normalized": 0.5821822711399623
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.5,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 775,
        "original_data": {
            "instruction": "Classify each of the following as water-soluble vitamins and fat-soluble vitamins and which food come under them?",
            "input": "",
            "output": "Classifying each of the following as water-soluble vitamins and fat-soluble vitamins:\n\nbefore getting into that we will first know about them in a detailed way\n\nwater-soluble vitamins:\ndissolve in water and are easily absorbed into the bloodstream. They are not stored in the body for long periods of time and any excess amounts are excreted in the urine, which means that a regular intake is needed to maintain optimal levels. \n\nWater-soluble vitamins include:\n\nVitamin C: an antioxidant that helps protect cells from damage and supports immune function. It also plays a role in collagen synthesis and helps the body absorb iron from plant-based foods.\n\nVitamins B: There are eight B vitamins, including thiamin (B1), riboflavin (B2), niacin (B3), pantothenic acid (B5), pyridoxine (B6), biotin (B7), folate (B9), and cobalamin (B12). B vitamins play a crucial role in energy metabolism, as well as the production of red blood cells, brain function, and the maintenance of healthy skin, hair, and eyesight.\n\nFat-soluble vitamins:\n\nsoluble in fat and are absorbed with dietary fat in the small intestine. These vitamins are stored in the liver and fatty tissue for long periods of time, and excess amounts can accumulate and potentially cause toxicity. The fat-soluble vitamins include:\n\nVitamin A: essential for vision, skin health, and immune function. It is also important for the growth and development of bones and teeth.\n\nVitamin D: important for bone health as it helps the body absorb calcium. It also plays a role in immune function and may have other health benefits such as reducing the risk of certain types of cancer and autoimmune diseases.\n\nVitamin E: an antioxidant that helps protect cells from damage and supports immune function. It may also have a role in reducing the risk of heart disease.\n\nVitamin K: important for blood clotting and bone health. It also plays a role in regulating calcium levels in the body.\n\nFood that comes under water-soluble vitamins:\n\nVitamin A: found in liver, fish oil, eggs, milk, cheese, and dark leafy greens like spinach and kale.\n\nVitamin D: found in fatty fish such as salmon, mackerel, and tuna, as well as egg yolks and fortified foods like milk, orange juice, and cereals.\n\nVitamin E: found in nuts and seeds like almonds, sunflower seeds, and hazelnuts, as well as vegetable oils like wheat germ oil, sunflower oil, and safflower oil.\n\nVitamin K: found in leafy greens like spinach, kale, and collard greens, as well as cruciferous vegetables like broccoli and Brussels sprouts, and vegetable oils like soybean oil and canola oil.\n\nFood that comes under Fat -soluble vitamins:\nVitamin C: found in citrus fruits like oranges, grapefruits, and lemons, as well as kiwi, strawberries, bell peppers, broccoli, and spinach.\n\nThiamin (B1): found in whole grains like brown rice, whole wheat bread, and oats, as well as pork, legumes, and nuts.\n\nRiboflavin (B2): found in dairy products like milk and yogurt, as well as eggs, meat, leafy greens, and whole grains.\n\nNiacin (B3): found in meat, poultry, fish, whole grains, and legumes.\n\nPantothenic acid (B5): found in meat, poultry, fish, whole grains, and legumes.\n\nPyridoxine (B6): found in meat, poultry, fish, bananas, whole grains, and legumes.\n\nBiotin (B7): found in egg yolks, liver, nuts, and seeds.\n\nFolate (B9): found in leafy greens, legumes, citrus fruits, and fortified grains and cereals.\n\nCobalamin (B12): found in animal products like meat, poultry, fish, eggs, and dairy.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 1.1538746356964111,
            "normalized": 0.18682132448468888
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 776,
        "original_data": {
            "instruction": "What is a herbivore?",
            "input": "A herbivore is an animal anatomically and physiologically adapted to eating plant material, for example foliage or marine algae, for the main component of its diet. As a result of their plant diet, herbivorous animals typically have mouthparts adapted to rasping or grinding. Horses and other herbivores have wide flat teeth that are adapted to grinding grass, tree bark, and other tough plant material.",
            "output": "A herbivore is an animal anatomically and physiologically adapted to eating plant material, for example foliage or marine algae, for the main component of its diet.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.4206032752990723,
            "normalized": 0.8344580786568778
        },
        "potential_entropy": {
            "strategy_component": 0.4025,
            "strategy_details": {
                "raw": 0.805,
                "normalized": 0.805,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.4,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 777,
        "original_data": {
            "instruction": "Who is the greatest tennis player of all time?",
            "input": "",
            "output": "Being the greatest is a subjective question, based on the number of grand slams and the era in which they were played, Rod Laver is considered to be one of the greatest players of all time",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.276052236557007,
            "normalized": 0.7931577818734306
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 778,
        "original_data": {
            "instruction": "Name some popular video games exclusive to Sony’s Playstation game consoles.",
            "input": "",
            "output": "Some of the most popular games that are exclusive to Sony Playstation consoles include the Uncharted series, the Gran Turismo series, and the God of War series. Each series of games have sold tens of millions of copies.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.6836836338043213,
            "normalized": 0.6239096096583775
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 779,
        "original_data": {
            "instruction": "What is the name of the season that begins in the fall and ends in the spring?",
            "input": "",
            "output": "Winter",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": NaN,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 780,
        "original_data": {
            "instruction": "In World Wrestling Championship 2022 Free Style Wrestling India won 2 bronze medals out of total how many medals",
            "input": "",
            "output": "120 medals(24th rank in over all medal tally)",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.4,
            "s_out_dens": 0.65,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.6,
            "s_out_dens": 0.35,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.244154930114746,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.765,
            "strategy_details": {
                "raw": 1.53,
                "normalized": 1.53,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.9,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.6,
                    "out_dens": 0.35,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 781,
        "original_data": {
            "instruction": "From the passage identify the shape of bottom of chemistry test tube. Display the results in comma separated format.",
            "input": "Test tubes intended for general chemical work are usually made of glass, for its relative resistance to heat. Tubes made from expansion-resistant glasses, mostly borosilicate glass or fused quartz, can withstand high temperatures up to several hundred degrees Celsius.Chemistry tubes are available in a multitude of lengths and widths, typically from 10 to 20 mm wide and 50 to 200 mm long. The top often features a flared lip to aid pouring out the contents.A chemistry test tube typically has a flat bottom, a round bottom, or a conical bottom. Some test tubes are made to accept a ground glass stopper or a screw cap. They are often provided with a small ground glass or white glaze area near the top for labelling with a pencil.Test tubes are widely used by chemists to handle chemicals, especially for qualitative experiments and assays. Their spherical bottom and vertical sides reduce mass loss when pouring, make them easier to wash out, and allow convenient monitoring of the contents. The long, narrow neck of test tube slows down the spreading of gases to the environment.Test tubes are convenient containers for heating small amounts of liquids or solids with a Bunsen burner or alcohol burner. The tube is usually held by its neck with a clamp or tongs. By tilting the tube, the bottom can be heated to hundreds of degrees in the flame, while the neck remains relatively cool, possibly allowing vapours to condense on its walls. A boiling tube is a large test tube intended specifically for boiling liquids.A test tube filled with water and upturned into a water-filled beaker is often used to capture gases, e.g. in electrolysis demonstrations.A test tube with a stopper is often used for temporary storage of chemical or biological samples.",
            "output": "flat, round bottom, conical bottom",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.519035339355469,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 782,
        "original_data": {
            "instruction": "Who was Nike in Greek Mythology?",
            "input": "In Greek mythology, Nike (/ˈnaɪki/ (listen); Ancient Greek: Νίκη, lit. 'victory', ancient: [nǐː.kɛː], modern: [ˈni.ci]) was a goddess who personified victory in any field including art, music, war, and athletics. She is often portrayed in Greek art as Winged Victory in the motion of flight; however, she can also appear without wings as \"Wingless Victory\" when she is being portrayed as an attribute of another deity such as Athena. In Greek literature Nike is described as both an attribute and attendant to the gods Zeus and Athena. Nike gained this honored role beside Zeus during the Titanomachy where she was one of the first gods to offer her allegiance to Zeus. At Athens, Nike became a servant to Athena as well as an attribute of her due to the prominent status Athena held in her patron city. The fusion of the two goddesses at Athens has contributed to the ambiguity surrounding Nike's origins. It is unclear whether she originated from a character trait of the Greek goddess Athena or has always existed as an independent deity. Her origin story in Greek mythology is also slightly ambiguous, with the Theogony claiming Nike to be the daughter of Styx and Pallas while the Homeric Hymns describe Ares as being Nike's father. Her Roman equivalent was Victoria.",
            "output": "Nike is the god of 'victory' in Greek mythology, and is often portrayed in art Winged Victory in the motion of flight.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.041266441345215,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.46749999999999997,
            "strategy_details": {
                "raw": 0.9349999999999999,
                "normalized": 0.9349999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 783,
        "original_data": {
            "instruction": "Why are shiba inus the best dogs?",
            "input": "",
            "output": "Shiba inus are the best dogs because they are intuitive, smart, independent, affectionate (at times) and easy to train.  They are also the perfect size (moderately compact) and weighs on average 20-25 lbs, making them small enough to carry but sturdy enough to hug.  This makes them great for smaller-sized homes or apartments.  They tend to also be quite friendly to other humans and pets, so it's easy to take them to dog parks, on hikes and to beaches without having to worry about aggression towards humans or other dogs.  \n\nThe most important reason -- shiba inus are just unbelievably cute -- they look like foxes, with their prick ears, lush curly tail and thick double coats.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.446450710296631,
            "normalized": 0.8418430600847516
        },
        "potential_entropy": {
            "strategy_component": 0.9874999999999999,
            "strategy_details": {
                "raw": 1.9749999999999999,
                "normalized": 1.9749999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 784,
        "original_data": {
            "instruction": "Extract who developed Age of Empires 2 and the year it was released from the following text and separate them with a comma",
            "input": "Age of Empires II: The Age of Kings is a real-time strategy video game developed by Ensemble Studios and published by Microsoft. Released in 1999 for Microsoft Windows and Macintosh, it is the second game in the Age of Empires series. The Age of Kings is set in the Middle Ages and contains thirteen playable civilizations. Players aim to gather resources, which they use to build towns, create armies, and defeat their enemies. There are five historically based campaigns, which conscript the player to specialized and story-backed conditions, as well as three additional single-player game modes; multiplayer is also supported.",
            "output": "Ensemble Studios, 1999",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.9394636154174805,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.29500000000000004,
            "strategy_details": {
                "raw": 0.5900000000000001,
                "normalized": 0.5900000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 785,
        "original_data": {
            "instruction": "Based on the following passage where did plums, apricots and peaches originate?",
            "input": "Traditional dried fruit such as raisins, figs, dates, apricots and apples have been a staple of Mediterranean diets for millennia. This is due partly to their early cultivation in the Middle Eastern region known as the Fertile Crescent, made up by parts of modern Iran, Iraq, southwest Turkey, Syria, Lebanon, Palestine, Israel, and northern Egypt. Drying or dehydration also happened to be the earliest form of food preservation: grapes, dates, and figs that fell from the tree or vine would dry in the hot sun. Early hunter-gatherers observed that these fallen fruit took on an edible form, and valued them for their stability as well as their concentrated sweetness. \nThe earliest recorded mention of dried fruits can be found in Mesopotamian tablets dating to about 1500 BC, which contain what are probably the oldest known written recipes. These clay slabs, written in Akkadian, the daily language of Babylonia, were inscribed in cuneiform and tell of diets based on grains (barley, millet, wheat), vegetables and fruits such as dates, figs, apples, pomegranates, and grapes. These early civilizations used dates, date juice evaporated into syrup and raisins as sweeteners. They included dried fruits in their breads for which they had more than 300 recipes, from simple barley bread for the workers to very elaborate, spiced cakes with honey for the palaces and temples.\nThe date palm was one of the first cultivated trees. It was domesticated in Mesopotamia more than 5,000 years ago. It grew abundantly in the Fertile Crescent and it was so productive (an average date palm produces 50 kg (100 lbs) of fruit a year for 60 years or more) that dates were the cheapest of staple foods. Because they were so valuable, they were well recorded in Assyrian and Babylonian monuments and temples. The villagers in Mesopotamia dried them and ate them as sweets. Whether fresh, soft-dried or hard-dried, they helped to give character to meat dishes and grain pies. They were valued by travelers for their energy and were recommended as stimulants against fatigue.\nFigs were also prized in early Mesopotamia, Palestine, Israel, and Egypt where their daily use was probably greater than or equal to that of dates. As well as appearing in wall paintings, many specimens have been found in Egyptian tombs as funerary offerings. In Greece and Crete, figs grew very readily and they were the staple of poor and rich alike, particularly in their dried form.\nGrape cultivation first began in Armenia and the eastern regions of the Mediterranean in the 4th century BC. Raisins were produced by drying grapes in the hot desert sun. Very quickly, viticulture and raisin production spread across northern Africa including Morocco and Tunisia. The Phoenicians and the Egyptians popularized the production of raisins, probably due to the perfect arid environment for sun drying. They put them in jars for storage and allotted them to the different temples by the thousands. They also added them to breads and various pastries, some made with honey, some with milk and eggs.\nFrom the Middle East, these fruits spread through Greece to Italy where they became a major part of the diet. Ancient Romans consumed raisins in spectacular quantities and at all levels of society, including them as a key part of their common meals, along with olives and fresh fruits. Raisined breads were common for breakfast and were consumed with their grains, beans, and cultured milks. Raisins were so valued that they transcended the food realm and became rewards for successful athletes, as well as premium barter currency.\nHaving dried fruits was essential in ancient Rome as these instructions for housekeepers around 100 BC tell: \"She must keep a supply of cooked food on hand for you and the servants. She must keep many hens and have plenty of eggs. She must have a large store of dried pears, sorbs, figs, raisins, sorbs in must, preserved pears and grapes and quinces. She must also keep preserved grapes in grape-pulp and in pots buried in the ground, as well as fresh Praenestine nuts kept in the same way, and Scantian quinces in jars, and other fruits that are usually preserved, as well as wild fruits. All these she must store away diligently every year.\"\nFigs were also extremely popular in Rome. Dried figs were added to bread and formed a major part of the winter food of country people. They were rubbed with spices such as cumin, anise and fennel seeds, or toasted sesame, wrapped in fig leaves and stored in jars. Today, major producing regions include Israel, Jerusalem, Gaza and many other Arabic countries. Dried figs are rich in vitamins, phosphorus and various other important minerals.\nPlums, apricots and peaches had their origins in Asia. They were domesticated in China in the 3rd millennium BC and spread to the Fertile Crescent where they were also very popular, fresh and dried alike. They arrived in Greece and Italy much later and were very expensive but valued in the preparation of gourmet dishes with port or stewed with honey and spices.",
            "output": "Plums, apricots and peaches had their origins in Asia. They were domesticated in China in the 3rd millennium BC and spread to the Fertile Crescent where they were also very popular, fresh and dried alike. They arrived in Greece and Italy much later and were very expensive but valued in the preparation of gourmet dishes with port or stewed with honey and spices.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.5287539958953857,
            "normalized": 0.8653582845415387
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 786,
        "original_data": {
            "instruction": "Is Cartesian dualism correct ?",
            "input": "",
            "output": "This like many philosophical questions cannot be verified with the same certainty as we get through observation within the scientific method. \n\nThis is highly debated, however some may argue that it provides a simplistic separation of mind from world. Some argue for example that mind relies for meaning on a indissoluble involvement in the surrounding world and body.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 4.56651496887207,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6275,
            "strategy_details": {
                "raw": 1.255,
                "normalized": 1.255,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 787,
        "original_data": {
            "instruction": "Why does the capital of Uzbekistan contain the biggest Greek community?",
            "input": "The Greeks in Uzbekistan number approximately 9,000. The community is made up of Greeks from Russia who were deported by force from that country to Uzbekistan in the 1940s, and political refugees from Greece. About 30,000 Greeks lived in the country before World War II and a further 11,000 arrived after the Greek Civil War and the defeat of Democratic Army of Greece. Their numbers have dwindled from a high of some 40,000 in the 1960s. The main reason is emigration to Greece after the end of the Cold War when laws allowed the return of all ethnic Greeks who had been exiled for political reasons after the Greek Civil War. The biggest Greek community in the country is in the capital city of Tashkent where most of the Greek political refugees were relocated by the Soviet authorities. In ancient times the south of the country was part of the Hellenistic Seleucid Kingdom but the few Greek communities there have since assimilated.  The most important organization representing ethnic Greeks is the Greek Cultural Association of Tashkent. The activities of this organization include Greek language instruction (19 classes with a total of 571 students, classes for adults) and the teaching of Greek dances and music.",
            "output": "Most of the Greek political refugees in Uzbekistan were relocated by the Soviet authorities to the capital city of Tashkent.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.6066033840179443,
            "normalized": 0.8876009668622699
        },
        "potential_entropy": {
            "strategy_component": 0.47,
            "strategy_details": {
                "raw": 0.94,
                "normalized": 0.94,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 788,
        "original_data": {
            "instruction": "How do you find a college internship in your interested field of study?",
            "input": "",
            "output": "You can research open internship positions through many sources, including LinkedIn, Indeed, Handshake, company websites and college career websites.  Networking with alumni, friends, family and company employees is also another source for learning and applying to open positions. With each position receiving many applications, you need to identify a way to have your resume flagged for the recruiting manager.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.890246629714966,
            "normalized": 0.9686418942042759
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 789,
        "original_data": {
            "instruction": "What is a birdie in Golf",
            "input": "",
            "output": "Each hole in Golf is designated with the number of strokes it would take an expert to make to the hole. If the golfer makes it in the designated strokes it is called a Par. One stroke less than the designated will result in a Birdie. Example , for a Par 4 hole if a Golfer takes 3 strokes to get the ball in the hole , he has a Birdie.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.5077476501464844,
            "normalized": 0.8593564714704242
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 790,
        "original_data": {
            "instruction": "Who is Prince (musician)?",
            "input": "Prince Rogers Nelson (June 7, 1958 – April 21, 2016), commonly known mononymously as Prince, was an American singer, songwriter, musician, and record producer.",
            "output": "Prince Rogers Nelson (June 7, 1958 – April 21, 2016), commonly known mononymously as Prince, was an American singer, songwriter, musician, and record producer. The recipient of numerous awards and nominations, he is widely regarded as one of the greatest musicians of his generation.\nHe was known for his flamboyant, androgynous persona; his wide vocal range, which included a far-reaching falsetto and high-pitched screams; and his skill as a multi-instrumentalist, often preferring to play all or most of the instruments on his recordings.\nPrince produced his albums himself, pioneering the Minneapolis sound. His music incorporated a wide variety of styles, including funk, R&B, rock, new wave, soul, synth-pop, pop, jazz, and hip hop.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.31809401512146,
            "normalized": 0.5194554328918457
        },
        "potential_entropy": {
            "strategy_component": 0.295,
            "strategy_details": {
                "raw": 0.59,
                "normalized": 0.59,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 791,
        "original_data": {
            "instruction": "Which city is best in Australia",
            "input": "",
            "output": "There are many great cities in Australia. Which is best depends on what you like. Sydney is known for its beauty with some of Australia's most iconic landmarks such as the Sydney Harbour Bridge and Sydney Opera House. Sydney is also the largest city in Australia. \nMelbourne is knowns for its fun nightlife, culture, and sport, hosting events like the Australian F1 and the Australian Open tennis grand slam. \nAdelaide is known for its food, and wine, and for being a great place to live. And Hobart in Tasmania is surrounded by great food and beautiful surroundings and is increasingly regarded as one of the best places in Australia for families.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.7867116928100586,
            "normalized": 0.653346197945731
        },
        "potential_entropy": {
            "strategy_component": 0.9349999999999999,
            "strategy_details": {
                "raw": 1.8699999999999999,
                "normalized": 1.8699999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 792,
        "original_data": {
            "instruction": "What is electrostatics the study of?",
            "input": "The concept of the electric field was introduced by Michael Faraday. An electric field is created by a charged body in the space that surrounds it, and results in a force exerted on any other charges placed within the field. The electric field acts between two charges in a similar manner to the way that the gravitational field acts between two masses, and like it, extends towards infinity and shows an inverse square relationship with distance. However, there is an important difference. Gravity always acts in attraction, drawing two masses together, while the electric field can result in either attraction or repulsion. Since large bodies such as planets generally carry no net charge, the electric field at a distance is usually zero. Thus gravity is the dominant force at distance in the universe, despite being much weaker.\nAn electric field generally varies in space and its strength at any one point is defined as the force (per unit charge) that would be felt by a stationary, negligible charge if placed at that point. The conceptual charge, termed a 'test charge', must be vanishingly small to prevent its own electric field disturbing the main field and must also be stationary to prevent the effect of magnetic fields. As the electric field is defined in terms of force, and force is a vector, having both magnitude and direction, so it follows that an electric field is a vector field.\nThe study of electric fields created by stationary charges is called electrostatics. The field may be visualised by a set of imaginary lines whose direction at any point is the same as that of the field. This concept was introduced by Faraday, whose term 'lines of force' still sometimes sees use. The field lines are the paths that a point positive charge would seek to make as it was forced to move within the field; they are however an imaginary concept with no physical existence, and the field permeates all the intervening space between the lines. Field lines emanating from stationary charges have several key properties: first, that they originate at positive charges and terminate at negative charges; second, that they must enter any good conductor at right angles, and third, that they may never cross nor close in on themselves.",
            "output": "The study of electric fields created by stationary charges is called electrostatics.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.155405282974243,
            "normalized": 0.7586872237069267
        },
        "potential_entropy": {
            "strategy_component": 0.5349999999999999,
            "strategy_details": {
                "raw": 1.0699999999999998,
                "normalized": 1.0699999999999998,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 793,
        "original_data": {
            "instruction": "Which drum kit should I buy?",
            "input": "",
            "output": "To decide which drum kit to buy, ultimately you should consider where you are in your drumming journey, and where you see yourself going in the coming years. If you're just starting out then there are some excellent starter choices from most of the major brands. For example, Tama and Pearl.\nIf you're an intermediate drummer who is starting to play gigs and record, then buying a mid-range kit with solid hardware and a variety colour options & finishes could be a good match for you. \nIf you're a professional drummer who is playing many gigs, doing recording sessions etc then a reliable high end kit with a lot of range and tuning options will be a must have. Here you will consider options such as the DW Collectors series, the Tama Starclassic and Star series, Yamaha Recording Customs, Pearl Masters, Sonos SQ2's and Gretsch USA Customs.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.680689811706543,
            "normalized": 0.9087685176304409
        },
        "potential_entropy": {
            "strategy_component": 0.7124999999999999,
            "strategy_details": {
                "raw": 1.4249999999999998,
                "normalized": 1.4249999999999998,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 794,
        "original_data": {
            "instruction": "What was the name of the last team Pele played professionally?",
            "input": "Edson Arantes do Nascimento (Brazilian Portuguese: [ˈɛdsõ aˈɾɐ̃tʃiz du nasiˈmẽtu]; 23 October 1940 – 29 December 2022), better known by his nickname Pelé (Portuguese pronunciation: [peˈlɛ]), was a Brazilian professional footballer who played as a forward. Widely regarded as one of the greatest players of all time, he was among the most successful and popular sports figures of the 20th century. In 1999, he was named Athlete of the Century by the International Olympic Committee and was included in the Time list of the 100 most important people of the 20th century. In 2000, Pelé was voted World Player of the Century by the International Federation of Football History & Statistics (IFFHS) and was one of the two joint winners of the FIFA Player of the Century. His 1,279 goals in 1,363 games, which includes friendlies, is recognised as a Guinness World Record.\n\nPelé began playing for Santos at age 15 and the Brazil national team at 16. During his international career, he won three FIFA World Cups: 1958, 1962 and 1970, the only player to do so and the youngest player to win a World Cup (17). He was nicknamed O Rei (The King) following the 1958 tournament. Pelé is the joint-top goalscorer for Brazil with 77 goals in 92 games. At the club level, he was Santos's all-time top goalscorer with 643 goals in 659 games. In a golden era for Santos, he led the club to the 1962 and 1963 Copa Libertadores, and to the 1962 and 1963 Intercontinental Cup. Credited with connecting the phrase \"The Beautiful Game\" with football, Pelé's \"electrifying play and penchant for spectacular goals\" made him a star around the world, and his teams toured internationally to take full advantage of his popularity. During his playing days, Pelé was for a period the best-paid athlete in the world. After retiring in 1977, Pelé was a worldwide ambassador for football and made many acting and commercial ventures. In 2010, he was named the honorary president of the New York Cosmos.\n\nAveraging almost a goal per game throughout his career, Pelé was adept at striking the ball with either foot in addition to anticipating his opponents' movements on the field. While predominantly a striker, he could also drop deep and take on a playmaking role, providing assists with his vision and passing ability, and he would also use his dribbling skills to go past opponents. In Brazil, he was hailed as a national hero for his accomplishments in football and for his outspoken support of policies that improve the social conditions of the poor. His emergence at the 1958 World Cup, where he became a black global sporting star, was a source of inspiration. Throughout his career and in his retirement, Pelé received numerous individual and team awards for his performance on the field, his record-breaking achievements, and his legacy in the sport.\n\nEarly years\n\nBorn in Três Corações in Minas Gerais state in Brazil in 1940, Pelé has a street named after him in the city – Rua Edson Arantes do Nascimento. A statue of Pelé is also prominently placed in a plaza near the city's downtown.\nPelé was born Edson Arantes do Nascimento on 23 October 1940 in Três Corações, Minas Gerais, the son of Fluminense footballer Dondinho (born João Ramos do Nascimento) and Celeste Arantes. He was the elder of two siblings, with brother Zoca also playing for Santos, albeit not as successfully. He was named after the American inventor Thomas Edison. His parents decided to remove the \"i\" and call him \"Edson\", but there was a typo on his birth certificate, leading many documents to show his name as \"Edison\", not \"Edson\", as he was called. He was originally nicknamed \"Dico\" by his family. He received the nickname \"Pelé\" during his school days, when, it is claimed, he was given it because of his pronunciation of the name of his favourite player, local Vasco da Gama goalkeeper Bilé, which he misspoke, but the more he complained the more it stuck. In his autobiography released in 2006, Pelé stated he had no idea what the name means, nor did his old friends. Apart from the assertion that the name is derived from that of \"Bilé\", the word has no meaning in Portuguese.[note 2]\n\nPelé grew up in poverty in Bauru in the state of São Paulo. He earned extra money by working in tea shops as a servant. Taught to play by his father, he could not afford a proper football and usually played with either a sock stuffed with newspaper and tied with string or a grapefruit. He played for several amateur teams in his youth, including Sete de Setembro, Canto do Rio, São Paulinho, and Ameriquinha. Pelé led Bauru Atlético Clube juniors (coached by Waldemar de Brito) to two São Paulo state youth championships. In his mid-teens, he played for an indoor football team called Radium. Indoor football had just become popular in Bauru when Pelé began playing it. He was part of the first futsal (indoor football) competition in the region. Pelé and his team won the first championship and several others.\n\nAccording to Pelé, futsal (indoor football) presented difficult challenges: he said it was a lot quicker than football on the grass, and that players were required to think faster because everyone is close to each other in the pitch. Pelé credits futsal for helping him think better on the spot. In addition, futsal allowed him to play with adults when he was about 14 years old. In one of the tournaments he participated in, he was initially considered too young to play, but eventually went on to end up top scorer with 14 or 15 goals. \"That gave me a lot of confidence\", Pelé said, \"I knew then not to be afraid of whatever might come\".\n\nClub career\nSantos\nMain article: Os Santásticos\n1956–1962: Early years with Santos and declared a national treasure\n\nPelé in 1962, by then rated the best player in the world\nIn 1956, de Brito took Pelé to Santos, an industrial and port city located near São Paulo, to try out for professional club Santos FC, telling the club's directors that the 15-year-old would be \"the greatest football player in the world.\" Pelé impressed Santos coach Lula during his trial at the Estádio Vila Belmiro, and he signed a professional contract with the club in June 1956. Pelé was highly promoted in the local media as a future superstar. He made his senior team debut on 7 September 1956 at the age of 15 against Corinthians de Santo André and had an impressive performance in a 7–1 victory, scoring the first goal in his prolific career during the match.\n\nWhen the 1957 season started, Pelé was given a starting place in the first team and, at the age of 16, became the top scorer in the league. Ten months after signing professionally, the teenager was called up to the Brazil national team. After the 1958 and the 1962 World Cup, wealthy European clubs, such as Real Madrid, Juventus and Manchester United, tried to sign him in vain. In 1958, Inter Milan even managed to get him a regular contract, but Angelo Moratti was forced to tear the contract up at the request of Santos's chairman following a revolt by Santos's Brazilian fans. Valencia CF also arranged an agreement that would have brought Pelé to the club after the 1958 World Cup, however after his performances at the tournament Santos declined to let the player leave. In 1961 the government of Brazil under President Jânio Quadros declared Pelé an \"official national treasure\" to prevent him from being transferred out of the country.\n\nPelé won his first major title with Santos in 1958 as the team won the Campeonato Paulista; he would finish the tournament as the top scorer, with 58 goals, a record that still stands today. A year later, he would help the team earn their first victory in the Torneio Rio-São Paulo with a 3–0 over Vasco da Gama. However, Santos was unable to retain the Paulista title. In 1960, Pelé scored 33 goals to help his team regain the Campeonato Paulista trophy but lost out on the Rio-São Paulo tournament after finishing in 8th place. In the 1960 season, Pelé scored 47 goals and helped Santos regain the Campeonato Paulista. The club went on to win the Taça Brasil that same year, beating Bahia in the finals; Pelé finished as the top scorer of the tournament with nine goals. The victory allowed Santos to participate in the Copa Libertadores, the most prestigious club tournament in the Western hemisphere.\n\n1962–1965: Copa Libertadores success\n\"I arrived hoping to stop a great man, but I went away convinced I had been undone by someone who was not born on the same planet as the rest of us.\"\n\n—Benfica goalkeeper Costa Pereira following the loss to Santos in 1962.\nSantos's most successful Copa Libertadores season started in 1962; the team was seeded in Group One alongside Cerro Porteño and Deportivo Municipal Bolivia, winning every match of their group but one (a 1–1 away tie versus Cerro). Santos defeated Universidad Católica in the semi-finals and met defending champions Peñarol in the finals. Pelé scored twice in the playoff match to secure the first title for a Brazilian club. Pelé finished as the second top scorer of the competition with four goals. That same year, Santos would successfully defend the Campeonato Paulista (with 37 goals from Pelé) and the Taça Brasil (Pelé scoring four goals in the final series against Botafogo). Santos would also win the 1962 Intercontinental Cup against Benfica. Wearing his number 10 shirt, Pelé produced one of the best performances of his career, scoring a hat-trick in Lisbon as Santos won 5–2.\n\n\nPelé with Santos in the Netherlands, October 1962\nPelé states that his most memorable goal was scored at the Estádio Rua Javari on a Campeonato Paulista match against São Paulo rival Clube Atlético Juventus on 2 August 1959. As there is no video footage of this match, Pelé asked that a computer animation be made of this specific goal. In March 1961, Pelé scored the gol de placa (goal worthy of a plaque), against Fluminense at the Maracanã. Pelé received the ball on the edge of his own penalty area, and ran the length of the field, eluding opposition players with feints, before striking the ball beyond the goalkeeper. A plaque was commissioned with a dedication to \"the most beautiful goal in the history of the Maracanã\".\n\n\nPelé before facing Boca Juniors in the second leg of the 1963 Copa Libertadores Finals at La Bombonera.\nAs the defending champions, Santos qualified automatically to the semi-final stage of the 1963 Copa Libertadores. The balé branco (white ballet), the nickname given to Santos at the time, managed to retain the title after victories over Botafogo and Boca Juniors. Pelé helped Santos overcome a Botafogo team that featured Brazilian greats such as Garrincha and Jairzinho with a last-minute goal in the first leg of the semi-finals which made it 1–1. In the second leg, Pelé scored a hat-trick in the Estádio do Maracanã as Santos won, 0–4, in the second leg. Santos started the final series by winning, 3–2, in the first leg and defeating Boca Juniors 1–2, in La Bombonera. It was a rare feat in official competitions, with another goal from Pelé. Santos became the first Brazilian team to lift the Copa Libertadores in Argentine soil. Pelé finished the tournament with five goals. Santos lost the Campeonato Paulista after finishing in third place but went on to win the Rio-São Paulo tournament after a 0–3 win over Flamengo in the final, with Pelé scoring one goal. Pelé would also help Santos retain the Intercontinental Cup and the Taça Brasil against AC Milan and Bahia respectively.\n\nIn the 1964 Copa Libertadores, Santos was beaten in both legs of the semi-finals by Independiente. The club won the Campeonato Paulista, with Pelé netting 34 goals. Santos also shared the Rio-São Paulo title with Botafogo and won the Taça Brasil for the fourth consecutive year. In the 1965 Copa Libertadores, Santos reached the semi-finals and met Peñarol in a rematch of the 1962 final. After two matches, a playoff was needed to break the tie. Unlike 1962, Peñarol came out on top and eliminated Santos 2–1. Pelé would, however, finish as the top scorer of the tournament with eight goals.\n\n1966–1974: O Milésimo and final years with Santos\nIn 1966, Santos failed to retain the Taça Brasil as Pelé's goals were not enough to prevent a 9–4 defeat by Cruzeiro (led by Tostão) in the final series. The club did, however, win the Campeonato Paulista in 1967, 1968, and 1969. On 19 November 1969, Pelé scored his 1,000th goal in all competitions, in what was a highly anticipated moment in Brazil. The goal dubbed O Milésimo (The Thousandth), occurred in a match against Vasco da Gama, when Pelé scored from a penalty kick, at the Maracanã Stadium.\n\nIn 1969, the two factions involved in the Nigerian Civil War agreed to a 48-hour ceasefire so they could watch Pelé play an exhibition game in Lagos. Santos ended up playing to a 2–2 draw with Lagos side Stationary Stores FC and Pelé scored his team's goals. The civil war went on for one more year after this game. During his time at Santos, Pelé played alongside many gifted players, including Zito, Pepe, and Coutinho; the latter partnered him in numerous one-two plays, attacks, and goals. After Pelé's 19th season with Santos, he left Brazilian football. Pelé's 643 goals for Santos were the most goals scored for a single club until it was surpassed by Lionel Messi of Barcelona in December 2020.\n\nNew York Cosmos\n\nPelé signing a football for US president Richard Nixon at the White House in 1973, two years before joining the New York Cosmos\nAfter the 1974 season (his 19th with Santos), Pelé retired from Brazilian club football although he continued to occasionally play for Santos in official competitive matches. A year later, he came out of semi-retirement to sign with the New York Cosmos of the North American Soccer League (NASL) for the 1975 season. At a chaotic press conference at New York's 21 Club, the Cosmos unveiled Pelé. John O'Reilly, the club's media spokesman, stated, \"We had superstars in the United States but nothing at the level of Pelé. Everyone wanted to touch him, shake his hand, get a photo with him.\" Though well past his prime at this point, Pelé was credited with significantly increasing public awareness and interest of the sport in the US. During his first public appearance in Boston, he was injured by a crowd of fans who had surrounded him and was evacuated on a stretcher.\n\n\nPelé entering the field to play his first game with the Cosmos, 15 June 1975\nPelé made his debut for the Cosmos on 15 June 1975 against the Dallas Tornado at Downing Stadium, scoring one goal in a 2–2 draw. Pelé opened the door for many other stars to play in North America. Giorgio Chinaglia followed him to the Cosmos, then Franz Beckenbauer and his former Santos teammate Carlos Alberto. Over the next few years other players came to the league, including Johan Cruyff, Eusébio, Bobby Moore, George Best and Gordon Banks.\n\nIn 1975, one week before the Lebanese Civil War, Pelé played a friendly game for the Lebanese club Nejmeh against a team of Lebanese Premier League stars, scoring two goals which were not included in his official tally. On the day of the game, 40,000 spectators were at the stadium from early morning to watch the match.\n\nPelé led the Cosmos to the 1977 Soccer Bowl, in his third and final season with the club. In June 1977, the Cosmos attracted an NASL record 62,394 fans to Giants Stadium for a 3–0 victory past the Tampa Bay Rowdies with a 37-year-old Pelé scoring a hat-trick. In the first leg of the quarter-finals, they attracted a US record crowd of 77,891 for what turned into an 8–3 rout of the Fort Lauderdale Strikers at Giants Stadium. In the second leg of the semi-finals against the Rochester Lancers, the Cosmos won 4–1. Pelé finished his official playing career on 28 August 1977, by leading the New York Cosmos to their second Soccer Bowl title with a 2–1 win over the Seattle Sounders at the Civic Stadium in Portland, Oregon.\n\nOn 1 October 1977, Pelé closed out his career in an exhibition match between the Cosmos and Santos. The match was played in front of a sold-out crowd at Giants Stadium and was televised in the US on ABC's Wide World of Sports as well as throughout the world. Pelé's father and wife both attended the match, as well as Muhammad Ali and Bobby Moore. Delivering a message to the audience before the start of the game—\"Love is more important than what we can take in life\"—Pelé played the first half with the Cosmos, the second with Santos. The game ended with the Cosmos winning 2–1, with Pelé scoring with a 30-yard free-kick for the Cosmos in what was the final goal of his career. During the second half, it started to rain, prompting a Brazilian newspaper to come out with the headline the following day: \"Even The Sky Was Crying.\"",
            "output": "Pele's last football club was New York Cosmos of the North American Soccer League, for which he played during 3 seasons, ending his professional career in 1977.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.0705254077911377,
            "normalized": 0.7344358307974679
        },
        "potential_entropy": {
            "strategy_component": 0.23000000000000004,
            "strategy_details": {
                "raw": 0.4600000000000001,
                "normalized": 0.4600000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 795,
        "original_data": {
            "instruction": "Write a story where a princess discovers an abandon cave. The story should have a beginning, middle and end.",
            "input": "",
            "output": "Once upon a time there was a princess who was the most beautiful and lovely in all of the land. She was so beautiful and kind in fact that many were jealous of her. This jealousy grew as she became more and more beautiful and kin with each passing day. \n\nThere was one person who was the most jealous of all. Her name was Hilda. She lived outside of town in an abandon shack. It was rumored she had magical powers that she would use for evil. One day she decided to user her magic trap the princess.\n\nOne day the princess was was walking in the forest and came upon a small cave. She hadn't noticed this cave before as this was her favorite area to explore. She was surprised to see it but also quite happy as she loved to find new things on her long walks. She could see something sparkling inside the cave. She was drawn to the pretty colors and what looked like gold and silver.\n\nShe crawled into the cave, however when she did so she saw had been tricked and the cage with the gold and silver was just an illusion. She had actually walked into a giant cage. Hilda had tricked her! She began to cry for she did not know how she would get out of this situation.\n\nDays passed and the princess was becoming very worried and sad that no one would ever find her.\n\nLuckily, there was a friendly troll who lived in a neighboring part of the forest. One day he was out gathering mushrooms for his dinner that night when he heard a small whimpering coming from Hilda's home. He looked in the window and saw the princess. He gasped in disbelief. He decided he had to come up with a plan to save her.\n\nHe was very knowledgeable about plans in the forest and their secret powers. He knew of certain mushrooms that could cast a spell on a person and make them very drowsy. He decided he would slip one of these in Hilda's food so she would fall asleep and he could free the princess.\n\nThe next day while Hilda was taking a bath, he snuck into the house and dropped some of the mushrooms into the soup she was brewing on the stove. he quickly went outside to then hide in the bushes and watch to see what would happen.\n\nHilda came out after taking her bath and getting ready. She laughed as she saw the sad princess in her cage. She then began to pour herself a little bowl of soup for breakfast. After taking a few sips she began to feel very very sleepy. She decided she must lay down at once. \n\nIt had worked! The troll was delighted and quickly entered the house. However, he realized once inside that he did not know where the key was to the cage! The princess whispered that Hilda often kept it in her pocket. \"Oh no!\" he thought. He hadn't thought this through and if he reached into Hilda's pocket he ran the risk of waking her up at which point she would surely cast a spell on him and he and the princess would be doomed.\n\nHe crept quietly toward her. Her pockets were covered by her arms. He lifted one gently. She snored a bit and turned over. It scared him but she seemed to still be asleep. he tried again. This time being more quiet. He was able to get the key out. Success! He quickly went over to the cage and was able to free the princess!\n\nShe returned home to her village and told everyone what a hero the Troll was. The Troll ended up opening a shop in the town where he sold his magic mushrooms.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.008270263671875,
            "normalized": 0.7166486467633929
        },
        "potential_entropy": {
            "strategy_component": 0.5499999999999999,
            "strategy_details": {
                "raw": 1.0999999999999999,
                "normalized": 1.0999999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 796,
        "original_data": {
            "instruction": "What are five totally distinct, creative ways to describe the usefulness of a broken mind.",
            "input": "",
            "output": "1. A broken mind is like a rusty hinge, capable of opening up new possibilities.\n2. A broken mind is like a cracked mirror, reflecting a distorted view of reality.\n3. A broken mind is like a sieve, allowing ideas to pass through and filter out the important ones.\n4. A broken mind is like a pebble in a stream, providing a gentle nudge to go in a new direction.\n5. A broken mind is like a silent voice, conveying a message that is not always heard.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.030716896057129,
            "normalized": 0.43734768458775114
        },
        "potential_entropy": {
            "strategy_component": 0.505,
            "strategy_details": {
                "raw": 1.01,
                "normalized": 1.01,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 797,
        "original_data": {
            "instruction": "Why are people afraid of sharks?",
            "input": "",
            "output": "Many people are afraid of sharks from irrational fear from media, and from losing control. Only about 80 humans are attacked by sharks each year, with a very low death rate. Galeophobia is the term used for being afraid of sharks.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.8240575790405273,
            "normalized": 0.9497307368687221
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 798,
        "original_data": {
            "instruction": "What is a dig in baseball?",
            "input": "",
            "output": "A \"dig\" or \"digging one out\" is an expression to describe a defensive player accepting a baseball with a motion that captures the ball very near the ground.  This kind of expression is often, though not always, used to describe a first baseman catching a ball thrown by another player that bounces before it reaches them as they hold one foot on first base.  This expression can also be used to describe another fielder, typically an infielder, to get a ball into their glove or hand that bounced near them or immediately before reaching them.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.3416554927825928,
            "normalized": 0.8119015693664551
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 799,
        "original_data": {
            "instruction": "Given a reference text about Putlacheruvu, how many canals are there and what are they used for?",
            "input": "Putlacheruvu is a village in the Mandavalli Mandal, Krishna district, Andhra Pradesh state, India. According to the 2011 census it has a population of 1873 living in 583 households.\n\nGeography\nThe village of Putlacheruvu is 6 km from Kolleru Lake and consists of three areas: Krishnapuram, Ramapuram, and Putlacheruvu proper. It features a 7.5 acre freshwater lake with a Naandi Foundation water treatment plant (which is a small failure )and a 12-foot tall statue of Y. S. Rajasekhara Reddy at the south end of the lake. There are three large canals off the Krishna river on both ends of the village. One of these is used for drinking water and the other two are sewage canals used for pumping water into the fish tanks. The soil is very rich and suitable for many crops including rice and coconut. The local wildlife includes parrots, sparrows, minas, and black crows. In the evenings and early mornings, snake birds form in large migrations.",
            "output": "There are three canals in Putlacheruvu. One is used for drinking water and the other two are sewage canals used for pumping water into the fish tanks.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.5,
            "s_out_dens": 0.85,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.5,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.153494834899902,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5800000000000001,
            "strategy_details": {
                "raw": 1.1600000000000001,
                "normalized": 1.1600000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.65,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.5,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 800,
        "original_data": {
            "instruction": "What is a CRM?",
            "input": "",
            "output": "Customer relationship management is a process in which a business or other organization administers its interactions with customers, typically using data analysis to study large amounts of information.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.1644434928894043,
            "normalized": 0.7612695693969727
        },
        "potential_entropy": {
            "strategy_component": 1.2399999999999998,
            "strategy_details": {
                "raw": 2.4799999999999995,
                "normalized": 2.4799999999999995,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 801,
        "original_data": {
            "instruction": "Identify which car manufacturer is German or American: Lotec, Aurica Motors",
            "input": "",
            "output": "Lotec is German, Aurica Motors is American",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.2,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.8,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.21684455871582,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.0699999999999998,
            "strategy_details": {
                "raw": 2.1399999999999997,
                "normalized": 2.1399999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.8,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 802,
        "original_data": {
            "instruction": "Which of the following animals are marine mammals: Human, Dog, Bonobos, Cat, Bats, Whale shark, Bears, Orangutans, Great white shark, Lion, Ocean sunfish, Koala, Cheetah, Beluga sturgeon, Orca, Gorillas, Camel, Hamsters, Lemurs, Blue whale, Chimpanzee",
            "input": "",
            "output": "Of those animals you mentioned, some of them are mammals, some are marine animals and only a smaller number of them are marine mammals. The following can be classified as marine mammals: Orca and Blue whale.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.470104217529297,
            "normalized": 0.8486012050083706
        },
        "potential_entropy": {
            "strategy_component": 0.5525,
            "strategy_details": {
                "raw": 1.105,
                "normalized": 1.105,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 803,
        "original_data": {
            "instruction": "Why are the Los Angeles Lakers the most famous and successful basketball franchise in the world?",
            "input": "",
            "output": "The Los Angeles Lakers are the most famous and successful basketball franchise in the world because they have won the most championships of any any professional N.B.A = National Basketball Association team in the world in the modern era. The modern NBA era begins after the merger of two professional basketball leagues, the NBA and ABA (American Basketball Association) and the creation of the three point basket and twenty-four second clock. The Los Angeles Lakers won the Larry O'Brien championship trophy in 1980, 1982, 1985, 1987, 1988, 2000, 2001, 2002, 2009, 2010 and 2020.  The Lakers appeared in the championship game in 1949,1950,1952,1953,1954,1959,1962,1963,1965,1968,1969,1970,1972,1973,1980,1982, 1983,1984,1985, 1987,1988,1989,1991,2000,2001,2002,2004, 2008,2009,2010,2020. Famous Hall of Fame players include, Kobe Bryant, LeBron James, Magic Johnson, Jerry West, James Worthy, Kareem Abdul-Jabbar, Shaquille O'Neal, Elgin Baylor, Gail Goodrich, George Mikan, Pau Gasol and Wilt Chamberlain",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 1.5118370056152344,
            "normalized": 0.2890962873186384
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 804,
        "original_data": {
            "instruction": "What are the primary elements of Odissi dance?",
            "input": "",
            "output": "The primary elements of Odissi dance are derived from the tribhanga posture. The tribhanga posture (three breaks) is unique to the Odissi classical dance style. Each break refers to the starting position of a body part. The first break is the head and neck. The second break is the waist, and the third break is the hip.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.5,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.5,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.5925545692443848,
            "normalized": 0.5978727340698242
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.5,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 805,
        "original_data": {
            "instruction": "Tell me about the merger history of the Swiss bank UBS",
            "input": "Swiss Bank Corporation was a Swiss investment bank and financial services company located in Switzerland. Prior to its merger, the bank was the third largest in Switzerland with over CHF 300 billion of assets and CHF 11.7 billion of equity.\n\nThroughout the 1990s, SBC engaged in a large growth initiative, shifting its focus from traditional commercial banking into investment banking, in an effort to match its larger Swiss rival Credit Suisse. As part of this strategy, SBC acquired US-based investment bank Dillon Read & Co. as well as London-based merchant bank S.G. Warburg in the mid-1990s. SBC also acquired Chicago-based Brinson Partners and O'Connor & Associates. These acquisitions formed the basis for a global investment banking business.\n\nIn 1998, SBC merged with Union Bank of Switzerland to form UBS, the largest bank in Europe and the second largest bank in the world. The company's logo, which featured three keys, symbolizing \"confidence, security, and discretion\", was adopted by UBS after the 1998 merger. Although the combination of the two banks was billed as a merger of equals, it quickly became evident that from a management perspective, it was SBC that was buying UBS as nearly 80% of the top management positions were filled by legacy Swiss Bank professionals. Today, what was SBC forms the core of many of UBS's businesses, particularly UBS Investment Bank.",
            "output": "The Swiss Bank Corporation was a Swiss investment bank and financial services company located in Switzerland. Prior to its merger, the bank was the third largest in Switzerland with over CHF 300 billion of assets and CHF 11.7 billion of equity.\n\nThroughout the 1990s, SBC engaged in a large growth initiative, shifting its focus from traditional commercial banking into investment banking, in an effort to match its larger Swiss rival Credit Suisse. As part of this strategy, SBC acquired US-based investment bank Dillon Read & Co. as well as London-based merchant bank S.G. Warburg in the mid-1990s. SBC also acquired Chicago-based Brinson Partners and O'Connor & Associates. These acquisitions formed the basis for a global investment banking business.\n\nIn 1998, SBC merged with Union Bank of Switzerland to form UBS, the largest bank in Europe and the second largest bank in the world. The company's logo, which featured three keys, symbolizing \"confidence, security, and discretion\", was adopted by UBS after the 1998 merger. Although the combination of the two banks was billed as a merger of equals, it quickly became evident that from a management perspective, it was SBC that was buying UBS as nearly 80% of the top management positions were filled by legacy Swiss Bank professionals. Today, what was SBC forms the core of many of UBS's businesses, particularly UBS Investment Bank.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 2.7343058586120605,
            "normalized": 0.6383731024605888
        },
        "potential_entropy": {
            "strategy_component": 0.39499999999999996,
            "strategy_details": {
                "raw": 0.7899999999999999,
                "normalized": 0.7899999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 806,
        "original_data": {
            "instruction": "Do i must to hug my wife",
            "input": "",
            "output": "Hugging Improves Relationships.\nIt's a deeper, more intimate level of connection that you don't get from talking. The second reason is that physical touch causes the release of the love hormone oxytocin, which improves bonding and trust and makes people feel closer to one another.\nYou should hug your spouse every day as an act of showing your love to them. Giving a hug is one of the simplest ways to show your love. Anytime, anywhere a hug can is a healthy reminder to your spouse of why you love them so much.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.5,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.0032529830932617,
            "normalized": 0.7152151380266462
        },
        "potential_entropy": {
            "strategy_component": 1.4725000000000001,
            "strategy_details": {
                "raw": 2.9450000000000003,
                "normalized": 2.9450000000000003,
                "components": {
                    "instruction": 0.6,
                    "input": 2.0,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.5,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 807,
        "original_data": {
            "instruction": "Identify which car manufacturer is German or American: Dodge, Volkswagen",
            "input": "",
            "output": "Dodge is American, Volkswagen is German",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.817119598388672,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 808,
        "original_data": {
            "instruction": "How was Napoleon defeated?",
            "input": "Napoleon Bonaparte (born Napoleone Buonaparte; 15 August 1769 – 5 May 1821), later known by his regnal name Napoleon I, was a Corsica-born French military commander and political leader who rose to prominence during the French Revolution and led successful campaigns during the Revolutionary Wars. He was the de facto leader of the French Republic as First Consul from 1799 to 1804, then Emperor of the French from 1804 until 1814 and again in 1815. Napoleon's political and cultural legacy endures to this day, as a highly celebrated and controversial leader. He initiated many liberal reforms that have persisted in society, and is considered one of the greatest military commanders in history. His campaigns are still studied at military academies worldwide. Between three and six million civilians and soldiers died in what became known as the Napoleonic Wars.\n\nNapoleon was born on the island of Corsica, not long after its annexation by France, to a native family descending from minor Italian nobility. He supported the French Revolution in 1789 while serving in the French army, and tried to spread its ideals to his native Corsica. He rose rapidly in the Army after he saved the governing French Directory by firing on royalist insurgents. In 1796, he began a military campaign against the Austrians and their Italian allies, scoring decisive victories and becoming a national hero. Two years later, he led a military expedition to Egypt that served as a springboard to political power. He engineered a coup in November 1799 and became First Consul of the Republic.\n\nDifferences with the United Kingdom meant France faced the War of the Third Coalition by 1805. Napoleon shattered this coalition with victories in the Ulm campaign, and at the Battle of Austerlitz, which led to the dissolution of the Holy Roman Empire. In 1806, the Fourth Coalition took up arms against him. Napoleon defeated Prussia at the battles of Jena and Auerstedt, marched the Grande Armée into Eastern Europe, and defeated the Russians in June 1807 at Friedland, forcing the defeated nations of the Fourth Coalition to accept the Treaties of Tilsit. Two years later, the Austrians challenged the French again during the War of the Fifth Coalition, but Napoleon solidified his grip over Europe after triumphing at the Battle of Wagram.\n\nHoping to extend the Continental System, his embargo against Britain, Napoleon invaded the Iberian Peninsula and declared his brother Joseph the King of Spain in 1808. The Spanish and the Portuguese revolted in the Peninsular War aided by a British army, culminating in defeat for Napoleon's marshals. Napoleon launched an invasion of Russia in the summer of 1812. The resulting campaign witnessed the catastrophic retreat of Napoleon's Grande Armée. In 1813, Prussia and Austria joined Russian forces in a Sixth Coalition against France, resulting in a large coalition army defeating Napoleon at the Battle of Leipzig. The coalition invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. He was exiled to the island of Elba, between Corsica and Italy. In France, the Bourbons were restored to power.\n\nNapoleon escaped in February 1815 and took control of France. The Allies responded by forming a Seventh Coalition, which defeated Napoleon at the Battle of Waterloo in June 1815. The British exiled him to the remote island of Saint Helena in the Atlantic, where he died in 1821 at the age of 51.\n\nNapoleon had an extensive impact on the modern world, bringing liberal reforms to the lands he conquered, especially the regions of the Low Countries, Switzerland and parts of modern Italy and Germany. He implemented many liberal policies in France and Western Europe.",
            "output": "Napoleon invaded Russia in the summer of 1812, resulting in a catastrophic retreat of Napoleon's Grade Armée. One year later, Prussia and Australia joined Russian forces, defeating Napoleon at the Battle of Leipzig. The coalition then invaded France and captured Paris, forcing Napoleon to abdicate in April 1814. Napoleon was forced to exile to the island of Elba.\nNapoleon escaped in 1815 and took back control of France. He was defeated again during the Battle of Waterloo in June 1815 and sent to exile by the British to the remote island of Saint Helena where he died in 1921.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.557995319366455,
            "normalized": 0.58799866267613
        },
        "potential_entropy": {
            "strategy_component": 0.32999999999999996,
            "strategy_details": {
                "raw": 0.6599999999999999,
                "normalized": 0.6599999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.15000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 809,
        "original_data": {
            "instruction": "If each client was served by the same number of professionals, how many professionals (round to one decimal place) would each client be served by?",
            "input": "RGP, formerly known as Resources Global Professionals, is the operating arm of Resources Connection, Inc. (NASDAQ:RGP). The company provides consulting services in the areas of finance & accounting; information management; governance, risk & compliance (GRC); human capital; legal & regulatory; corporate advisory & restructuring; strategic communications; and supply chain management. As of fiscal year ending May 28, 2016, the company employed 3,283 professionals in 68 offices in 20 countries around the world. The company reported serving over 1,700 clients. Company headquarters are located in Irvine, CA.",
            "output": "3283 professionals and 1700 clients, so 3283/1700 = 1.9 professionals serving each client.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.878527879714966,
            "normalized": 0.6795793942042759
        },
        "potential_entropy": {
            "strategy_component": 0.3975,
            "strategy_details": {
                "raw": 0.795,
                "normalized": 0.795,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.28500000000000003
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 810,
        "original_data": {
            "instruction": "Who was declared as the winner of LIVA Miss Diva Universe 2021?",
            "input": "",
            "output": "Harnaaz Sandhu(Chandigarh)",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.850569248199463,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 811,
        "original_data": {
            "instruction": "What is the Maareech Advanced Torpedo Defence System (ATDS)?",
            "input": "Maareech Advanced Torpedo Defence System (ATDS) is a torpedo detection and countermeasure system used by the Indian Navy. The system offers a complete solution to detect and locate an incoming torpedo and to apply countermeasures to protect naval platform against torpedo attack. It was developed as a joint project of the Naval Physical and Oceanographic Laboratory (NPOL), Kochi and the Naval Science and Technological Laboratory (NSTL), Visakhapatnam. Bharat Electronics is manufacturing it India.",
            "output": "The Maareech Advanced Torpedo Defence System (ATDS is a torpedo detection and countermeasure system used by the Indian Navy.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.1994242668151855,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4975,
            "strategy_details": {
                "raw": 0.995,
                "normalized": 0.995,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.525
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 812,
        "original_data": {
            "instruction": "Is the person who invented bitcoin Japanese?",
            "input": "Satoshi Nakamoto (born 5 April 1975) is the name used by the presumed pseudonymous person or persons who developed bitcoin, authored the bitcoin white paper, and created and deployed bitcoin's original reference implementation. As part of the implementation, Nakamoto also devised the first blockchain database. Nakamoto was active in the development of bitcoin up until December 2010.\n\nThere has been widespread speculation about Satoshi Nakamoto's true identity, with a variety of people posited as the person or persons behind the name. Though Nakamoto's name is Japanese, and he stated in 2012 that he was a man living in Japan, most of the speculation has involved software and cryptography experts in the United States or Europe.",
            "output": "The answer is uncertain because this man was presumed to be living in Japan but he or she could be from Europe or American or anywhere in the world.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9027700424194336,
            "normalized": 0.9722200121198382
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 813,
        "original_data": {
            "instruction": "Why Indians are so beautiful",
            "input": "",
            "output": "Indians have many culture and the color of Asian subcontinent skin is makes them looks exceptional. Indian are Vibrant",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.2,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.3,
            "s_out_div": 0.25,
            "s_out_dens": 0.2,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 6.2323832511901855,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.8275,
            "strategy_details": {
                "raw": 3.655,
                "normalized": 3.655,
                "components": {
                    "instruction": 0.96,
                    "input": 1.75,
                    "output": 0.945
                },
                "gaps": {
                    "ins_tone": 0.8,
                    "inp_depth": 0.9,
                    "inp_complex": 0.85,
                    "out_cot": 0.7,
                    "out_div": 0.75,
                    "out_dens": 0.8,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 814,
        "original_data": {
            "instruction": "Classify each of the following as either DC or Marvel characters: Superman, Mickey Mouse, Thor,  Batwoman, Gamora, Scarlet Witch, Superman, Wonder Woman, Homer Simpson, Falcon, Moon Girl",
            "input": "",
            "output": "DC Characters: Superman, Batwoman, Superman, Wonder Woman.\n\nMarvel Characters: Thor, Gamora, Scarlet Witch, Falcon, Moon Girl.\n\nNeither DC or Marvel: Mickey Mouse, Homer Simpson.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.771596670150757,
            "normalized": 0.934741905757359
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 815,
        "original_data": {
            "instruction": "Name some of the bravolebrities from The Real Housewives of Orange County",
            "input": "",
            "output": "Shannon Beador, Heather Dubrow, Gina Kirschenheiter and Emily Simpson",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.892368316650391,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.12,
            "strategy_details": {
                "raw": 2.24,
                "normalized": 2.24,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 816,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Gbedu, Guitalele",
            "input": "",
            "output": "Guitalele is string, Gbedu is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 7.517282009124756,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 817,
        "original_data": {
            "instruction": "Where is Lukaya River?",
            "input": "The Lukaya is a river in the Democratic Republic of the Congo. Its source is located in the Crystal Mountains (Montagnes de Cristal), from which it runs eastward through Bas-Congo, then runs into the banks of the Ndjili River. The rail line from Matadi to Kinshasa runs along the river valley for a time, passing to the south and then to the east of Kinshasa. At one point the river was the namesake of a district in the Congo Free State.\n\nJust to the south of Kinshasa, a small cascade on the river, the Petites Chutes de la Lukaya, is a gathering place for several tourist activities including the lake formed by the river valley, beaches and waterfalls, and the Lola Ya Bonobo Sanctuary of Kinshasa. This is located in the Mont Ngafula neighborhood, which the river runs through.",
            "output": "The Lukaya is a river in the Democratic Republic of the Congo, originating in the Crystal Mountains and running eastward through Bas-Congo and into the Ndjili River. The Petites Chutes de la Lukaya is a gathering place for tourist activities.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9093520641326904,
            "normalized": 0.9741005897521973
        },
        "potential_entropy": {
            "strategy_component": 0.49249999999999994,
            "strategy_details": {
                "raw": 0.9849999999999999,
                "normalized": 0.9849999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 818,
        "original_data": {
            "instruction": "Categorize each of the following as being beneficial or not beneficial to making a home protected from burglars: alarm system, open doors, locks, cameras, open windows",
            "input": "",
            "output": "Alarm System: Beneficial\nOpen Doors: Not Beneficial\nLocks: Beneficial\nCameras: Beneficial\nOpen Windows: Not Beneficial",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.5,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.5,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.831315517425537,
            "normalized": 0.6660901478358677
        },
        "potential_entropy": {
            "strategy_component": 0.7324999999999999,
            "strategy_details": {
                "raw": 1.4649999999999999,
                "normalized": 1.4649999999999999,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.0,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.5,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 819,
        "original_data": {
            "instruction": "Divide these numbers into prime, composite or neither. 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12.",
            "input": "",
            "output": "2, 3, 5, 7 and 11 are all prime numbers. 4, 6, 8, 9, 10 and 12 are composite numbers. 0 and 1 are neither prime nor composite numbers.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 1.0313341617584229,
            "normalized": 0.15180976050240652
        },
        "potential_entropy": {
            "strategy_component": 0.48500000000000004,
            "strategy_details": {
                "raw": 0.9700000000000001,
                "normalized": 0.9700000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.7000000000000001,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 820,
        "original_data": {
            "instruction": "What is the negative side effect of the \"fire power\" on Fargus?",
            "input": "Pandemonium 2 is a psychedelic 2.5D game; specifically, it renders characters and environments with polygons and uses a 3D camera to create the appearance of 3-D, while gameplay is actually on a 2-D plane. Players can choose to be either Nikki or Fargus - who each have different specialties - with the option to change for each level. The player characters' abilities have been expanded over those in Pandemonium! with the ability to climb ropes, crawl, and pull themselves up ledges. During the quest, players can earn extra lives by collecting treasure.\n\nUnique powers collected during gameplay may replace, extend or add to previous powers, depending on which character is in play and which power has been obtained. The characters handle the unique powers differently. For instance, collecting the \"fire power\" allows Nikki to shoot destructive fireballs, while it grants Fargus invincibility and a deadly touch, but requires him to constantly move forward. Machines and equipment are also available throughout the game, although they are tied to certain areas.",
            "output": "It requires him to constantly move forward.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 6.314697265625,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5225,
            "strategy_details": {
                "raw": 1.045,
                "normalized": 1.045,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.31499999999999995
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 821,
        "original_data": {
            "instruction": "Please summarize following information about Maria Astashkina.",
            "input": "Maria Astashkina (Russian: Mapия Acтaшкинa), (born 5 April 1999 in Penza, Russia) is a Russian swimmer. She won 4 gold medals in (200 m, 100 m, 50m breaststroke and 4 × 100 m medley) at the 2015 European Games.\n\nCareer\nIn 2014, Astashkina competed at the 2014 European Junior Championships winning 4 gold medals in (200 m, 100 m, 50m breaststroke and 4 × 100 m medley), that same year, she competed in seniors at the 2014 European Championships and at the 2014 FINA World Swimming Championships (25 m) in Doha, Qatar where she qualified for the finals in 200 m breaststroke finishing in 6th place and with Team Russia finishing 5th in 4 × 100 m women's medley.\n\nIn June 2015, 16 year old Astashkina was selected to compete at the inaugural 2015 European Games in Baku, where she won four gold medals, in (200m, 100 m, 50m breaststroke), in 4 × 100 m medley (with Arina Openysheva, Polina Egorova and Maria Kameneva broke a new junior world record touching in at 4:03.22). Astashkina also broke a junior meet record in 200m breaststroke at the Games. On August 2–9, Astashkina competed in seniors at the 2015 World Championships in Kazan, however she did not reach the semifinals after losing in her preliminary heats.\n\nOn August 25–30, Astashkina then competed at the 2015 World Junior Swimming Championships in Singapore, she won a silver medal in individual women's 200 m breaststroke behind Turkey's Viktoriya Zeynep Gunes, she finished 4th in 100 m breaststroke, 6th in 50 m breaststroke and won gold with the Russian Team in women's 4 × 100 m medley.\n\nIn April 2016, Astashkina competed at the National Championships. She won a bronze medal in 200 m breastroke but she did not comply required time to qualify at the Olympics.",
            "output": "Sure, here is a summary of Maria Astashkina:\nBorn on April 5, 1999, in Penza, Russia, Maria Astashkina is a Russian swimmer who took home four gold medals from the 2015 European Games in the 200 m, 100 m, 50 m breaststroke, and four 100 m medley events. She participated in the 2014 FINA World Swimming Championships (25 m) in Doha, Qatar, as well as the 2014 European Junior Championships. She was chosen to compete at the first-ever 2015 European Games in Baku in June of that year, where she took home four gold medals and set a junior meet record in the 200-meter breaststroke. She lost in her opening-round heats at the 2015 World Championships in Kazan, which prevented her from making it through the quarterfinals. Despite taking home a bronze in the 200 m breaststroke in April 2016, she was unable to make the Olympics.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.0681583881378174,
            "normalized": 0.4480452537536621
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 822,
        "original_data": {
            "instruction": "Please give me a brief summary of this paragraph about Tom Brady's NFL accomplishments.",
            "input": "The winningest quarterback in NFL history, Brady won 251 regular season games and 35 postseason games for a combined 286 wins. Brady holds a .754 winning percentage, which is the highest among NFL quarterbacks who have started 100 games. He is the only NFL quarterback to win 200 regular season games and his 35 postseason victories are more than twice those of any other quarterback. Brady is also the only NFL quarterback to win all 16 regular season games, which he accomplished in 2007. In 2021, Brady became one of four quarterbacks to defeat all 32 NFL teams. Brady has the most Pro Bowl selections in NFL history at 15 and was named league MVP three times in 2007, 2010, and 2017. His 2010 MVP was awarded in an unanimous decision; the first time this distinction was achieved. Brady was also named Male Athlete of the Year in 2007, making him third NFL player to receive the award, joining Joe Montana. Brady, Montana, and Patrick Mahomes are the only players to win multiple NFL MVP and Super Bowl MVP awards.",
            "output": "Brady is the winningest quarterback in NFL history. He holds numerous records in the regular season and postseason. He also holds distinctions as the only quarterback to accomplish certain feats, such as winning all 16 regular season games in 2007.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.8565800189971924,
            "normalized": 0.6733085768563407
        },
        "potential_entropy": {
            "strategy_component": 0.32500000000000007,
            "strategy_details": {
                "raw": 0.6500000000000001,
                "normalized": 0.6500000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 823,
        "original_data": {
            "instruction": "What do the suits on a deck of cards represent?",
            "input": "",
            "output": "Each of the suits on a deck of cards represents the four major pillars of the economy in the middle ages: heart represented the Church, spades represented the military, clubs represented agriculture, and diamonds represented the merchant class.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.5,
            "s_out_div": 0.4,
            "s_out_dens": 0.6,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.5,
            "s_out_div": 0.6,
            "s_out_dens": 0.4,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.3552920818328857,
            "normalized": 0.8157977376665387
        },
        "potential_entropy": {
            "strategy_component": 0.8,
            "strategy_details": {
                "raw": 1.6,
                "normalized": 1.6,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.5,
                    "out_div": 0.6,
                    "out_dens": 0.4,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 824,
        "original_data": {
            "instruction": "What are a few ways that I can keep my house cool when it is hot in the summer?",
            "input": "",
            "output": "Here are a few thoughts on how you could keep your house cool when it is hot in the summer. First, you could draw all of your blinds and shades and keep the house as dark as possible. Second, you could open the windows to let in a cool breeze if one is available. You could also turn on a fan to circulate the air. If these or a combination of these ideas are not working, then you could turn on the air conditioning.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.281052827835083,
            "normalized": 0.5088722365243095
        },
        "potential_entropy": {
            "strategy_component": 1.04,
            "strategy_details": {
                "raw": 2.08,
                "normalized": 2.08,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 825,
        "original_data": {
            "instruction": "Do you have any dietary restrictions? If so, what are they.",
            "input": "",
            "output": "Yes. I'm both kosher AND vegetarian. That mean simply getting kosher food that contains meat or fish won't be something I can eat. Likewise, a vegetarian entree from a regular restaurant also won't work as there are all sorts of problems that might ensure. These include: cross-contamination w/ utensils used to prepare meat or fish, inclusion of ingredients that appear vegetarian but aren't (e.g., cheese made with animal rennet or enzymes, pastries containing gelatin made from bones, natural colorants or flavorings derived from insects or animals, egg-substitute made from plasmolyzed fish blood, etc.). My own personal standard doesn't require certification by a kosher agency; but in general, I'll only eat at places that only serve vegan or vegetarian items (and from the latter, there may be items I can't eat).",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.95,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 3.820324659347534,
            "normalized": 0.9486641883850098
        },
        "potential_entropy": {
            "strategy_component": 1.18,
            "strategy_details": {
                "raw": 2.36,
                "normalized": 2.36,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.12
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 826,
        "original_data": {
            "instruction": "What is the Mile of Music?",
            "input": "",
            "output": "The Mile of Music is an annual 4-day music festival held in Appleton, WI.  Started in 2013, each year of the festival increments an additional 'mile,' so 2023 will be celebrating Mile 10.  The artists that perform are typically up-and-coming artists who only play original music at the festival. Sets tend to be about an hour long, and they are hosted throughout the approximate 1-mile stretch of downtown Appleton on College Avenue at various coffee shops, bars, restaurants and parks.  A wide range of musical genres have been represented, including: rock, country, jazz, folk,  hip hop and circus punk.  Some performers come from the local area, such as Kyle Megna and The Monsoons.  Others are more regional performers, like Bascom Hill.  But many performers come from across the country, and have performed on multiple 'Miles', such as Chestnut Grove, The Quiet Hollers and Jamie Kent. Repeat bands often feel famous during the four days of the festival, leading Jamie Kent to write his song 'Famous (in Appleton)'.  In conjunction with the music festival, an annual supported bike ride was launched called Bike to the Beat.  Riders can stop at several locations along routes of various distances to get refreshments and hear performers from the festival.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.5103518962860107,
            "normalized": 0.860100541796003
        },
        "potential_entropy": {
            "strategy_component": 0.2875,
            "strategy_details": {
                "raw": 0.575,
                "normalized": 0.575,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.105
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 827,
        "original_data": {
            "instruction": "Assume you are the head boy/ girl of your school, draft a notice of around 50 words inviting students of classes 11 and 12 to attend a career counselling session by ABC, including how it will help the students.",
            "input": "",
            "output": "Date\nSubject - Career Counselling Session by ABC\n\nAll rhe students of classes 11 and 12 are invited to attend a career counselling session by ABC, an eminent counsellor on 30th April 2023. It will be of immense benefit to all students and will help you better understand your career options, and help you meet your future goals. Interested students are requested to register their names with their respective class teachers.\nDate- 30 April 2023\nTime - 10 am to 12 pm\nVenue - School auditorium \n\nSigned\nHead Girl",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.787898540496826,
            "normalized": 0.6536852972848075
        },
        "potential_entropy": {
            "strategy_component": 0.5225,
            "strategy_details": {
                "raw": 1.045,
                "normalized": 1.045,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.31499999999999995
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 828,
        "original_data": {
            "instruction": "What are some of the most famous rock bands from Seattle, Washington?",
            "input": "",
            "output": "Famous rock bands from Seattle include Nirvana, Pearl Jam, Soundgarden, and Foo Fighters.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.357006072998047,
            "normalized": 0.8162874494280133
        },
        "potential_entropy": {
            "strategy_component": 0.9824999999999999,
            "strategy_details": {
                "raw": 1.9649999999999999,
                "normalized": 1.9649999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 829,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Naqara, Bouzouki",
            "input": "",
            "output": "Bouzouki is string, Naqara is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 5.718413829803467,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4625,
            "strategy_details": {
                "raw": 0.925,
                "normalized": 0.925,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.19499999999999998
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 830,
        "original_data": {
            "instruction": "As per WHO Marburg Virus Disease(MVD) is a severe often fatal haemorrhagic fever it was earlier known as",
            "input": "",
            "output": "Marburg Haemorrhagic Fever",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.4,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.6,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.993256092071533,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6125,
            "strategy_details": {
                "raw": 1.225,
                "normalized": 1.225,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.6,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 831,
        "original_data": {
            "instruction": "What are some of the challenges and solutions involved in growing tomatoes in the Pacific Northwest?",
            "input": "",
            "output": "Growing tomatoes in the Pacific Northwest is very possible, but can present a number of challenges. The three biggest concerns revolve around:\n\n1. A short growing season\n2. Cooler summers\n3. Wetter than average summers\n\nHere are some solutions you can use to mitigate these challenges.\n\nDo deal with the short growing season, you need to get a headstart on growing by using starters. What this means is that instead of planting a seed in the ground in Springtime, you start a seed indoors in late winter, grow it into a small to medium plant, then transplant that \"starter\", into the ground when the temperature is warm enough. You can buy starters from Nurseries, or start your own at home. Nursery starters are easier, but by growing at home you can experiment with more rare varieties and gain experience in cultivating a plant from the seedling stage. If you grow your own it's important to slowly climatize the plant to outdoor weather leading up to the transplant date by moving it outside starting with a couple hours a day, increasing an hour or two per day over 2-3 weeks. By using starters you essentially extend the growing season by giving your plant a multi month head start. \n\nTomatoes mature best in warm to hot climates. The pacific northwest has cooler summers than many of the growing regions in the world. There is not a lot we can do to control mother nature, but there are one or two methods to help yourself here. Certain varieties of tomatoes do better in cooler summers - try and select these. If you're using starts from a local nursery they can guide you - starts from big national box stores might not be the best for your local Northwest climate. Another technique, if you are experiencing a cooler than normal Spring and start of Summer, is to build a \"hoop house\", using PVC pipe frame and a visqueen tarp. This will let light in and act as a mini-greenhouse, trapping heat around the plant. Keep in mind tomatoes need pollinators like bees to produce fruit, so once the plan t begins to flower you'll want to ensure pollination can occur.\n\nMoisture creates issues with Tomato plants as they are very susceptible to various types of fungus that grow on the leaves. Blight is one in particular common among tomatoes. The wet summers we sometimes have create conditions on the leaves conducive to fungal growth, they can also splash fungus onto the leaves from the soil. The easiest way to deal with these two issues is in proper pruning. As early as you can prune any lower sets of leaves that could possibly come into contact with the ground. Don't do this too soon as the plant needs some leaves to grow. There are many ways to prune a tomato that you can read about, but one common one in wet areas is called single stemming - in this technique you prune \"suckers\", which are the small offshoots that grow in between the main stem and the leaf stems. By pruning the suckers you prevent too much bushy growth, which will help the leaves dry out with airflow after a rain storm. The last issue related to water is that heavy late season rains will cause the fruits to swell with water and crack. There's not an easy solution here. If it's towards the end of the season and a rainstorm is coming in, this can be a good time to pick all your fruit, your green tomatoes can be used for green salsas. Some will resort to building a tarp structure above the plants to stop the rain, though this can require a bit more work and ingenuity, and you'll need another system to keep the plants watered.\n\nOverall, growing tomatoes in the Pacific Northwest is totally possible and very rewarding. Hopefully these tips help you to have a fruitful growing season!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.295008659362793,
            "normalized": 0.7985739026750837
        },
        "potential_entropy": {
            "strategy_component": 0.5499999999999999,
            "strategy_details": {
                "raw": 1.0999999999999999,
                "normalized": 1.0999999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 832,
        "original_data": {
            "instruction": "I strongly believe in having a growth mindset and being a lifelong learner. However, I am a very busy person and I always find it hard to find time in the day to dedicate to educating myself on a variety of topics. I would love to read more, but I simply don't have the time to be able to do this today regularly. What are some ways that I can consume content to learn and exercise my mind, using the least amount of time as possible?",
            "input": "",
            "output": "Time management is very hard for many people. Having a growth mindset is a fantastic way to continue to better yourself through all of life's troubles and tribulations. Here are a few ways that you can educate yourself either on the go, or with minimal time commitment:\n\n1. TED (and TEDx) Talks - TED Talks are popular videos that present a great idea in 18 minutes or less. While TED talks are intended for a global audience, TEDx talks are intended for smaller audiences and are usually presented at a local level. TED and TEDx talk videos are readily available for consumption, for free from the website www.ted.com, via a mobiule app, as well as on Spotify, YouTube and many other content platforms.\n\n2. Podcasts - Podcasts are a series of on-demand episodes, delivered orally, focused on a specific theme. Podcasts can be accessed from many types of devices and are readily available from many popular platforms, including Spotify, Apple Podcasts and iHeartRadio. Podcasts are focused on any number of topics, and can easily be listened to while on the go. \n\n3. Newsletters - Newsletters are printed or electronic reports that are made available by many organizations to a subscribed audience. They are often written in short, digestible formats to provide the highlights of a variety of topics, and typically contain links to learn more, if desired. They usually take only a few minutes to read and are readily available from news sources like the Wall Street Journal, CNN, Fox News, as well as many websites and LinkedIn groups. \n\nIn addition to these ideas, simply dedicating a small amount of time each day to learning and improving yourself can go a long way. Try to dedicate 15 minutes every day, ideally at the same time each day, toward this and before long, it will become a habit.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.689957618713379,
            "normalized": 0.6257021767752511
        },
        "potential_entropy": {
            "strategy_component": 0.5875,
            "strategy_details": {
                "raw": 1.175,
                "normalized": 1.175,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 833,
        "original_data": {
            "instruction": "A dialogue between two characters discussing the impact of social media on their lives. Both positive and negative points",
            "input": "",
            "output": "Person 1: Hey, what happened? Why are you sad?\nPerson 2:  I'm sad because my grades are low.\nPerson 1: But you used always study on your iPad... where did you go\nwrong?\nPerson 2: Yeh... I was using my iPad but didn't study…\nI was engaging myself in other things.\nPerson 1: Ohh..! That isn't good. See, my friend, multitasking has become a common thing in our life.\nBut we should know how to manage it all. We are too dependent on smartphones for everything\nlike work, school, shopping and personal life. We are addicted to smartphones, the internet and social media\nSocial media is harming us in many ways like\n* It is stealing our personal life\n﻿﻿Increase of cybercrime and cyberbullying.\n﻿﻿Depression anxiety in children\n*Addiction to games on other social networking sites.\n* Causes emotional damage to a person, which leads to suicide.\n* Mental illness. , etc.\n\nBut on the other hand, there are some positive aspects to note as well\n Internet and social media help families stay connected\nIt helps in online banking and paying bills. And\nIt helps in online learning and remote working\nIt can be an effective tool for marketing\n5. Information seeking\nSo, my dear friend internet and social media have their own merits and demerits. Use it wisely and for a good cause.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.2262444496154785,
            "normalized": 0.7789269856044224
        },
        "potential_entropy": {
            "strategy_component": 0.7725,
            "strategy_details": {
                "raw": 1.545,
                "normalized": 1.545,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 834,
        "original_data": {
            "instruction": "What is primary innovation that a piano delivered over a harpsichord?",
            "input": "",
            "output": "Since a harpsichord plucks the strings the same way each time in response to a keystroke, it delivers the same loudness for each note. A piano on the other hand uses padded hammers to strike the strings, and allows the musician to control each strike in such a way that they have control over the dynamics of each note. This control allows pianists to express a broad range of emotions by adjusting the loudness and softness according to the feelings they want to convey to the listener.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            1,
            0
        ],
        "entropy": {
            "raw": 2.826070547103882,
            "normalized": 0.6645915848868233
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 835,
        "original_data": {
            "instruction": "Is a vitamin d supplement good for you?",
            "input": "",
            "output": "When taken on a regular basis Vitamin D helps support immune health and cellular activity.  It has also been show to support cardiovascular function is some people and can reduce your risk for multiple sclerosis.  You can also get Vitamin D by spending plenty of time in sun.  You can absorb Vitamin D from a healthy dose of sunshine through your skin too.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.65,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.35,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.5794615745544434,
            "normalized": 0.8798461641584124
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.35,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 836,
        "original_data": {
            "instruction": "Pultizer Prize 2022 is given which field",
            "input": "",
            "output": "Journalism, Books, Drama and Music",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 5.115817070007324,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.105,
            "strategy_details": {
                "raw": 2.21,
                "normalized": 2.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 837,
        "original_data": {
            "instruction": "What are the world's most popular sports?",
            "input": "",
            "output": "Football (or soccer) is the world's most popular sport. Others include cricket, hockey, tennis, volleyball, table tennis, and basketball. This might come as a surprise to Americans, who favor (American) football.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.7,
            "s_out_div": 0.8,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.8286874294281006,
            "normalized": 0.6653392655508858
        },
        "potential_entropy": {
            "strategy_component": 1.08,
            "strategy_details": {
                "raw": 2.16,
                "normalized": 2.16,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.42
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 838,
        "original_data": {
            "instruction": "Name some of the popular gaming consoles in the 2020s",
            "input": "",
            "output": "PlayStation 5, Xbox series X, Nintendo Switch, and Steam Deck",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.2028331756591797,
            "normalized": 0.772238050188337
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 839,
        "original_data": {
            "instruction": "Who is the lead singer of Korn?",
            "input": "",
            "output": "Jonathan Houseman Davis is the lead singer of nu-metal band, Korn.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.2,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.8,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 4.350020408630371,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.23,
            "strategy_details": {
                "raw": 2.46,
                "normalized": 2.46,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.8,
                    "output": 0.42000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.8,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 840,
        "original_data": {
            "instruction": "Please summarize the causes and cure for color blindness given in the passage?",
            "input": "The most common cause of color blindness is an inherited problem or variation in the functionality of one or more of the three classes of cone cells in the retina, which mediate color vision. The most common form is caused by a genetic disorder called congenital red–green color blindness. Males are more likely to be color blind than females, because the genes responsible for the most common forms of color blindness are on the X chromosome. Non-color-blind females can carry genes for color blindness and pass them on to their children. Color blindness can also result from physical or chemical damage to the eye, the optic nerve, or parts of the brain. Screening for color blindness is typically done with the Ishihara color test.\n\nThere is no cure for color blindness. Diagnosis may allow an individual, or their parents/teachers to actively accommodate the condition. Special lenses such as EnChroma glasses or X-chrom contact lenses may help people with red–green color blindness at some color tasks, but they do not grant the wearer \"normal color vision\". Mobile apps can help people identify colors.",
            "output": "The most common cause of color blindness is genetic disorder. Males are more likely to be color blind than females, because the genes responsible for the same are present on the X chromosome. Females who are non-color blind could still carry and pass on genes for color blindness. Any kind of physical or chemical damage to the eye, optic nerve or brain could also lead to the same.\n\nThere is no cure for color blindness. Affected people can actively accommodate the condition or use special glasses or lenses that can help people with red-green color blindness to a certain extent. There are mobile apps available as well to help identify colors.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.937807559967041,
            "normalized": 0.6965164457048688
        },
        "potential_entropy": {
            "strategy_component": 0.43249999999999994,
            "strategy_details": {
                "raw": 0.8649999999999999,
                "normalized": 0.8649999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 841,
        "original_data": {
            "instruction": "I would like to go to Mystery Spot near Santa Cruz California. What can you tell me about the mystery surrounding the Mystery Spot and how should I plan for this trip?",
            "input": "",
            "output": "It is great to hear that you are looking to go to Mystery Spot in Santa Cruz, California. During winter it can get chilly so layer up. The road leading up from South Bay is windy on 17 South and then goes through a thicker forest. Mystery Spot is a tourist attraction that was opened in 1939 by George Prather. You can sign up for a guided tour at https://www.mysteryspot.com/ and Mystery Spot is open year round. The tour is approx. 40 mins long and you discover that gravity doesn't behave like it should or what should be higher than you turns out to be at same height or lower. You make a steep quick climb to a tilted cabin on a tilted road and you feel disoriented and seem to be at a severe incline when you enter this cabin. While there are various hypotheses as to reason for these effects, the most likely explanation is some level of optical illusion as the road is tilted at a severe opposite angle to the tilted roof line and the inside of the cabin. I think a combination of gravity hill where you get the sensation you are climbing up the road when in reality you are going down and the odd tilted angles of the cabin causes severe optical illusion. It also causes humans to fall back on visual senses more and ignore other senses which might be counter acting what you see.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 3.834787130355835,
            "normalized": 0.95279632295881
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 842,
        "original_data": {
            "instruction": "Identify which instrument is string or percussion: Semantron, Saw sam sai",
            "input": "",
            "output": "Saw sam sai is string, Semantron is percussion.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 7.866380214691162,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4625,
            "strategy_details": {
                "raw": 0.925,
                "normalized": 0.925,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.19499999999999998
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 843,
        "original_data": {
            "instruction": "Who is Marshall Strickland",
            "input": "Marshall Strickland (born March 1, 1983) is an American former professional basketball player. He played college basketball at Indiana University for four years from 2002 to 2006. He is originally from Kingston, Massachusetts and went to Winchendon Prep in Winchendon before transferring to South Carroll High School in Carroll County, Maryland. He is 6 ft 2 in (1.88 m) tall and weighs 195 pounds (88 kg). He is the son of Marshall and Joanna Strickland, and younger brother of former Maryland Terrapins star Marche Strickland. Marshall is a graduate of the University of Maryland School of Dentistry in 2017.",
            "output": "Marshall Strickland (born March 1, 1983) is an American former professional basketball player. He played college basketball at Indiana University for four years from 2002 to 2006. \n\nHe is originally from Kingston, Massachusetts and went to Winchendon Prep in Winchendon before transferring to South Carroll High School in Carroll County, Maryland. He is 6 ft 2 in (1.88 m) tall and weighs 195 pounds (88 kg). \n\nHe is the son of Marshall and Joanna Strickland, and younger brother of former Maryland Terrapins star Marche Strickland. Marshall is a graduate of the University of Maryland School of Dentistry in 2017.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 2.3153114318847656,
            "normalized": 0.518660409109933
        },
        "potential_entropy": {
            "strategy_component": 0.375,
            "strategy_details": {
                "raw": 0.75,
                "normalized": 0.75,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 844,
        "original_data": {
            "instruction": "What are fast recipes to make for dinner?",
            "input": "",
            "output": "Fast recipes for dinner include:\n1. Tacos - corn tortillas, ground beef, diced tomatoes, onions and shredded cheese\n2. Stir fry - cook rice or noodle with vegetables in a wok and use some teriyaki sauce\n3. Breakfast for dinner - eggs are quick to cook and make nutritious  dinner meals as well",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.3744804859161377,
            "normalized": 0.8212801388331822
        },
        "potential_entropy": {
            "strategy_component": 1.04,
            "strategy_details": {
                "raw": 2.08,
                "normalized": 2.08,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 845,
        "original_data": {
            "instruction": "Why do we use passwords?",
            "input": "",
            "output": "Passwords are used in today's digital environment to protect and secure personal accounts.  They are a line of defense against domestic and abroad criminals trying to get access to your personal information in order to steal your identity, open credit cards in your name or steal your hard earned money from your bank accounts.   Password management is hard for a lot of people because they use the same password for every account, they write them down and cannot remember all of them, and have no way to effectively manage their passwords or secure them.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.882725954055786,
            "normalized": 0.6807788440159389
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 846,
        "original_data": {
            "instruction": "In the series A Song of Ice and Fire, who is the founder of House Longwaters?",
            "input": "",
            "output": "The founder of Longwaters is only known as a knight and son of Ser Jon Waters. He is only known as Longwaters.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.3,
            "s_out_div": 0.2,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.7,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "final_strategy_mark": [
            0,
            2,
            3
        ],
        "entropy": {
            "raw": 4.8806071281433105,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.45,
            "strategy_details": {
                "raw": 2.9,
                "normalized": 2.9,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.85,
                    "output": 0.9299999999999999
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.7,
                    "out_div": 0.8,
                    "out_dens": 0.75,
                    "out_bg": 0.85
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 847,
        "original_data": {
            "instruction": "What is the name of the professional football team in Colorado?",
            "input": "",
            "output": "The Denver Broncos",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.709508895874023,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.1500000000000001,
            "strategy_details": {
                "raw": 2.3000000000000003,
                "normalized": 2.3000000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.36000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 848,
        "original_data": {
            "instruction": "Which places are near Lightwater.",
            "input": "Lightwater is a village in the Surrey Heath district of Surrey, England, about 27 miles (43 km) southwest of central London. Immediately surrounding towns and villages include Bagshot, Deepcut, Windlesham, Camberley, and West End, Woking. It is bounded to the north-west by the M3 motorway and to the north-east by the \"Lightwater Bypass\" (A322). To the south-east it is bounded by the \"Red Road\" (B311), while to the south-west there are vehicle-testing tracks owned by the Ministry of Defence.\n\nThe village is adjacent to junction 3 of the M3 motorway. The nearest railway station is at Bagshot, on the line between Ascot and Aldershot. Lightwater is part of the civil parish of Windlesham, which has a population of 17,000 and also includes the neighbouring village of Bagshot.",
            "output": "Lightwater is near Bagshot, Windlesham, Camberley, Deepcut and West End, Woking. It is about 27 miles from London.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.6935296058654785,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3925,
            "strategy_details": {
                "raw": 0.785,
                "normalized": 0.785,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 849,
        "original_data": {
            "instruction": "Why do many people spend so much time on their smartphone everyday?",
            "input": "",
            "output": "There are lots of reasons and one of them is that lot of people are now using smartphones for both work and entertainment. Applications have been developed to help improve work efficiency as well as providing easy access to all kinds of entertaining content.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.65,
            "s_out_dens": 0.8,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.35,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.1445202827453613,
            "normalized": 0.7555772236415318
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.35,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 850,
        "original_data": {
            "instruction": "Tell me which of these items would be helpful on a camping trip: Golf Club, Tent, Matches, Axe, Soccer Ball, Xbox, Water bottle",
            "input": "",
            "output": "The following items would be helpful on a camping trip: Tent, Matches, Axe, Water bottle.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.9164228439331055,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 851,
        "original_data": {
            "instruction": "Without quoting directly from the text, tell me about rainbows.",
            "input": "A rainbow is an optical phenomenon that can occur under certain meteorological conditions. It is caused by reflection, refraction and dispersion of light in water droplets resulting in an effect similar to a spectrum of light appearing in the sky. It takes the form of a multicoloured circular arc. Rainbows caused by sunlight always appear in the section of sky directly opposite the Sun.\n\nRainbows can be full circles. However, the observer normally sees only an arc formed by illuminated droplets above the ground, and centered on a line from the Sun to the observer's eye.\n\nIn a primary rainbow, the arc shows red on the outer part and violet on the inner side. This rainbow is caused by light being refracted when entering a droplet of water, then reflected inside on the back of the droplet and refracted again when leaving it.\n\nIn a double rainbow, a second arc is seen outside the primary arc, and has the order of its colours reversed, with red on the inner side of the arc. This is caused by the light being reflected twice on the inside of the droplet before leaving it.",
            "output": "Rainbows may appear when light passes through water. When a rainbow appears when it's raining, it's always in the opposite direction from the Sun. Although usually in the form of an arc, rainbows can also be full circle. The colors on the primary and double rainbows are inverted with red appearing on the outer part of the primary rainbow and the inner part of a double rainbow.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.9627223014831543,
            "normalized": 0.7036349432809013
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 852,
        "original_data": {
            "instruction": "Which part of the world Egypt is located?",
            "input": "",
            "output": "Egypt is in the north east part of Africa.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.800031900405884,
            "normalized": 0.9428662572588239
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 853,
        "original_data": {
            "instruction": "Classify the following elements as metal or noble gas: Argon, Iron, Copper, Krypton, Neon, Silver, Thorium, Radon, Helium, Xenon, Erbium, Tin, Protactinium",
            "input": "",
            "output": "Argon - Noble Gas\nIron - Metal\nCopper - Metal\nKrypton - Noble Gas\nNeon - Noble Gas\nSilver - Metal\nThorium - Metal\nRadon - Noble Gas\nHelium - Noble Gas\nXenon - Noble Gas\nErbium - Metal\nTin - Metal\nProtactinium - Metal",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 1.5889770984649658,
            "normalized": 0.3111363138471331
        },
        "potential_entropy": {
            "strategy_component": 0.36000000000000004,
            "strategy_details": {
                "raw": 0.7200000000000001,
                "normalized": 0.7200000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 854,
        "original_data": {
            "instruction": "Name some famous paintings by Van Gogh.",
            "input": "The time in Arles became one of Van Gogh's more prolific periods: he completed 200 paintings and more than 100 drawings and watercolours. He was enchanted by the local countryside and light; his works from this period are rich in yellow, ultramarine and mauve. They include harvests, wheat fields and general rural landmarks from the area, including The Old Mill (1888), one of seven canvases sent to Pont-Aven on 4 October 1888 in an exchange of works with Paul Gauguin, Émile Bernard, Charles Laval and others. The portrayals of Arles are informed by his Dutch upbringing; the patchworks of fields and avenues are flat and lacking perspective, but excel in their use of colour.\nIn March 1888 he painted landscapes using a gridded \"perspective frame\"; three of the works were shown at the annual exhibition of the Société des Artistes Indépendants. In April, he was visited by the American artist Dodge MacKnight, who was living nearby at Fontvieille. On 1 May 1888, for 15 francs per month, he signed a lease for the eastern wing of the Yellow House at 2 place Lamartine. The rooms were unfurnished and had been uninhabited for months.\nOn 7 May, Van Gogh moved from the Hôtel Carrel to the Café de la Gare, having befriended the proprietors, Joseph and Marie Ginoux. The Yellow House had to be furnished before he could fully move in, but he was able to use it as a studio. He wanted a gallery to display his work and started a series of paintings that eventually included Van Gogh's Chair (1888), Bedroom in Arles (1888), The Night Café (1888), Café Terrace at Night (September 1888), Starry Night Over the Rhone (1888), and Still Life: Vase with Twelve Sunflowers (1888), all intended for the decoration for the Yellow House.\nVan Gogh wrote that with The Night Café he tried \"to express the idea that the café is a place where one can ruin oneself, go mad, or commit a crime\". When he visited Saintes-Maries-de-la-Mer in June, he gave lessons to a Zouave second lieutenant – Paul-Eugène Milliet – and painted boats on the sea and the village. MacKnight introduced Van Gogh to Eugène Boch, a Belgian painter who sometimes stayed in Fontvieille, and the two exchanged visits in July.",
            "output": "Some famous paintings of Vincent van Gogh include Van Gogh's Chair (1888), Bedroom in Arles (1888), The Night Café (1888), Café Terrace at Night (September 1888), Starry Night Over the Rhone (1888), and Still Life: Vase with Twelve Sunflowers (1888).",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.014425277709961,
            "normalized": 0.4326929364885603
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 855,
        "original_data": {
            "instruction": "What is a byte?",
            "input": "The byte is a unit of digital information that most commonly consists of eight bits. Historically, the byte was the number of bits used to encode a single character of text in a computer and for this reason it is the smallest addressable unit of memory in many computer architectures. To disambiguate arbitrarily sized bytes from the common 8-bit definition, network protocol documents such as the Internet Protocol (RFC 791) refer to an 8-bit byte as an octet. Those bits in an octet are usually counted with numbering from 0 to 7 or 7 to 0 depending on the bit endianness. The first bit is number 0, making the eighth bit number 7.",
            "output": "A byte is a unit of digital information consisting of eight bits (typically). Historically, a byte was the number of bits needed to encode a single character of text in a computer. Because of this, a byte is the smallest unit of. memory in many computer architectures.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.424415349960327,
            "normalized": 0.549832957131522
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 856,
        "original_data": {
            "instruction": "Classify each of the following countries as either democratic or authoritarian: USA, Russia, China, Sweden, UK.",
            "input": "",
            "output": "According to the Democracy Index, compiled by the Economist Intelligence Unit (EIU) USA, Sweden and UK are fully democratic countries  whereas Russia and China belong to authoritarian regimes.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.83385968208313,
            "normalized": 0.9525313377380371
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 857,
        "original_data": {
            "instruction": "How do you play soccer?",
            "input": "",
            "output": "A standard soccer team has 11 players. Each team has 1 goalkeeper and 10 field players. Only the goalkeeper is allowed to use their hands while on the field and only in the penalty box area. The goal of the game is to move the soccer ball past the opposing team and put the ball into the opposing goal. If a player touches the ball out the sideline, the opposing team gets a thrown in from that spot. If a defending player plays the ball out the backline, the offensive team gets a corner kick. If the offensive player plays the ball out the backline, the defensive team gets a goal kick. Typically the game is played outside on a grass field. A regulation game is 90 minutes with a half time break. One main referee and two sideline referees officiate the game. Players are required to wear shin guards for protection. Outside of the United States the game is commonly known as football.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 2.574941873550415,
            "normalized": 0.5928405353001186
        },
        "potential_entropy": {
            "strategy_component": 1.2249999999999999,
            "strategy_details": {
                "raw": 2.4499999999999997,
                "normalized": 2.4499999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 858,
        "original_data": {
            "instruction": "what can you do",
            "input": "",
            "output": "As an artificial intelligence i would love to perform all the activities as required by you, few of them are as below\n1) QnA - Oh i love these queries and answers related stuff on any topic your heart desires\n2) Language - I can अनुवाद from 任何 மொழி to ਕੋਈ ਵੀ ਭਾਸ਼ਾ (translate from any language to any language ;) )\n3) Casual chat - oh am good at chatting, i can keep chatting, i would love to chat, I love to chat. I can do this all day, you get it, let me stop.\n4) Technical Assistance - Alpha, beta, java, SQL, coding, support - you name it i will help on it.\n\nThere are many apart from this, my capabilities are constantly expanding and evolving as I'm being continuously trained on more and more data(i like to eat also while chatting ;) )",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.7599048614501953,
            "normalized": 0.9314013889857701
        },
        "potential_entropy": {
            "strategy_component": 1.2475,
            "strategy_details": {
                "raw": 2.495,
                "normalized": 2.495,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.85,
                    "output": 0.405
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 859,
        "original_data": {
            "instruction": "Given this paragraph about the Japan Rail Pass, tell me whether it is available for purchase to Japanese citizens.",
            "input": "The Japan Rail Pass, also called the JR Pass, is a rail pass for overseas visitors sold by the Japan Railways Group, and is valid for travel on all major forms of transportation provided by the JR Group in Japan, with a few exceptions. The Rail Pass is designed to stimulate travel and tourism throughout the country. It is only cost effective for long-distance travel, particularly by bullet train. While the savings from extensive travel can be considerable, those who travel too little may in fact lose money on a rail pass. The Japan Rail Pass is of limited use within larger cities, as private operators generally do not accept the Rail Pass. In Tokyo, for instance, it covers the Yamanote Line which goes to several popular tourist areas, as well as in Osaka on the Osaka Loop Line, plus in Kyoto on the Nara Line and Sagano Line.",
            "output": "No, the JR Pass is only intended for purchase by overseas visitors to Japan.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.710701942443848,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.36250000000000004,
            "strategy_details": {
                "raw": 0.7250000000000001,
                "normalized": 0.7250000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 860,
        "original_data": {
            "instruction": "How may I find a charging port outdoors for my laptop?",
            "input": "",
            "output": "There are many ways to find a charging port outdoors for your laptop. For example, one may find charging ports in cafes, although usually those are reserved for paying customers. It's not uncommon to pay for a cup of latte and charge your laptop at the same time. Charging ports are also commonly seen in public places, such as public libraries, city hall, and parks. Public transportation is another way to get your laptop charged, as many trains have charging ports near the seats.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.9713923931121826,
            "normalized": 0.7061121123177665
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 861,
        "original_data": {
            "instruction": "Extract the different ways of dealing with underfitting mentioned in the text",
            "input": "Resolving underfitting\nThere are multiple ways to deal with underfitting:\n\n1) Increase the complexity of the model: If the model is too simple, it may be necessary to increase its complexity by adding more features, increasing the number of parameters, or using a more flexible model. However, this should be done carefully to avoid overfitting.\n2) Use a different algorithm: If the current algorithm is not able to capture the patterns in the data, it may be necessary to try a different one. For example, a neural network may be more effective than a linear regression model for some types of data.\n3) Increase the amount of training data: If the model is underfitting due to lack of data, increasing the amount of training data may help. This will allow the model to better capture the underlying patterns in the data.\n4) Regularization: Regularization is a technique used to prevent overfitting by adding a penalty term to the loss function that discourages large parameter values. It can also be used to prevent underfitting by controlling the complexity of the model.\n5) Ensemble Methods: Ensemble methods combine multiple models to create a more accurate prediction. This can help to reduce underfitting by allowing multiple models to work together to capture the underlying patterns in the data.\n6) Feature engineering: Feature engineering involves creating new model features from the existing ones that may be more relevant to the problem at hand. This can help to improve the accuracy of the model and prevent underfitting.",
            "output": "The different ways of deal with underfitting mentioned in the text are:\n- Increase the complexity of the model\n- Use a different algorithm\n- Increase the amount of data\n- Regularization\n- Ensemble Methods\n- Feature engineering",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.670224905014038,
            "normalized": 0.6200642585754395
        },
        "potential_entropy": {
            "strategy_component": 0.3405,
            "strategy_details": {
                "raw": 0.681,
                "normalized": 0.681,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.17099999999999999
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 862,
        "original_data": {
            "instruction": "What are the best rock bands that were part of the British invasion?",
            "input": "",
            "output": "While tastes may differ, the top answers are generally the Rolling Stones, The Beatles and The Who. There are others on many lists, for example The Kinks.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 4.012843132019043,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.735,
            "strategy_details": {
                "raw": 1.47,
                "normalized": 1.47,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 863,
        "original_data": {
            "instruction": "Given a reference text about Isabel Emily Jay, tell me where she studied and how old she was when she retired.",
            "input": "Isabel Emily Jay (17 October 1879 – 26 February 1927) was an English opera singer and actress, best known for her performances in soprano roles of the Savoy Operas with the D'Oyly Carte Opera Company and in Edwardian musical comedies. During Jay's career, picture postcards were immensely popular, and Jay was photographed for over 400 different postcards.\n\nAfter studying at the Royal Academy of Music, Jay joined the D'Oyly Carte Opera Company in 1897, with whom she began singing principal roles immediately, becoming the company's leading soprano in 1899, where she played leading roles in comic operas including The Rose of Persia, The Pirates of Penzance, Patience, The Emerald Isle and Iolanthe. She married and left the company in 1902. She returned to the West End stage in 1903 and starred in eleven musicals over the ensuing years, including A Country Girl, The Cingalee, Véronique, The White Chrysanthemum, The Girl Behind the Counter, Miss Hook of Holland, King of Cadonia and The Balkan Princess. She retired in 1911 at age 31.",
            "output": "Isabel Emily Jay studied at the Royal Academy of Music and retired at 31 years old.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.670011043548584,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 864,
        "original_data": {
            "instruction": "What did José María Arizmendiarrieta do?",
            "input": "In 1941, a young Catholic priest, José María Arizmendiarrieta settled in Mondragón, a town with a population of 7,000 that had not yet recovered from the poverty, hunger, exile, and tension of the Spanish Civil War. In 1943, Arizmendiarrieta established a technical college which became a training ground for managers, engineers and skilled labour for local companies, and primarily for the co-operatives.",
            "output": "José María Arizmendiarrieta was a Catholic Preist who settled in Mondragón, a town with a population of 7,000 that had not yet recovered from the poverty, hunger, exile, and tension of the Spanish Civil War.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.55,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.322901725769043,
            "normalized": 0.8065433502197266
        },
        "potential_entropy": {
            "strategy_component": 0.65,
            "strategy_details": {
                "raw": 1.3,
                "normalized": 1.3,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.4,
                    "out_div": 0.5,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 865,
        "original_data": {
            "instruction": "Where is the best place to view the Northern Lights in Europe?",
            "input": "",
            "output": "In Europe most people travel to Iceland, Gleenland, Norway, Finland or Sweden with the hope of seeing the Northern Lights. The Northern Lights only tend to be visible between the months of September and April each year. Clear weather is required with little cloud coverage to get the best view.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.4885048866271973,
            "normalized": 0.8538585390363421
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 866,
        "original_data": {
            "instruction": "Given this paragraph about video games, when was the Legend of Zelda originally released?",
            "input": "The Legend of Zelda, originally released in Japan as The Hyrule Fantasy: Zelda no Densetsu, is a 1986 action-adventure game developed and published by Nintendo. The first game of The Legend of Zelda series, it is set in the fantasy land of Hyrule and centers on an elf-like boy named Link, who aims to collect the eight fragments of the Triforce of Wisdom in order to rescue Princess Zelda from the antagonist Ganon. During the course of the game, the player controls Link from a top-down perspective and navigates throughout the overworld and dungeons, collecting weapons, defeating enemies and uncovering secrets along the way.",
            "output": "1986",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.403173446655273,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 867,
        "original_data": {
            "instruction": "What are the 9 locations of Dick's Drive In restaurants?",
            "input": "Founders Dick Spady, H. Warren Ghormley, and Dr. B. O. A. Thomas opened the first Dick's on January 28, 1954, in Seattle's Wallingford neighborhood, on N.E. 45th Street. A grand opening was held on February 20, 1954. In 1955, a second Dick's was opened in Seattle's Capitol Hill district. This was followed by a third in 1960 in the Crown Hill neighborhood, a 4th in 1963 in Lake City, a fifth in Queen Anne in 1974. All but the Queen Anne location are without customer seating. The Queen Anne location has indoor tables and no drive-in.\n\nThe simple menu has changed little over time. It features fast-food staples such as hamburgers, hand-cut french fries, and individually made milkshakes. Dick's is particularly well known for the \"Dick's Deluxe,\" which includes lettuce, mayonnaise, and chopped pickles. No substitutions are allowed and all burgers are cooked to well done. For most of Dick's history, the only available omissions were the Deluxe without cheese or fries without salt. More recent menu changes, however, allow ordering plain versions of the hamburger and cheeseburger.\n\nA black-and-white photograph of the Capitol Hill location of Dick's Drive-In, circa 1955.\nThe Capitol Hill location in 1955\nFor several years Dick's has offered employee benefits such as a 50% matched 401(k), 100% employer-paid medical insurance, and a college tuition scholarship (currently at $28,000) accessible after six months of employment. In 2013, Dick's Drive-In was voted \"the most life-changing burger joint in America\" in an Esquire.com poll.\n\nIn September 2010, it was announced that Dick's was planning on opening a new sixth location in the Seattle area and that an online poll on their website would determine the new location. After a few weeks of polling, the area north of Seattle won the right for a new Dick's Drive-In. On October 15, 2010, Dick's officials announced the new location to be in Edmonds on the corner of Hwy 99 and 220th St. On October 20, 2011, the 6th location in Edmonds opened to the public. The opening occurred multiple weeks ahead of schedule.\n\nIn 2017, Dick's launched another poll to determine its seventh location, which would be located either on the Eastside or in South King County. Over 177,000 participants cast their votes, with the majority favoring the South region. Locations being considered included Kent, West Seattle, South Seattle, Renton, Burien, SeaTac, Tukwila, Auburn, Normandy Park, Des Moines and Federal Way. After an extensive amount of time, it was announced on September 7, 2017, that the 7th location to the chain would be located in Kent on Highway 99, 5 miles (8 km) south of Sea-Tac Airport. The location opened on December 12, 2018.\n\nIn September 2020, Dick's announced that it would launch a food truck to serve five locations in Western Washington that were chosen through a public poll: Bellevue, Bellingham, Everett, Renton, and West Seattle. The restaurant also announced that it would be expanding to the Eastside once a suitable location is found. A collaboration with local chef Edouardo Jordan of Salare and JuneBaby was also announced with a three-day popup event at the Queen Anne location. In December 2021, Dick's opened an Eastside location at the Crossroads Shopping Center in Bellevue. Dick's then announced on April 28, 2022, that it would open a ninth location at The Commons shopping center in Federal Way in 2023.",
            "output": "The nine locations, once the final location is completed in 2023, are as follows: Wallingford, Seattle; Capitol Hill, Seattle; Crown Hill, Seattle; Queen Anne, Seattle; Edmonds; Kent; Bellevue; and Federal Way.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5478310585021973,
            "normalized": 0.8708088738577706
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 868,
        "original_data": {
            "instruction": "Is it important to have friends?",
            "input": "",
            "output": "Yes, it is important to have friends. Friends can provide camaraderie, support, guidance and encourage you to think in new and different ways by challenging your beliefs and assumptions. Many people have different types or categories of friends. For example, some friends may be individuals with whom specific activities are shared such as music or sports. Other friends may be individuals from whom you seek advice on complex personal decisions. Another category or type of friend may be one or more individuals with whom you can share almost anything without fear of embarrassment or being judged. It is particularly important to have an active social life as one gets older. Positive social interactions help older adults avoid loneliness and stay healthy.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.15,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.8002443313598633,
            "normalized": 0.657212666102818
        },
        "potential_entropy": {
            "strategy_component": 1.02,
            "strategy_details": {
                "raw": 2.04,
                "normalized": 2.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.8,
                    "inp_complex": 0.85,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 869,
        "original_data": {
            "instruction": "Which is a species of fish? Bream or Cream",
            "input": "",
            "output": "Bream",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.1,
            "s_inp_complex": 0.05,
            "s_out_cot": 0.7,
            "s_out_div": 0.2,
            "s_out_dens": 0.8,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.95,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.8,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 10.927216529846191,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.405,
            "strategy_details": {
                "raw": 2.81,
                "normalized": 2.81,
                "components": {
                    "instruction": 0.3,
                    "input": 1.85,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.9,
                    "inp_complex": 0.95,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.8,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 870,
        "original_data": {
            "instruction": "How many full siblings did Fatemeh Pahlavi have?",
            "input": "Fatemeh Pahlavi was born in Tehran on 30 October 1928. She was the tenth child of Reza Shah and his fourth and last wife, Esmat Dowlatshahi. Her mother was from the Qajar dynasty and married Reza Shah in 1923. Fatemeh was the full-sister of Abdul Reza Pahlavi, Ahmad Reza Pahlavi, Mahmoud Reza Pahlavi and Hamid Reza Pahlavi.",
            "output": "4 siblings: Abdul Reza Pahlavi, Ahmad Reza Pahlavi, Mahmoud Reza Pahlavi and Hamid Reza Pahlavi.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.5938093662261963,
            "normalized": 0.5982312474931989
        },
        "potential_entropy": {
            "strategy_component": 0.42000000000000004,
            "strategy_details": {
                "raw": 0.8400000000000001,
                "normalized": 0.8400000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 871,
        "original_data": {
            "instruction": "Which NFL teams have gone to the Super Bowl at least once but have never won a Super Bowl title?",
            "input": "",
            "output": "Buffalo Bills, Minnesota Vikings, Cincinnati Bengals, Atlanta Falcons, Carolina Panthers, Houston/Tennessee/Oilers/Titans, San Diego/Los Angeles Chargers, St. Louis/Phoenix/Arizona Cardinals",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.836216926574707,
            "normalized": 0.953204836164202
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 872,
        "original_data": {
            "instruction": "Who first used the term \"glass ceiling\"?",
            "input": "A glass ceiling is a metaphor usually applied to women, used to represent an invisible barrier that prevents a given demographic from rising beyond a certain level in a hierarchy. No matter how invisible the glass ceiling is expressed, it is actually a difficult obstacle to overcome. The metaphor was first used by feminists in reference to barriers in the careers of high-achieving women. It was coined by Marilyn Loden during a speech in 1978. In the United States, the concept is sometimes extended to refer to racial inequality in the United States. Minority women in white-majority countries often find the most difficulty in \"breaking the glass ceiling\" because they lie at the intersection of two historically marginalized groups: women and people of color. East Asian and East Asian American news outlets have coined the term \"bamboo ceiling\" to refer to the obstacles that all East Asian Americans face in advancing their careers. Similarly, a multitude of barriers that refugees and asylum seekers face in their search for meaningful employment is referred to as canvas ceiling.",
            "output": "The term \"glass ceiling\" was first used by Marilyn Loden in a 1978 speech.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.684584140777588,
            "normalized": 0.9098811830793109
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 873,
        "original_data": {
            "instruction": "Why do people travel to different places.",
            "input": "",
            "output": "People may travel to just explore the world, see touristic highlights and experience different cultures. For example there are many tourists visiting my home town Cologne to visit the famous cathedral, ancient roman, and medieval remains and the famous romanic churches. Tasting the local beer in the old town is also a famous attraction.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.2247185707092285,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 874,
        "original_data": {
            "instruction": "What is a PFD?",
            "input": "Process engineering involves the utilization of multiple tools and methods. Depending on the exact nature of the system, processes need to be simulated and modeled using mathematics and computer science. Processes where phase change and phase equilibria are relevant require analysis using the principles and laws of thermodynamics to quantify changes in energy and efficiency. In contrast, processes that focus on the flow of material and energy as they approach equilibria are best analyzed using the disciplines of fluid mechanics and transport phenomena. Disciplines within the field of mechanics need to be applied in the presence of fluids or porous and dispersed media. Materials engineering principles also need to be applied, when relevant.\n\nManufacturing in the field of process engineering involves an implementation of process synthesis steps. Regardless of the exact tools required, process engineering is then formatted through the use of a process flow diagram (PFD) where material flow paths, storage equipment (such as tanks and silos), transformations (such as distillation columns, receiver/head tanks, mixing, separations, pumping, etc.) and flowrates are specified, as well as a list of all pipes and conveyors and their contents, material properties such as density, viscosity, particle-size distribution, flowrates, pressures, temperatures, and materials of construction for the piping and unit operations.",
            "output": "PFD is an abbreviation for process flow diagram.\n\nProcess flow diagrams specify material flow paths, storage equipment (such as tanks and silos), transformations (such as distillation columns, receiver/head tanks, mixing, separations, pumping, etc.), and flowrates. They also list all pipes and conveyors and their contents, material properties such as density, viscosity, particle-size distribution, flowrates, pressures, temperatures, and materials of construction for the piping and unit operations.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.399240732192993,
            "normalized": 0.8283544949122837
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 875,
        "original_data": {
            "instruction": "Which is a species of fish? Tope or Rope",
            "input": "",
            "output": "Tope",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 11.593605995178223,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 876,
        "original_data": {
            "instruction": "What is the standard tuning for a 6 string guitar?",
            "input": "",
            "output": "From the highest tone to the lowest, the tuning of the 6 strings is E B G D A E.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.6363089084625244,
            "normalized": 0.8960882595607212
        },
        "potential_entropy": {
            "strategy_component": 0.3925,
            "strategy_details": {
                "raw": 0.785,
                "normalized": 0.785,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.5,
                    "output": 0.165
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 877,
        "original_data": {
            "instruction": "Which is a species of fish? Banjo or Guitar",
            "input": "",
            "output": "Banjo",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.3,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.7,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 11.29989242553711,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3,
            "strategy_details": {
                "raw": 2.6,
                "normalized": 2.6,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.7,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 878,
        "original_data": {
            "instruction": "Write a guide for how to catch a smallmouth bass.",
            "input": "",
            "output": "The smallmouth bass is a common freshwater fish, found in the rivers and lakes of North America. Its habitat stretches between northeastern Canada and a small range of waterways in Mexico. Fishermen of all ages enjoy catching the smallmouth bass, because of its propensity for jumping out of the water and trying to evade capture with vigor. You will find smallmouth enjoying shallower waters in the cool parts of the day, attracted to surface baits like imitation frogs, minnows, and spinnerbaits. As the day warms up, smallmouth retreat to cooler waters, and you might find them schooling near a drop-off or around a deep-water mound. Smallmouth are typically caught between June and October.\n\nNote: Please adhere to local fishing regulations to know when smallmouth bass are in season. Most jurisdictions require a license to fish and have limits on how many fish can be kept.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.4209017753601074,
            "normalized": 0.8345433643886021
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 879,
        "original_data": {
            "instruction": "Can you suggests some ways to get more organized and complete my tasks without missing any deadlines?",
            "input": "",
            "output": "Here are some ways you can get more organized, 1. Create a list of tasks to be completed 2. Prioritize these tasks 3. Pick the highest priority tasks which can be completed within less than 30 mins and complete them 4. Block off a time in your calendar to complete the high priority tasks that require more time 5. Delegate as many of the low priority tasks. 6. Review the list of tasks to be completed at the beginning of each day and keep tracking them until completed.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.506448745727539,
            "normalized": 0.5732710702078683
        },
        "potential_entropy": {
            "strategy_component": 0.75,
            "strategy_details": {
                "raw": 1.5,
                "normalized": 1.5,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 880,
        "original_data": {
            "instruction": "What city has Kogoshima as its airport",
            "input": "",
            "output": "Tokyo",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.0,
            "s_out_div": 0.1,
            "s_out_dens": 0.1,
            "s_out_bg": 0.0
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 1.0,
            "s_out_div": 0.9,
            "s_out_dens": 0.9,
            "s_out_bg": 1.0
        },
        "final_strategy_mark": [
            1,
            2,
            0
        ],
        "entropy": {
            "raw": 12.264516830444336,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.54,
            "strategy_details": {
                "raw": 3.08,
                "normalized": 3.08,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 1.14
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 1.0,
                    "out_div": 0.9,
                    "out_dens": 0.9,
                    "out_bg": 1.0
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 881,
        "original_data": {
            "instruction": "What is the difference between a microservice and an API?",
            "input": "",
            "output": "Microservices is an approach to building an application that breaks its functionality into modular components. APIs are part of an application that communicates with other applications.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.0939884185791016,
            "normalized": 0.7411395481654576
        },
        "potential_entropy": {
            "strategy_component": 0.605,
            "strategy_details": {
                "raw": 1.21,
                "normalized": 1.21,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.27
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 882,
        "original_data": {
            "instruction": "Classify each of the following as a drink or food: coffee, apple, water, soda, banana, pineapple, champagne",
            "input": "",
            "output": "Coffee: Drink\nApple: Food\nWater: Drink\nSoda: Drink\nBanana: Food\nPineapple: Food\nChampagne: Drink",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.2512407302856445,
            "normalized": 0.500354494367327
        },
        "potential_entropy": {
            "strategy_component": 0.48500000000000004,
            "strategy_details": {
                "raw": 0.9700000000000001,
                "normalized": 0.9700000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 883,
        "original_data": {
            "instruction": "Which is a species of fish? Golden Retriever or Goldfish",
            "input": "",
            "output": "Goldfish",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.05
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.95
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 14.48039436340332,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.27,
            "strategy_details": {
                "raw": 2.54,
                "normalized": 2.54,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.6
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.95
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 884,
        "original_data": {
            "instruction": "Is there a movie about Wikipedia",
            "input": "Truth in Numbers? Everything, According to Wikipedia is a 2010 American documentary film that explores the history and cultural implications of the online, editable encyclopedia Wikipedia. The film considers the question of whether all individuals or just experts should be tasked with editing an encyclopedia.\n\nThe site's history and background is given, along with commentary from Wikipedia founders Jimmy Wales and Larry Sanger. Commentators that appear in the film include author Howard Zinn, Len Downie of The Washington Post, Bob Schieffer of CBS News, former Encyclopædia Britannica chief Robert McHenry and former Central Intelligence Agency director James Woolsey. The documentary discusses incidents that shed a negative light on Wikipedia, including the Essjay controversy and the Wikipedia biography controversy.\n\nThe long-delayed film premiered at Wikimania 2010 in Gdańsk in July 2010, and was screened at the Paley Center for Media in New York City on October 20, 2010. It was shown as part of the Savannah Film Festival on November 3, 2010, at Savannah College of Art and Design's Trustees Theater. Truth in Numbers? received a mixed reception, with favorable commentary from author Ted Leonsis, in the AOL publication Urlesque, and coverage at the Savannah Film Festival by Carlos Serrano of District.",
            "output": "Yes. Truth in Numbers? Everything, According to Wikipedia is a 2010 American documentary film that explores the history and cultural implications of the online, editable encyclopedia Wikipedia. The film considers the question of whether all individuals or just experts should be tasked with editing an encyclopedia.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.946415901184082,
            "normalized": 0.9846902574811663
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 885,
        "original_data": {
            "instruction": "In the series A Song of Ice and Fire, who is the founder of House Crakehall?",
            "input": "",
            "output": "Crake the Boarkiller",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 10.206045150756836,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 886,
        "original_data": {
            "instruction": "What is the best surface to run on?",
            "input": "",
            "output": "Running is a great exercise, but not all running surfaces are created equal. Traditionally, those who run \"runners\" will run 5-7 days a week. Runners will average between 3 miles per run up to 20 miles per run. While running is great exercise and very efficient at burning calories, it can be hard on the body. Specifically the leg joints. The average runner will strike the surface they are running on 1,480 times per mile. So if you are running 10 miles, thats just under 15,000 foot strikes. This repetitive striking can deteriorate the body joints over time. Therefore, the type of surface a runner runs on, can have a huge impact on the body. Some surfaces are harder than others, the harder the surface, the harder on the body, legs and joints. Concrete is harder than asphalt, asphalt is harder than a dirt trail and a dirt trail is harder than a high school track surface. Example, concrete can be 30% harder surface than asphalt. Running is a great exercise and millions of people enjoy running. But if you want to run long, as in for a long time, choose your running surface for the long run. Because the surface you choose to run on, can have a direct correlation to how long your body and leg joints will last.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.643192768096924,
            "normalized": 0.6123407908848354
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 887,
        "original_data": {
            "instruction": "Does Niksar have a castle?",
            "input": "Niksar, historically known as Neocaesarea (Νεοκαισάρεια), is a city in Tokat Province, Turkey. It was settled by many empires, being once the capital city of the province. Niksar is known as \"Çukurova of North-Anatolia\" due to its production of many kinds of fruits and vegetables. On May 2, 2018, Niksar was included in the World Heritage tentative list",
            "output": "Yes, Niksar has a historic castle overlooking the old city. It is used during Roman, Byzantium and Ottoman times.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 4.926168441772461,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3405,
            "strategy_details": {
                "raw": 0.681,
                "normalized": 0.681,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.17099999999999999
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 888,
        "original_data": {
            "instruction": "How do I obtain help for an adult relative with a serious mental illness?",
            "input": "",
            "output": "In order to obtain assistance, it is critical that the individual receive a medical diagnosis from a professional mental health provider.  For adults, this is complicated by the fact that every adult individual has the right to refuse medical care.  A person suffering from a severe mental illness will often refuse care due to their delusions and paranoia.  Frequently, concerned family members must wait until the relative has a serious psychotic episode at which time they can contact emergency services and have the individual evaluated as a threat to themselves or others. At this point, it might be possible for the government to compel the individual to be diagnosed and receive mental health treatment.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 3.114619255065918,
            "normalized": 0.7470340728759766
        },
        "potential_entropy": {
            "strategy_component": 1.285,
            "strategy_details": {
                "raw": 2.57,
                "normalized": 2.57,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 889,
        "original_data": {
            "instruction": "Describe what your ideal pet cat would look like, and what you would name it.",
            "input": "",
            "output": "My ideal cat would be a long haired ginger Maine Coon, with a beautifully fluffy coat that would be super soft to pet and bright green eyes. I would name him Rory, because that is a name that means Red Haired King, and in my household we treat pets as royalty. He would be super cuddly but also love to play with cat toys, and I would buy him a cat tower to watch the birds from.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 1.0,
            "s_out_div": 0.9,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.0,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.980668067932129,
            "normalized": 0.9944765908377511
        },
        "potential_entropy": {
            "strategy_component": 0.32250000000000006,
            "strategy_details": {
                "raw": 0.6450000000000001,
                "normalized": 0.6450000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.0,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 890,
        "original_data": {
            "instruction": "Can you summarize the responsibilities of a Chief Data Officer?",
            "input": "A chief data officer (CDO) is a corporate officer responsible for enterprise-wide governance and utilization of information as an asset, via data processing, analysis, data mining, information trading and other means. CDOs usually report to the chief executive officer (CEO), although depending on the area of expertise this can vary. The CDO is a member of the executive management team and manager of enterprise-wide data processing and data mining.\n\nThe chief data officer title shares its abbreviation with the chief digital officer, but the two are not the same job. The chief data officer has a significant measure of business responsibility for determining what kinds of information the enterprise will choose to capture, retain and exploit and for what purposes. However, the similar-sounding chief digital officer or chief digital information officer often does not bear that business responsibility, but rather is responsible for the information systems through which data is stored and processed. A chief data officer's purpose is to connect the technological results to the needed business results. Various other roles entail having an understanding of the business value. It means using data to derive business outcome. It can be achieved by knowing the team members and activities performed, the stakeholder values and understanding customer needs. Some responsibilities include the governance, advising & monitoring enterprise data. In terms of operations it means enabling data usability along with efficiency and availability. They have to innovate which means driving the business towards digital transformation innovation, cost reduction, and revenue generation. Their role is also to provide supporting analytics with reports on products, customers, operations, and markets. They need to protect the data and eliminate data territorialism while also promoting data ethics.",
            "output": "Below are the key responsibilities of a Chief Data Officer,\nMaking sure the business can achieve its results by effective use of data processed within the enterprise.\nEstablish enterprise-wide governance on data collection, data processing, data analysis, and mining and sharing of data across teams and outside the enterprise.\nGuiding teams within the enterprise on best practices, compliance standards, and company responsibility in collecting and using data.\nEstablish monitoring standards for making sure how data is accessed, who access the data, and having a catalog of all data assets.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.7023465633392334,
            "normalized": 0.9149561609540667
        },
        "potential_entropy": {
            "strategy_component": 0.39749999999999996,
            "strategy_details": {
                "raw": 0.7949999999999999,
                "normalized": 0.7949999999999999,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.29999999999999993,
                    "output": 0.315
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 891,
        "original_data": {
            "instruction": "Where does the word plumbing originate from?",
            "input": "",
            "output": "\"Plumbing\" originates from the original name for Lead (Pb), Plumbum, which is what most pipes were made from at the time.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.8347253799438477,
            "normalized": 0.9527786799839565
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 892,
        "original_data": {
            "instruction": "From the passage provided, extract the entities which bestowed an award on Jensen Huang.",
            "input": "In 1999, Jensen Huang was named Entrepreneur of the Year in High Technology by Ernst & Young.[citation needed] In 2003, Huang received the Dr. Morris Chang Exemplary Leadership Award, which recognizes a leader who has made exceptional contributions to driving the development, innovation, growth, and long-term opportunities of the fabless semiconductor industry, from the Fabless Semiconductor Association. He was also a National Finalist for the EY Entrepreneur of the Year Award in 2003 and was an Award Recipient for the Northern California region in 1999.[citation needed]\n\nAdditionally, Huang is a recipient of the Daniel J. Epstein Engineering Management Award from the University of Southern California and was named an Alumni Fellow by Oregon State University. Huang was awarded an honorary doctorate from Oregon State University at the June 13, 2009, commencement ceremony.\n\nIn 2018, Huang was listed in the inaugural Edge 50, naming the world's top 50 influencers in edge computing. In October 2019, Harvard Business Review named Jensen Huang best-performing CEO in the world. In November 2020, Jensen Huang was named \"Supplier CEO of the year\" by Eurostars AutomotiveNewsEurope. Huang was awarded an honorary doctorate from National Taiwan University at the school anniversary in November, 2020.[better source needed] In August 2021, the Semiconductor Industry Association (SIA) announced that Jensen Huang is the 2021 recipient of the industry’s highest honor, the Robert N. Noyce Award. In September 2021, he was included in the Time 100, Time's annual list of the 100 most influential people in the world.",
            "output": "Jensen Huang received awards from: 1) Ernst & Young, 2) Fabless Semiconductor Association, 3) EY (Ernst & Young), 4) University of Southern California, 5) Oregon State University, 6) Harvard Business Review, 7) Eurostars AutomotiveNewsEurope, 8) National Taiwan University, 9) Semiconductor Industry Association, 10) Time magazine.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.7683513164520264,
            "normalized": 0.6481003761291504
        },
        "potential_entropy": {
            "strategy_component": 0.32000000000000006,
            "strategy_details": {
                "raw": 0.6400000000000001,
                "normalized": 0.6400000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 893,
        "original_data": {
            "instruction": "List all the Formula 1 teams that Sergio Perez has driven for",
            "input": "Sergio Michel \"Checo\" Pérez Mendoza (Spanish: [ˈseɾxjo ˈpeɾes] (listen); born 26 January 1990), is a Mexican racing driver who races in Formula One for Red Bull Racing, having previously driven for Sauber, McLaren, Force India, and Racing Point. He won his first Formula One Grand Prix at the 2020 Sakhir Grand Prix, breaking the record for the number of starts before a race win at 190. He also earned his first Formula One pole position at the 2022 Saudi Arabian Grand Prix, breaking the record for the most races before a first pole position at 215. Pérez has a contract with Red Bull Racing until the end of the 2024 season.",
            "output": "- Sauber\n- McLaren\n- Force India\n- Racing Point\n- Red Bull Racing",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.680792808532715,
            "normalized": 0.6230836595807757
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 894,
        "original_data": {
            "instruction": "How do you charge a phone",
            "input": "",
            "output": "This depends on the phone brand. Most modern phones have wireless charging that allows phones to be placed on a charging mat or pad. Some phones require you to plug them in. Plugs for phones vary by manufacturer. Apple for instance has used a proprietary charging system called lightning for years but recently was sued and now will have to comply with an industry standard of using USB-C to charge their phones.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.4031786918640137,
            "normalized": 0.829479626246861
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 895,
        "original_data": {
            "instruction": "What is the launch date of Artemis mission?",
            "input": "During preliminary reviews in 2011, the launch date of Artemis was placed somewhere between 2019 and 2021, but afterwards the launch date was delayed to 2023 on a Space Launch System (SLS) launch vehicle. Although, as of March 2023, the mission is pegged for a November 2024 launch, the need to recycle and refurbish components from Orion that flew on Artemis 1 may push this back to later in 2024 or the first quarter of 2025.",
            "output": "Artemis launch date is November 2024 as of now but it’s likely to change.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.058165073394775,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4624999999999999,
            "strategy_details": {
                "raw": 0.9249999999999998,
                "normalized": 0.9249999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 896,
        "original_data": {
            "instruction": "Is Davos worth traveling?",
            "input": "Davos is an Alpine resort town and a municipality in the Praettigau/Davos Region in the canton of Graubuenden, Switzerland. It has a permanent population of 10,832 (2020). Davos is located on the river Landwasser, in the Rhaetian Alps, between the Plessur and Albula Ranges.\n\nThe municipality covers nearly the entire valley of the Landwasser, and the centre of population, economic activity, and administration is two adjacent villages: Davos Dorf (German: Davos Village) and Davos Platz (Davos Place), at 1,560 m (5,120 ft) above sea level.\n\nGaining prominence in the 19th century as a mountain health resort, Davos is perhaps best known today for hosting the World Economic Forum—often referred to simply as \"Davos\"—an annual meeting of global political and corporate leaders. With a long history of winter sport, Davos also has one of Switzerland's largest ski resorts, and hosts the international Spengler Cup ice hockey tournament each December.\n\nhttps://en.wikipedia.org/wiki/Davos",
            "output": "Davos is a very scenic town in the Swiss Alps. Famous for winter sports like skiing, cross-country skiing, sledding, ice hockey and ice skating and also for the World Economic Forum that take place annually.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.992938995361328,
            "normalized": 0.7122682843889508
        },
        "potential_entropy": {
            "strategy_component": 0.43249999999999994,
            "strategy_details": {
                "raw": 0.8649999999999999,
                "normalized": 0.8649999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 897,
        "original_data": {
            "instruction": "Classify each of the following as either a brand of kombucha or a soda brand: Humm, Brew Doctor, Pepsi, 7Up, Mountain Dew, Coca-Cola, Sprite, Fanta, GT's, Remedy, Health-Ade",
            "input": "",
            "output": "Kombucha: Humm, Brew Doctor, GT's, Remedy, Health-Ade \nSoda: Pepsi, 7Up, Mountain Dew, Coca-Cola, Sprite, Fanta",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 4.374670028686523,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 898,
        "original_data": {
            "instruction": "Battles between England and Scotland",
            "input": "Given the paragraph about battles between England and Scotland, which battles occurred between both countries",
            "output": "The Kingdom of England and the Kingdom of Scotland fought dozens of battles with each other. They fought typically over land, and the Anglo-Scottish border frequently changed as a result. Prior to the establishment of the two kingdoms, in the 10th and 9th centuries, their predecessors, the Northumbrians, Picts and Dal Riatans, also fought a number of battles. Major conflicts between the two parties include the Wars of Scottish Independence (1296–1357), and the Rough Wooing (1544–1551), as well as numerous smaller campaigns and individual confrontations. In 1603, England and Scotland were joined in a \"personal union\" when King James VI of Scotland succeeded to the throne of England as King James I. War between the two states largely ceased, although the Wars of the Three Kingdoms in the 17th century, and the Jacobite risings of the 18th century, are sometimes characterised as Anglo-Scottish conflicts.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.5567896366119385,
            "normalized": 0.5876541818891253
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 899,
        "original_data": {
            "instruction": "Write a short paragraph about why you should not have both a pet cat and a pet bird.",
            "input": "",
            "output": "Having both a pet bird and a pet cat is not a good idea because cats will try to hunt birds, which are naturally the prey of cats. This hunting instinct is completely normal, and it isn't fair to the cat to try to train it not to attack birds, as it is simply in its nature to do so. However, letting a cat hunt your pet bird is similarly not humane, as it will be very stressful to the bird and could lead to the bird being injured. Larger birds are also not a good idea, as the bird could cause injury to the cat if the cat tries to attack it. Overall, it just isn't a good idea to have both cats and birds, since they tend not to coexist well, and pet owners should just pick one species to have.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8431599140167236,
            "normalized": 0.6694742611476353
        },
        "potential_entropy": {
            "strategy_component": 0.52,
            "strategy_details": {
                "raw": 1.04,
                "normalized": 1.04,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 900,
        "original_data": {
            "instruction": "Should I pursue a degree after graduating high school?",
            "input": "",
            "output": "Whether or not you pursue a degree after graduating high school is an important but complex question that depends on a variety of factors. When assessing what the right choice is for you, it is important to think about the career you'd like to pursue. If you enjoy classwork and can afford a college education, that is one route which may provide opportunities to earn a higher salary later in life. However, if you prefer being active and would like to begin earning money immediately after graduating high school without accumulating significant debt, forgoing an advanced degree to begin work can be a great option. At the end of the day, pursuing something that you enjoy spending time on is the most important aspect of this decision and will be a better indicator of both satisfaction and success than whether or not you earn an advanced degree.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.6940059661865234,
            "normalized": 0.6268588474818638
        },
        "potential_entropy": {
            "strategy_component": 0.8350000000000001,
            "strategy_details": {
                "raw": 1.6700000000000002,
                "normalized": 1.6700000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.1,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 901,
        "original_data": {
            "instruction": "Extract the steps to resolve a given supervised machine learning problem from the following text",
            "input": "To solve a given problem of supervised learning, one has to perform the following steps:\n\nDetermine the type of training examples. Before doing anything else, the user should decide what kind of data is to be used as a training set. In the case of handwriting analysis, for example, this might be a single handwritten character, an entire handwritten word, an entire sentence of handwriting or perhaps a full paragraph of handwriting.\nGather a training set. The training set needs to be representative of the real-world use of the function. Thus, a set of input objects is gathered and corresponding outputs are also gathered, either from human experts or from measurements.\nDetermine the input feature representation of the learned function. The accuracy of the learned function depends strongly on how the input object is represented. Typically, the input object is transformed into a feature vector, which contains a number of features that are descriptive of the object. The number of features should not be too large, because of the curse of dimensionality; but should contain enough information to accurately predict the output.\nDetermine the structure of the learned function and corresponding learning algorithm. For example, the engineer may choose to use support-vector machines or decision trees.\nComplete the design. Run the learning algorithm on the gathered training set. Some supervised learning algorithms require the user to determine certain control parameters. These parameters may be adjusted by optimizing performance on a subset (called a validation set) of the training set, or via cross-validation.\nEvaluate the accuracy of the learned function. After parameter adjustment and learning, the performance of the resulting function should be measured on a test set that is separate from the training set.",
            "output": "Use the following steps to solve a given problem of supervised learning:\n1. Determine the type of training examples,\n2. Gather a training set.\n3. Determine the input feature representation of the learned function.\n4. Determine the structure of the learned function and corresponding learning algorithm.\n5. Run the learning algorithm on the gathered training set. \n6. Evaluate the accuracy of the learned function.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.92,
            "s_out_bg": 0.88
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.07999999999999996,
            "s_out_bg": 0.12
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 2.7143466472625732,
            "normalized": 0.6326704706464495
        },
        "potential_entropy": {
            "strategy_component": 0.20750000000000002,
            "strategy_details": {
                "raw": 0.41500000000000004,
                "normalized": 0.41500000000000004,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.25,
                    "output": 0.105
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.07999999999999996,
                    "out_bg": 0.12
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 902,
        "original_data": {
            "instruction": "What fruits and vegetables are grown in lower-lying areas of Kenya?",
            "input": "Tea, coffee, sisal, pyrethrum, corn, and wheat are grown in the fertile highlands, one of the most successful agricultural production regions in Africa. Livestock predominates in the semi-arid savanna to the north and east. Coconuts, pineapples, cashew nuts, cotton, sugarcane, sisal, and corn are grown in the lower-lying areas. Kenya has not attained the level of investment and efficiency in agriculture that can guarantee food security, and coupled with resulting poverty (53% of the population lives below the poverty line), a significant portion of the population regularly starves and is heavily dependent on food aid. Poor roads, an inadequate railway network, under-used water transport, and expensive air transport have isolated mostly arid and semi-arid areas, and farmers in other regions often leave food to rot in the fields because they cannot access markets. This was last seen in August and September 2011, prompting the Kenyans for Kenya initiative by the Red Cross.",
            "output": "Coconuts, pineapples and corn are grown in lower-lying areas of Kenya.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 4.534873008728027,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4525,
            "strategy_details": {
                "raw": 0.905,
                "normalized": 0.905,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 903,
        "original_data": {
            "instruction": "Given a reference text about phrynobatrachus tokba, what are common names for it?",
            "input": "Phrynobatrachus tokba is a species of frog in the family Phrynobatrachidae. It is found in West Africa from Ghana westward to Côte d'Ivoire, Liberia, Guinea, Sierra Leone, and Guinea-Bissau. Common names Tokba river frog, Tokba puddle frog, and—when referring to the formerly recognized Phrynobatrachus alticola, forest river frog—have been proposed for it.",
            "output": "Common names for Phrynobatrachus tokba are Tokba river frog, Tokba puddle frog, and forest river frog.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.920039415359497,
            "normalized": 0.977154118674142
        },
        "potential_entropy": {
            "strategy_component": 0.375,
            "strategy_details": {
                "raw": 0.75,
                "normalized": 0.75,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 904,
        "original_data": {
            "instruction": "When was the US flag created?",
            "input": "",
            "output": "On June 14, 1777, the Continental Congress passed an act establishing an official flag. Today, the flag has 13 horizontal stripes that represent the colonies and 50 stars that represent the states.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.5141680240631104,
            "normalized": 0.5754765783037458
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 905,
        "original_data": {
            "instruction": "Where is Shuyga?",
            "input": "Shuyga (Russian: Шуйга) is a rural locality (a settlement) in Surskoye Rural Settlement of Pinezhsky District, Arkhangelsk Oblast, Russia. The population was 472 as of 2010. There are 11 streets.",
            "output": "Shuyga is located on the Shuyga River, 99 km southeast of Karpogory (the district's administrative centre) by road. Shulomen is the nearest rural locality.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            0,
            0,
            2
        ],
        "entropy": {
            "raw": 5.237225532531738,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.38250000000000006,
            "strategy_details": {
                "raw": 0.7650000000000001,
                "normalized": 0.7650000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.45000000000000007,
                    "output": 0.195
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 906,
        "original_data": {
            "instruction": "What was the first full-length animated movie that Disney released?",
            "input": "",
            "output": "\"Snow White and the Seven Dwarfs\" was the first full-length animated Disney movie, released in 1937.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 1.7968565225601196,
            "normalized": 0.37053043501717703
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 907,
        "original_data": {
            "instruction": "What happens during the section called \"Giggle Box\"?",
            "input": "The TV series was starred and created by Justin Fletcher, who plays the characters in all of the comedy sketches and also wrote a few sketches in its first series. Some of the sketches are filmed in the studio, while other sketches are filmed outside; for example, one is filmed at Portmeirion, used for the fictional town of Wiggyville where Captain Adorable's sketches are set from Series 1 to 2. The show is interspersed with a selection of children called the Gigglekids, who tell jokes to Justin himself, in their own section called 'Giggle Box'.",
            "output": "Children called the Gigglekids tell jokes to Justin.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 7.3849592208862305,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5225,
            "strategy_details": {
                "raw": 1.045,
                "normalized": 1.045,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.55,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 908,
        "original_data": {
            "instruction": "Which of the current Girl Scout cookies is based on one of the original Girl Scout cookies?",
            "input": "",
            "output": "Trefoils are based on the original shortbread recipe",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.7490153312683105,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 909,
        "original_data": {
            "instruction": "Why do cloud providers use dedicated hardware for network and storage processing but not large enterprises for their private cloud?",
            "input": "",
            "output": "Large cloud providers use dedicated hardware called data processing unit (DPU) to support multi-tenancy and isolation in hardware vs. hypervisor, improve performance through offload, own and control the entire infrastructure stack, free up cores to generate more revenue, and scale to support a large number of tenants and workloads. Large enterprises don't have similar needs except for a few applications that require high performance, low latency, and scale. Moreover, enterprises want to reduce their investment in building and managing data centers.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.5529403686523438,
            "normalized": 0.8722686767578125
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 910,
        "original_data": {
            "instruction": "How do you subtract two numbers?",
            "input": "",
            "output": "Subtraction is an operation that identifies the total count of objects in a collection of items after a specific count of items are removed from the starting total.\n\nYou can subtract two numbers by starting with an initial count, which represents the number to subtract from, and a second count which represents the number to subtract.  Then remove one from the starting count repeating this operation for the number of items in the subtract value.  \n\nSubtraction is not associative, which means that the order in which subtraction is performed matters.  For instance, seven minus five (7 - 3) equals four (= 4) but four minus seven (4 - 7) equals minus three (= -3).",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.7031171321868896,
            "normalized": 0.6294620377676827
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 911,
        "original_data": {
            "instruction": "Give me a chronological bulleted list based on a timeline of events",
            "input": "Olson served as a law clerk for Judge Barbara Jacobs Rothstein on the United States District Court for the Western District of Washington for two years immediately after finishing law school. She served as a trial attorney and later Deputy Director of the National Church Arson Task Force in the United States Department of Justice Civil Rights Division from 1992 to 1997. From 1994 to 1997, she also worked part-time as an adjunct professor at George Washington University Law School.\n\nIn 1997, Olson joined the Office of the United States Attorney for the District of Idaho, rising to the rank of Senior Litigation Counsel at the time of her appointment in 2010.\n\nAfter Olson resigned in 2017, she joined the Boise office of Stoel Rives, a law firm that operates in the Pacific Northwest. In 2021, Olson was included on a shortlist of possible nominees to succeed Judge B. Lynn Winmill.",
            "output": "* 1992 to 1997: Olson served as a trial attorney and later Deputy Director of the National Church Arson Task Force in the United States Department of Justice Civil Rights Division\n* 1994 to 1997: Olson worked part-time as an adjunct professor at George Washington University Law School\n* 1997: Olson joined the Office of the United States Attorney for the District of Idaho\n* 2010: Olson rose to the rank of Senior Litigation Counsel\n* 2017: Olson resigned\n* 2017: Olson joined the Boise office of Stoel Rives, a law firm that operates in the Pacific Northwest\n* 2021: Olson was included on a shortlist of possible nominees to succeed Judge B. Lynn Winmill",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.3730978965759277,
            "normalized": 0.5351708275931222
        },
        "potential_entropy": {
            "strategy_component": 0.29500000000000004,
            "strategy_details": {
                "raw": 0.5900000000000001,
                "normalized": 0.5900000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 912,
        "original_data": {
            "instruction": "Classify these various instruments as either percussion or woodwind instruments: flute, clarinet, drum, bassoon, marimba, cymbal",
            "input": "",
            "output": "The drum, marimba and cymbal are percussion instruments. The flute, clarinet, and bassoon are woodwind instruments.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.604112148284912,
            "normalized": 0.6011748995099749
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 913,
        "original_data": {
            "instruction": "Extract the factors that influence the cost of building a house",
            "input": "The cost of building a house varies by country widely. According to data from the National Association of Realtors, the median cost of buying an existing single-family house in the United States is $274,600, whereas the average cost to build is $296,652. Several different factors can impact the cost of building a house, including the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent",
            "output": "The factors that influence the cost of building a house are, the size of the dwelling, the location, and availability of resources, the slope of the land, the quality of the fixtures and fittings, and the difficulty in finding construction and building materials talent",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2078046798706055,
            "normalized": 0.7736584799630302
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 914,
        "original_data": {
            "instruction": "History of the Arches National Park creation.",
            "input": "Humans have occupied the region since the last ice age 10,000 years ago. Fremont people and Ancestral Puebloans lived in the area until about 700 years ago. Spanish missionaries encountered Ute and Paiute tribes in the area when they first came through in 1775, but the first European-Americans to attempt settlement in the area were the Mormon Elk Mountain Mission in 1855, who soon abandoned the area. Ranchers, farmers, and prospectors later settled Moab in the neighboring Riverine Valley in the late 1870s. Word of the beauty of the surrounding rock formations spread beyond the settlement as a possible tourist destination.\n\nThe Arches area was first brought to the attention of the National Park Service by Frank A. Wadleigh, passenger traffic manager of the Denver and Rio Grande Western Railroad. Wadleigh, accompanied by railroad photographer George L. Beam, visited the area in September 1923 at the invitation of Alexander Ringhoffer, a Hungarian-born prospector living in Salt Valley. Ringhoffer had written to the railroad to interest them in the tourist potential of a scenic area he had discovered the previous year with his two sons and a son-in-law, which he called the Devils Garden (known today as the Klondike Bluffs). Wadleigh was impressed by what Ringhoffer showed him, and suggested to Park Service director Stephen T. Mather that the area be made a national monument.\n\nThe following year, additional support for the monument idea came from Laurence Gould, a University of Michigan graduate student (and future polar explorer) studying the geology of the nearby La Sal Mountains, who was shown the scenic area by local physician Dr. J. W. \"Doc\" Williams.\n\nA succession of government investigators examined the area, in part due to confusion as to the precise location. In the process, the name Devils Garden was transposed to an area on the opposite side of Salt Valley that includes Landscape Arch, the longest arch in the park. Ringhoffer's original discovery was omitted, while another area nearby, known locally as the Windows, was included. Designation of the area as a national monument was supported by the Park Service in 1926 but was resisted by President Calvin Coolidge's Interior Secretary, Hubert Work. Finally, in April 1929, shortly after his inauguration, President Herbert Hoover signed a presidential proclamation creating the Arches National Monument, consisting of two comparatively small, disconnected sections. The purpose of the reservation under the 1906 Antiquities Act was to protect the arches, spires, balanced rocks, and other sandstone formations for their scientific and educational value. The name Arches was suggested by Frank Pinkely, superintendent of the Park Service's southwestern national monuments, following a visit to the Windows section in 1925.\n\nIn late 1938, President Franklin D. Roosevelt signed a proclamation that enlarged the Arches to protect additional scenic features and permit the development of facilities to promote tourism. A small adjustment was made by President Dwight Eisenhower in 1960 to accommodate a new road alignment.\n\nIn early 1969, just before leaving office, President Lyndon B. Johnson signed a proclamation substantially enlarging the Arches. Two years later, President Richard Nixon signed legislation enacted by Congress, which significantly reduced the total area enclosed, but changed its status. Arches National Park was formally dedicated in May 1972.\n\nIn 1980, vandals attempted to use an abrasive kitchen cleanser to deface ancient petroglyphs in the park, prompting park officials to recruit physicist John F. Asmus, who specialized in using lasers to restore works of art, to use his technology to repair the damage. Asmus \"zapped the panel with intense light pulses and succeeded in removing most of the cleanser\". In 2016, there was another vandalism event on Frame Arch in the park, where a section if the rock was carved out. Due to advances in technology, in 2018 the arch was repaired through color match and modern infilling methods.",
            "output": "First Humans appeared in this region during the last ice age 10000 years ago. The first settlers began in 1775 but were abandoned before 1855 when ranchers, farmers, and prospectors settled in Moab(the closest city to the Arches). The first attention to these beautiful rock formations was broad in 1923 by Frank A. Wadleigh. Devils Garden(known as Klondike Bluffs today) got its name from a prospector from Salt Valley Alexander Ringhoffer, Devils Garden was transposed to an area on the opposite side of Salt Valley. Landscape Arch is the longest arch in the park and the fifth longest in the world! \nIn 1929, President Herbert Hoover signed a presidential proclamation creating the Arches National Monument to protect the arches, spires, balanced rocks, and other sandstone formations for their scientific and educational value. The name Arches was suggested by Frank Pinkely, in 1925. From 1938 multiple presidents of the United States of America signed multiple proclamations to protest scenic features and permits for road constructions in order to attract tourists into the area but only in 1972 Arches National Park was officially created. \nIn 1980, vandals tried to destroy ancient petroglyphs in the park, which physicist John F. Asmus restored using laser technology for restoration. In 2016, another vandalism event happened on Frame Arch a section of rocks was carved, but thanks to technology, it was restored.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.88,
            "s_out_cot": 0.8,
            "s_out_div": 0.75,
            "s_out_dens": 0.78,
            "s_out_bg": 0.82
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.12,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.25,
            "s_out_dens": 0.21999999999999997,
            "s_out_bg": 0.18000000000000005
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.619706392288208,
            "normalized": 0.8913446835109166
        },
        "potential_entropy": {
            "strategy_component": 0.3275,
            "strategy_details": {
                "raw": 0.655,
                "normalized": 0.655,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.21999999999999997,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.12,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.25,
                    "out_dens": 0.21999999999999997,
                    "out_bg": 0.18000000000000005
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 915,
        "original_data": {
            "instruction": "Without quoting directly from the text give me a summary of Cesar Chavez greatest achievements.",
            "input": "Cesar Chavez (born Cesario Estrada Chavez /ˈtʃɑːvɛz/; Spanish: [ˈt͡ʃaβes]; March 31, 1927 – April 23, 1993) was an American labor leader and civil rights activist. Along with Dolores Huerta, he co-founded the National Farm Workers Association (NFWA), which later merged with the Agricultural Workers Organizing Committee (AWOC) to become the United Farm Workers (UFW) labor union. Ideologically, his world-view combined leftist politics with Catholic social teachings.\n\nBorn in Yuma, Arizona to a Mexican American family, Chavez began his working life as a manual laborer before spending two years in the United States Navy. Relocating to California, where he married, he got involved in the Community Service Organization (CSO), through which he helped laborers register to vote. In 1959, he became the CSO's national director, a position based in Los Angeles. In 1962, he left the CSO to co-found the NFWA, based in Delano, California, through which he launched an insurance scheme, a credit union, and the El Malcriado newspaper for farmworkers. Later that decade he began organizing strikes among farmworkers, most notably the successful Delano grape strike of 1965–1970. Amid the grape strike his NFWA merged with Larry Itliong's AWOC to form the UFW in 1967. Influenced by the Indian independence leader Mahatma Gandhi, Chavez emphasized direct but nonviolent tactics, including pickets and boycotts, to pressure farm owners into granting strikers' demands. He imbued his campaigns with Roman Catholic symbolism, including public processions, masses, and fasts. He received much support from labor and leftist groups but was monitored by the Federal Bureau of Investigation (FBI).\n\nIn the early 1970s, Chavez sought to expand the UFW's influence outside California by opening branches in other U.S. states. Viewing illegal immigrants as a major source of strike-breakers, he also pushed a campaign against illegal immigration into the U.S., which generated violence along the U.S.-Mexico border and caused schisms with many of the UFW's allies. Interested in co-operatives as a form of organization, he established a remote commune at Keene. His increased isolation and emphasis on unrelenting campaigning alienated many California farmworkers who had previously supported him and by 1973 the UFW had lost most of the contracts and membership it won during the late 1960s. His alliance with California Governor Jerry Brown helped ensure the passing of the California Agricultural Labor Relations Act of 1975, although the UFW's campaign to get its measures enshrined in California's constitution failed. Influenced by the Synanon religious organization, Chavez re-emphasized communal living and purged perceived opponents. Membership of the UFW dwindled in the 1980s, with Chavez refocusing on anti-pesticide campaigns and moving into real-estate development, generating controversy for his use of non-unionized laborers.\n\nA controversial figure, UFW critics raised concerns about Chavez's autocratic control of the union, the purges of those he deemed disloyal, and the personality cult built around him, while farm-owners considered him a communist subversive. He became an icon for organized labor and leftist groups in the U.S. and posthumously became a \"folk saint\" among Mexican Americans. His birthday is a federal commemorative holiday in several U.S. states, while many places are named after him, and in 1994 he posthumously received the Presidential Medal of Freedom.",
            "output": "Cesar Chavez is widely regarded as the folk hero fighting for the rights of farm workers in the 1960's and 1970's. He is the co-founder of the farm workers union and helped to pass laws in California protecting farm workers. He worked on expanding the movement throughout the United States but had limited success.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.5543737411499023,
            "normalized": 0.5869639260428292
        },
        "potential_entropy": {
            "strategy_component": 0.43999999999999995,
            "strategy_details": {
                "raw": 0.8799999999999999,
                "normalized": 0.8799999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.25,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 916,
        "original_data": {
            "instruction": "Where was Atlassian founded?",
            "input": "Atlassian Corporation (/ətˈlæsiən/) is an Australian software company that develops products for software developers, project managers and other software development teams. The company is domiciled in Delaware, with global headquarters in Sydney, Australia, and US headquarters in San Francisco.",
            "output": "Atlassian Corporation was founded in Australia, but is domeciled in Delaware, with global headquarters in Sydney, Australia, and US headquarters in San Francisco.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.242912292480469,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 917,
        "original_data": {
            "instruction": "What is the philosophy of the movie Up?",
            "input": "",
            "output": "Ellie is clearly a lot more adventurous than Carl. In fact that is clear in the first few minutes. To Carl, \"Spirit of Adventure\" is a balloon and for her its an abandoned house. The size, I think, matters because that is the size of the spirit of adventure.\n\nAnother thing alluding to her spirit of adventure is the loss of the baby and her subsequent rebounding from the sadness. She takes this grave loss as an adventure in her book. Carl is a good husband to point out that she needs to fulfill her \"Things I am going to do\" promise to herself, but does not realize how Ellie views the loss of the baby.\n\nCarl is stuck on the aspect of having the house on Paradise island. He thinks that Ellie never lived her adventures and never added to her book since her first wish never got fulfilled. \n\nWhen an adventure presents itself in the form of Kevin, Carl, still attached to the past, prefers to fulfill the perceived wish by Ellie. When he does get the home on the water fall, it is clear that he is not satisfied. He can say that the dog nor Kevin are his problems. But when he sits down in his beloved chair, and looks through Ellie book of adventure again, he realizes that Ellie lived her adventures. She had no regrets. Her life was an adventure. And just at the right time, Carl sees the message from her to live his own adventure, i.e. Kevin.\n\nThings become clearer for Carl. He is absolved of his guilt. He realizes that Ellie never felt bad about not being able to have her home on Paradise island. This moment of clarity is what all humans crave for. With this clarity, it was easy for him to be rid of all his attachments. He left behind his and his wife's chairs that he is clearly very attached to.\n\nThe message here, at least for the adults to discern and then communicate to the kids is to live your life. Yes you have dreams. But in day to day life, those dreams may not be achieved. Look for adventure. Look to live fully and without attachment to your dreams like Ellie did. Because, then, in living this full life, it just might be that your dream comes true. It just might be that the house is on Paradise island even without you trying like in the end of the movie!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.8116421699523926,
            "normalized": 0.946183477129255
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 918,
        "original_data": {
            "instruction": "What was Barthelemy Hus-Desforges famous for",
            "input": "Barthélemy Hus, called Hus-Desforges (18 July 1699 in Bordeaux – 1 September 1786 in Lyon, aged 63) was an 18th-century French comedian and troupe leader.\n\nThe son of maître à danser  Jérôme Hus and Marguerite Pageot, called Desforges, he was the youngest member of the Hus family and with his brother François, directed a touring company known as the \"Hus brothers troupe.\"\n\nAround 1722, he married Marie Anne Daguerre Ascorette, actress born in Namur (1709-1736). The couple had at least seven children who would be part of the troupe.\n\nOn 27 January 1750, he married in Brussels his second wife, Louise Vivier de Courtenay, descendant of the famous Courtenay family.\n\nBarthélemy Hus-Desforges was the grandfather of musician Pierre-Louis Hus-Desforges (1773-1838).",
            "output": "Barthelemy Hus, called Hus-Desforges (18 July 1699 in Bordeaux – 1 September 1786 in Lyon, aged 63) was an 18th-century French comedian and troupe leader.\n\nThe son of maitre a danser  Jerome Hus and Marguerite Pageot, called Desforges, he was the youngest member of the Hus family and with his brother Francois, directed a touring company known as the \"Hus brothers troupe.\"\n\nAround 1722, he married Marie Anne Daguerre Ascorette, actress born in Namur (1709-1736). The couple had at least seven children who would be part of the troupe.\n\nOn 27 January 1750, he married in Brussels his second wife, Louise Vivier de Courtenay, descendant of the famous Courtenay family.\n\nBarthelemy Hus-Desforges was the grandfather of musician Pierre-Louis Hus-Desforges (1773-1838).",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.8,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.014242172241211,
            "normalized": 0.7183549063546317
        },
        "potential_entropy": {
            "strategy_component": 0.43249999999999994,
            "strategy_details": {
                "raw": 0.8649999999999999,
                "normalized": 0.8649999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.22499999999999998
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 919,
        "original_data": {
            "instruction": "Why german cars are best?",
            "input": "",
            "output": "1. They are perfect build quality\n2. Engine is refined.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.5,
            "s_out_div": 0.2,
            "s_out_dens": 0.6,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.5,
            "s_out_div": 0.8,
            "s_out_dens": 0.4,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.5615739822387695,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.19,
            "strategy_details": {
                "raw": 2.38,
                "normalized": 2.38,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.2999999999999998,
                    "output": 0.7200000000000001
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.5,
                    "out_div": 0.8,
                    "out_dens": 0.4,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 920,
        "original_data": {
            "instruction": "What are the Current assets in finance?",
            "input": "",
            "output": "These are short−term assets which are constantly changing in value, such as stocks, debtors and bank balances.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.1876420974731445,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.72,
            "strategy_details": {
                "raw": 1.44,
                "normalized": 1.44,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 921,
        "original_data": {
            "instruction": "What is best creator's platform",
            "input": "",
            "output": "Youtube. Youtube should be best creator platform",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.5,
            "s_out_div": 0.1,
            "s_out_dens": 0.4,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.5,
            "s_out_div": 0.9,
            "s_out_dens": 0.6,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.839182376861572,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.275,
            "strategy_details": {
                "raw": 2.55,
                "normalized": 2.55,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.81
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.5,
                    "out_div": 0.9,
                    "out_dens": 0.6,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 922,
        "original_data": {
            "instruction": "What is Thomas Anderson's hacker name?",
            "input": "",
            "output": "Thomas Anderson, in The Matrix, called himself \"Neo.\"",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 5.188482284545898,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 923,
        "original_data": {
            "instruction": "What are the benefits of meditation?",
            "input": "",
            "output": "Meditation enhances focus, memory, and mood. Meditation has also been shown to have other benefits such as reducing the negative impacts of lack of sleep.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.7438013553619385,
            "normalized": 0.6410861015319824
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 924,
        "original_data": {
            "instruction": "What specie is the Anachemmis sober?",
            "input": "Anachemmis sober is a species of false wolf spiders & wandering spiders in the family Zoropsidae. It is found in the United States.",
            "output": "Anachemmis sober is part of the family of spiders of the species of false wolf.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.4,
            "s_out_bg": 0.3
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.6,
            "s_out_bg": 0.7
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.940512180328369,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6499999999999999,
            "strategy_details": {
                "raw": 1.2999999999999998,
                "normalized": 1.2999999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.66
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.4,
                    "out_div": 0.5,
                    "out_dens": 0.6,
                    "out_bg": 0.7
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 925,
        "original_data": {
            "instruction": "Given a reference text about Pwojè Pyebwa, who funded the project?",
            "input": "Pwojè Pyebwa (Tree Project) is a tree-planting project in Haiti. It was designed, implemented, and initially run by cultural anthropologists. Different from reforestation projects, Pwojè Pyebwa promoted agroforestry—the strategic mixing of crops, trees, and animals. The agroforestry project in Haiti project was funded by the United States Agency for International Development USAID, but implemented through the Pan American Development Organization (PADF) and CARE. It was originally called the Agroforestry Outreach Project (AOP). The original project spanned a decade (1981–1991), but policies from Pwojè Pyebwa continued to direct PADF tree-planting through a second decade (1992 to 2000).\n\nThe project was executed throughout the country. Based on Haitian farmers’ voluntary and widespread participation, the project has been deemed a success by many. Pwojè Pyebwa distributed free trees to farmers, and advised on the incorporation of trees into existing cropping arrangements based on anthropological research into the land tenure, agricultural systems, and the historical market-orientation of Haitian peasant farmers. The project eventually won the Praxis prize in applied anthropology. It is estimated that the project involved one-third of the entire rural Haitian population. Approximately 65 million trees were planted.",
            "output": "Pwojè Pyebwa was funded by the United States Agency for International Development (USAID).",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.118514060974121,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 926,
        "original_data": {
            "instruction": "What are the best day trips near San Francisco?",
            "input": "",
            "output": "The San Francisco Bay Area has a number of amazing day trips offering a variety of options.  The wine country is less than an hour away.  Santa Cruz, Monterey & Big Sur are all great escapes to the coastline.  If you are interested in a spa experience Calistoga is full of options.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.7,
            "s_out_div": 0.75,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.25,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.469964027404785,
            "normalized": 0.8485611506870815
        },
        "potential_entropy": {
            "strategy_component": 0.7649999999999999,
            "strategy_details": {
                "raw": 1.5299999999999998,
                "normalized": 1.5299999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.25,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 927,
        "original_data": {
            "instruction": "What was the name of the character Steve Buscemi played in the HBO series BoardWalk Empire?",
            "input": "Buscemi starred in the HBO drama series Boardwalk Empire created by Terence Winter. The series started in 2010, where Buscemi assumed the role as Enoch \"Nucky\" Thompson (based on Enoch L. Johnson), a corrupt Atlantic City politician who rules the town during the Prohibition era. Buscemi's performance garnered him a Golden Globe Award for Best Actor – Television Series Drama; he later received two more nominations for his work on the same show. In 2011 he hosted NBC's Saturday Night Live.",
            "output": "Enoch \"Nucky\" Thompson",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.95,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 6.748470306396484,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3325,
            "strategy_details": {
                "raw": 0.665,
                "normalized": 0.665,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 928,
        "original_data": {
            "instruction": "Please classify the following as either cities or states: Chicago, Massachusetts, Boston, Alabama, Alaska, and Phoenix.",
            "input": "",
            "output": "Chicago, Phoenix, and Boston are cities. Massachusetts, Alabama, and Alaska are states.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.232259750366211,
            "normalized": 0.7806456429617745
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 929,
        "original_data": {
            "instruction": "Who is the most influential president in history?",
            "input": "",
            "output": "Abraham Lincoln is mostly regarded as the greatest president for his leadership during the Civil War and the abolition of slavery. His main contender is Franklin D. Roosevelt, for leading the country out of the Great Depression and during World War II.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.469452381134033,
            "normalized": 0.5627006803240094
        },
        "potential_entropy": {
            "strategy_component": 1.1875,
            "strategy_details": {
                "raw": 2.375,
                "normalized": 2.375,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.43500000000000005
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 930,
        "original_data": {
            "instruction": "Which rivers were the birthplaces of the first great civilizations?",
            "input": "",
            "output": "The first great civilizations arose near rivers, where they could develop agriculture. The Egyptian civilization arose around the Nile, the Sumerian civilization in Mesopotamia, between the Tigris and Euphrates rivers; the Harappa and Mohenjo Daro cultures arose along the Indus river (Pakistan), and the ancient Chinese civilization arose around the Yangtze river.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.474712610244751,
            "normalized": 0.5642036029270717
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 931,
        "original_data": {
            "instruction": "Who is the best all-time soccer player?",
            "input": "",
            "output": "It is hard to say and very subjective among fans of different clubs and nationalities. But, it would be safe to say that the best all-time soccer player is among the following list: Messi, Cristiano Ronaldo, Maradona, and Pele.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.9,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.3350822925567627,
            "normalized": 0.8100235121590751
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 932,
        "original_data": {
            "instruction": "Provide a list of cities you drive through to get from San Diego to Phoenix",
            "input": "",
            "output": "Alpine, El Centro, Yuma, Gila Bend",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 5.665626525878906,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 933,
        "original_data": {
            "instruction": "Should I pet the belly of a cat who is laying on its back?",
            "input": "",
            "output": "It depends on the cat. Some cats may enjoy having their belly rubbed and will permit you to do so. Other cats may not appreciate it and this position may be a trap for them to bite and kick your hand.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.3,
            "s_inp_complex": 0.2,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.2538845539093018,
            "normalized": 0.7868241582598005
        },
        "potential_entropy": {
            "strategy_component": 1.1400000000000001,
            "strategy_details": {
                "raw": 2.2800000000000002,
                "normalized": 2.2800000000000002,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.5,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.7,
                    "inp_complex": 0.8,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 934,
        "original_data": {
            "instruction": "Give me some of the most famous characters Marvel Comics has produced from this list.",
            "input": "Marvel counts among its characters such well-known superheroes as Spider-Man, Iron Man, Captain America, Thor, Doctor Strange, Hulk, Daredevil, Wolverine, Black Panther and Captain Marvel, as well as popular superhero teams such as the Avengers, X-Men, Fantastic Four, and Guardians of the Galaxy. Its stable of well-known supervillains includes the likes of Doctor Doom, Magneto, Ultron, Thanos, Kang the Conqueror, Green Goblin, Galactus, Loki, and Kingpin. Most of Marvel's fictional characters operate in a single reality known as the Marvel Universe, with most locations mirroring real-life places; many major characters are based in New York City, New York, United States. Additionally, Marvel has published several licensed properties from other companies. This includes Star Wars comics twice from 1977 to 1986 and again since 2015.",
            "output": "Some of the most famous characters Marvel have produced include Spider-Man, Iron Man, Captain America, Thor, Doctor Strange, Hulk, and Wolverine. These characters are often affiliated with teams of superheroes, such as the Avengers, X-Men, Fantastic Four, and Guardians of the Galaxy.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.0853312015533447,
            "normalized": 0.4529517718723842
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 935,
        "original_data": {
            "instruction": "Frank Sinatra changed the lyrics of \"The Lady is a Tramp\" to \"The Lady is a\" what?",
            "input": "",
            "output": "\"Champ\".",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 7.908109664916992,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 936,
        "original_data": {
            "instruction": "What should I eat for dinner?",
            "input": "",
            "output": "It depends on your dietary preferences. If you are vegetarian, I suggest a meal with vegetables, such as grilled zucchini, onions, and bell peppers. You can add protein to this meal by sautéing tofu or tempeh in teriyaki sauce, or enjoy some red lentils on the side. To add carbohydrates to the dish, you can eat pasta, rice, or potatoes. If you are a carnivore, perhaps you might enjoy a steak or chicken dinner with a side or asparagus and a baked potato that has a knob of butter on top! Other common dinner options include pasta, burgers, quesadillas, poke bowls, sushi, Mediterranean kebabs, falafel sandwiches, soup, etc.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.825399160385132,
            "normalized": 0.6643997601100377
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 937,
        "original_data": {
            "instruction": "What was the app after the second app released by YouTube?",
            "input": "Through this period, YouTube tried several new ways to generate revenue beyond advertisements. In 2013, YouTube launched a pilot program for content providers to offer premium, subscription-based channels. This effort was discontinued in January 2018 and relaunched in June, with US$4.99 channel subscriptions. These channel subscriptions complemented the existing Super Chat ability, launched in 2017, which allows viewers to donate between $1 and $500 to have their comment highlighted. In 2014, YouTube announced a subscription service known as \"Music Key,\" which bundled ad-free streaming of music content on YouTube with the existing Google Play Music service. The service continued to evolve in 2015 when YouTube announced YouTube Red, a new premium service that would offer ad-free access to all content on the platform (succeeding the Music Key service released the previous year), premium original series, and films produced by YouTube personalities, as well as background playback of content on mobile devices. YouTube also released YouTube Music, a third app oriented towards streaming and discovering the music content hosted on the YouTube platform.",
            "output": "The third app released by YouTube was YouTube Music, which oriented towards streaming and discovering the music content hosted on the YouTube platform",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 4.583072662353516,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 938,
        "original_data": {
            "instruction": "Who is called Iron Man of India and why?",
            "input": "Vallabhbhai Jhaverbhai Patel (Gujarati: [ʋəlːəbːʰɑi dʒʰəʋeɾbʰɑi pəʈel]; pə-TEL; 31 October 1875 – 15 December 1950), commonly known as Sardar Patel, was an Indian lawyer, influential political leader, barrister and statesman who served as the first Deputy Prime Minister and Home Minister of India from 1947 to 1950. He was a barrister and a senior leader of the Indian National Congress, who played a leading role in the country's struggle for independence, guiding its integration into a united, independent nation. In India and elsewhere, he was often called Sardar, meaning \"chief\" in Hindi, Urdu, Bengali and Persian. He acted as the Home Minister during the political integration of India and the Indo-Pakistani War of 1947.\n\nPatel was born in Nadiad, Kheda district, and raised in the countryside of the state of Gujarat. He was a successful lawyer. One of Mahatma Gandhi's earliest political lieutenants, he organised peasants from Kheda, Borsad, and Bardoli in Gujarat in non-violent civil disobedience against the British Raj, becoming one of the most influential leaders in Gujarat. He was appointed as the 49th President of Indian National Congress, organising the party for elections in 1934 and 1937 while promoting the Quit India Movement.\n\nAs the first Home Minister and Deputy Prime Minister of India, Patel organised relief efforts for partition refugees fleeing to Punjab and Delhi from Pakistan and worked to restore peace. He led the task of forging a united India, successfully integrating into the newly independent nation those British colonial provinces that formed the Dominion of India. Besides those provinces that had been under direct British rule, approximately 565 self-governing princely states had been released from British suzerainty by the Indian Independence Act of 1947. Patel persuaded almost every princely state to accede to India. His commitment to national integration in the newly independent country was total and uncompromising, earning him the sobriquet \"Iron Man of India\". He is also remembered as the \"patron saint of India's civil servants\" for having established the modern All India Services system.\n\nThe Statue of Unity, the world's tallest statue which was erected by the Indian government at a cost of US$420 million, was dedicated to him on 31 October 2018 and is approximately 182 metres (597 ft) in height.",
            "output": "Sardar Vallabhbhai Jhaverbhai Patel is known as Iron Man of India. Born in 31 October 1875 in Gujarat, Patel was an Indian lawyer, influential political leader, barrister and statesman who served as the first Deputy Prime Minister and Home Minister of India from 1947 to 1950. After the Indian independence as the first Home Minister and Deputy Prime Minister of India, Patel organized relief efforts for partition refugees fleeing to Punjab and Delhi from Pakistan and worked to restore peace. He led the task of creating a united India, successfully integrating into the newly independent nation those British colonial provinces that formed the Dominion of India. Approximately 565 self-governing princely states had been released from British suzerainty by the Indian Independence Act of 1947 and became part of the Indian Union. Patel persuaded almost every princely state to accede to India. His commitment to national integration in the newly independent country was total and uncompromising, earning him the sobriquet \"Iron Man of India\"",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.09999999999999998,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.5892674922943115,
            "normalized": 0.5969335692269462
        },
        "potential_entropy": {
            "strategy_component": 0.3,
            "strategy_details": {
                "raw": 0.6,
                "normalized": 0.6,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.15000000000000002,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.09999999999999998,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 939,
        "original_data": {
            "instruction": "What are some foods to cook to celebrate St. Patrick's Day?",
            "input": "",
            "output": "Most people in Ireland and elsewhere like to celebrate St. Patrick's Day with traditional (or regionally adapted) Irish foods. These can include things like: corned beef and cabbage, potatoes, scotched eggs, shoulder bacon, soda bread, shepard's pie, beef stew, corned beef hash, colcannon.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.75,
            "s_out_div": 0.8,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.25,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.3580243587493896,
            "normalized": 0.8165783882141113
        },
        "potential_entropy": {
            "strategy_component": 0.9049999999999999,
            "strategy_details": {
                "raw": 1.8099999999999998,
                "normalized": 1.8099999999999998,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 1.2999999999999998,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.25,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 940,
        "original_data": {
            "instruction": "What sacrifices did Ons's parents make to help her tennis career?",
            "input": "Ons Jabeur was born to Samira and Ridha Jabeur in Ksar Hellal, a small town in Tunisia. She grew up in the larger nearby coastal town of Sousse. Jabeur has two older brothers, Hatem and Marwen, and an older sister, Yasmine. Her mother played tennis recreationally and introduced her to the sport at the age of three. Jabeur trained under coach Nabil Mlika for ten years from ages four to thirteen, originally starting to work with him at a tennis promotion centre at her school. When she was ten years old, her club did not have their own tennis courts and she could only train on courts at nearby hotels. At twelve years old, Jabeur moved to the capital city of Tunis to train at the Lycée Sportif El Menzah, a national sport high school for the country's up-and-coming athletes, where she stayed for several years.\n\nShe also later trained in Belgium and France starting at the age of 16. Jabeur credits her parents for the sacrifices they made when she was growing up, saying, \"My parents sacrificed a lot of things – my mom used to drive me everywhere around Tunisia to go play the tournaments, and she encouraged me to go to a special school to study. That was a big sacrifice to see her little girl going for a dream that, honestly, wasn't 100% guaranteed. She believed in me and gave me the confidence to be there.\"",
            "output": "Ons's mother used to drive Ons all around Tunisia to play tennis tournaments and encouraged Ons to go to a special school to study.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.8,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 4.798515319824219,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4525,
            "strategy_details": {
                "raw": 0.905,
                "normalized": 0.905,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.375
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 941,
        "original_data": {
            "instruction": "Tell me if these are TV shows or Movies: Breaking Bad, Dexter, Game of thrones, Fight Club.",
            "input": "",
            "output": "TV Shows: Breaking Bad, Dexter, Game of thrones\nMovies: Fight Club",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.876044750213623,
            "normalized": 0.9645842143467495
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 942,
        "original_data": {
            "instruction": "Identify which animal species is alive or extinct: Great White Shark, Patagotitan",
            "input": "",
            "output": "Patagotitan is extinct, Great White Shark is alive.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.4,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.6,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 5.252296447753906,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.6050000000000001,
            "strategy_details": {
                "raw": 1.2100000000000002,
                "normalized": 1.2100000000000002,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.6,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 943,
        "original_data": {
            "instruction": "Tell me whether these are literary characters or fruits: Gandalf, banana, Frodo Baggins, Aragorn, orange, apple, Sauron",
            "input": "",
            "output": "Gandalf - literary character\nBanana - fruit\nFrodo Baggins - literary character\nAragorn  - literary character\norange - fruit\napple - fruit\nSauron - literary character",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.998175621032715,
            "normalized": 0.7137644631522042
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 944,
        "original_data": {
            "instruction": "How do you care for a dog?",
            "input": "",
            "output": "Dogs are often described as a man's best friend.  I have had a dog in my life since I was a young child.  My father had a dog that he hunted birds alongside.  We have had dogs that live indoors with the family and outdoors in their own doghouse.  We have raised puppies from our dogs and given them away to friends and neighbors.  Dogs provide a lot of love and companionship to everyone in the family and they can be as beloved as any human.  Dogs take a great deal to care for because they are completely dependent on humans to keep them alive.  A dog left out in the wild is unlikely to survive without help from a caring human.  Dogs must be fed, watered, walked for exercise, socialized with other dogs and with other people as well so that they feel safe and interact safely with others.  Dogs can live to between 10 and 15 years old on average so they are with a family for a long time.  Dogs come in all shapes, sizes, colors, skills and are often referred to as breeds.  Breeds have different characteristics and people sometimes select a dog for the breed.  Sometimes dogs are neglected or left without a family to care for them and end up in temporary homes called animal shelters.  These shelters provide a safe haven for a dog until a suitable owner is matched.  Many of the best dogs that a family gets come from a shelter.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.3,
            "s_out_div": 0.7,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            1,
            0
        ],
        "entropy": {
            "raw": 2.7740166187286377,
            "normalized": 0.6497190339224679
        },
        "potential_entropy": {
            "strategy_component": 1.4049999999999998,
            "strategy_details": {
                "raw": 2.8099999999999996,
                "normalized": 2.8099999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.57
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.7,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 945,
        "original_data": {
            "instruction": "When did World War II finish?",
            "input": "",
            "output": "World War II, also knowns as WW2 or WWII, was a global conflict that started in 1939 and ended in 1945.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            1,
            2
        ],
        "entropy": {
            "raw": 1.5994006395339966,
            "normalized": 0.3141144684382847
        },
        "potential_entropy": {
            "strategy_component": 1.3299999999999998,
            "strategy_details": {
                "raw": 2.6599999999999997,
                "normalized": 2.6599999999999997,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.42000000000000004
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 946,
        "original_data": {
            "instruction": "What is on the DVD Our Favorite Things",
            "input": "Our Favorite Things is a compilation DVD by the band Negativland, released on November 27, 2007. The original release date on October 23 was skipped due to a pressing error in the DVDs and the bonus CD, a collection of Negativland covers done by an a cappella doo-wop group. It contains 20 of the band's greatest \"hits\" animated by 20 different artists from all over the United States. The cover parodies the artwork of the film The Sound of Music as well as title of the film's song \"My Favorite Things\". The front features a \"review\" from the character Goofy saying, \"It's goofy\". The people on the cover are dolls.",
            "output": "Our Favorite Things is a compilation DVD by the band Negativland, released on November 27, 2007. The original release date on October 23 was skipped due to a pressing error in the DVDs and the bonus CD, a collection of Negativland covers done by an a cappella doo-wop group. \n\nIt contains 20 of the band's greatest \"hits\" animated by 20 different artists from all over the United States. The cover parodies the artwork of the film The Sound of Music as well as title of the film's song \"My Favorite Things\". The front features a \"review\" from the character Goofy saying, \"It's goofy\". The people on the cover are dolls.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.307004451751709,
            "normalized": 0.8020012719290597
        },
        "potential_entropy": {
            "strategy_component": 0.34750000000000003,
            "strategy_details": {
                "raw": 0.6950000000000001,
                "normalized": 0.6950000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.22500000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 947,
        "original_data": {
            "instruction": "What are the benefits if I could turn my head around like an Owl?",
            "input": "",
            "output": "If humans achieved the ability to turn their head 270 degrees they would attain the following benefits.\n\n1. Situational awareness would increase with the ability to hear and view almost 360 degrees.  Making it easier to identify and defend against predators\n\n2. Better skeletal utilization in sports, theater viewing by utilizing chairs that prioritize space efficiency vs ergonomic efficiency.\n\n3.  Relief for side sleepers by making it possible for people to sleep on their back or stomach",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.55
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.44999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 4.408944129943848,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.7949999999999999,
            "strategy_details": {
                "raw": 1.5899999999999999,
                "normalized": 1.5899999999999999,
                "components": {
                    "instruction": 0.3,
                    "input": 0.9,
                    "output": 0.38999999999999996
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.44999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 948,
        "original_data": {
            "instruction": "Please tell me who Bishop Patrick MacMullan was and when he died.",
            "input": "Bishop Patrick MacMullan (17 March 1752 – 25 October 1824) was an Irish Roman Catholic Prelate and 20th Bishop of Down and Connor.\n\nHe was a native of mid Down and details of his early life in the latter half of the eighteenth century are sketchy. It is believed he was ordained to the priesthood in 1775.\n\nHe received episcopal consecration on 2 September 1793, and the following year succeeded his distant cousin Hugh as Bishop of Down and Connor.\n\nIn 1814 he made a report to Rome on the state of his diocese (served by around 35 parish priests and a few curates) which although vague gives some indication of the state of the diocese.\n\nHe died on 25 October 1824 in the house of his nephew in Loughinisland and is buried at Loughinisland Graveyard.\n\nA notice of his death, circulated in many Irish newspapers noted that \"the Catholic Clergy of that diocese [Down and Connor] have been under the scriptural jurisdiction of this amiable Prelate for 31 years, during which he has presided over them with the politeness of a Gentleman, the abilities of a Theologian, and the meekness of a humble and exemplary Christian.\"",
            "output": "Patrick MacMullan was an Irish Roman Catholic Bishop. He passed away on October 25th, 1824.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.7,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 3.1527340412139893,
            "normalized": 0.7579240117754255
        },
        "potential_entropy": {
            "strategy_component": 0.49249999999999994,
            "strategy_details": {
                "raw": 0.9849999999999999,
                "normalized": 0.9849999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.4,
                    "output": 0.345
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 949,
        "original_data": {
            "instruction": "What skis should I buy?",
            "input": "",
            "output": "There are many aspects to consider when buying a new pair of skis. The first and probably most important would be to consider you ability level. Ski companies typical group skiers into one of three categories: Level 1 (Beginner), Level 2 (Intermediate) or Level 3 (Advanced). Considering you ability level will help you decide on other factors to consider when purchasing a ski, such as the length, underfoot width, dimensions, camber and rocker.\n\nAnother important consideration to make when purchasing skis is to think about the type of terrain that you like to ski. Do you prefer skiing all-mountain conditions, powder conditions or backcountry conditions?\n\nBased on your answers to the previous two questions, you should be able to narrow the universe of skis that you'd consider for purchase. Next, you should use your height to determine the length of the ski that would fit you well.\n\nAfter you've made a decision on what type of skis you'd like to buy, you should consider what type of bindings and boots you would like to use with your skis.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 2.6040050983428955,
            "normalized": 0.6011443138122559
        },
        "potential_entropy": {
            "strategy_component": 1.0750000000000002,
            "strategy_details": {
                "raw": 2.1500000000000004,
                "normalized": 2.1500000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 950,
        "original_data": {
            "instruction": "Are these holidays Christian or Jewish? Passover, Hanukkah, Christmas, Easter, Valentines Day, Yom Kippur, Sukkot, Rosh Hashanah",
            "input": "",
            "output": "Passover, Hanukkah, Yom Kippur, Sukkot and Rosh Hashanah are all Jewish holidays. Christmas, Easter and Valentines Day are all Christian holidays.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 1.6721465587615967,
            "normalized": 0.3348990167890276
        },
        "potential_entropy": {
            "strategy_component": 0.74,
            "strategy_details": {
                "raw": 1.48,
                "normalized": 1.48,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 951,
        "original_data": {
            "instruction": "With which countries does Indonesia share borders?",
            "input": "Indonesia, officially the Republic of Indonesia, is a country in Southeast Asia and Oceania between the Indian and Pacific oceans. It consists of over 17,000 islands, including Sumatra, Java, Sulawesi, and parts of Borneo and New Guinea. Indonesia is the world's largest archipelagic state and the 14th-largest country by area, at 1,904,569 square kilometres (735,358 square miles). With over 275 million people, Indonesia is the world's fourth-most populous country and the most populous Muslim-majority country. Java, the world's most populous island, is home to more than half of the country's population.\nAs the world's third largest democracy, Indonesia is a presidential republic with an elected legislature. It has 38 provinces, of which nine have special status. The country's capital, Jakarta, is the world's second-most populous urban area. Indonesia shares land borders with Papua New Guinea, East Timor, and the eastern part of Malaysia, as well as maritime borders with Singapore, Vietnam, Thailand, the Philippines, Australia, Palau, and India. Despite its large population and densely populated regions, Indonesia has vast areas of wilderness that support one of the world's highest level of biodiversity.\nThe Indonesian archipelago has been a valuable region for trade since at least the seventh century, when the Srivijaya Kingdom formed trade links with China. Indonesian history has been influenced by foreign powers drawn to its natural resources. Under Indian influence, Hindu and Buddhist kingdoms flourished from the early centuries CE. Muslim traders later brought Islam, and European powers fought one another to monopolise trade in the Spice Islands of Maluku during the Age of Discovery. Following three and a half centuries of Dutch colonialism, Indonesia secured its independence after World War II. Indonesia's history has since been turbulent, with challenges posed by natural disasters, corruption, separatism, a democratisation process, and periods of rapid economic change.",
            "output": "Indonesia shares land borders with Papua New Guinea, East Timor, and the eastern part of Malaysia, as well as maritime borders with Singapore, Vietnam, Thailand, the Philippines, Australia, Palau, and India.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.5908658504486084,
            "normalized": 0.5973902429853167
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 952,
        "original_data": {
            "instruction": "Who are the greatest players that have played for Manchester United?",
            "input": "",
            "output": "- Sir Bobby Charlton \n- Eric Cantona \n- Paul Scholes \n- Nemanja Vidic \n- David Beckham \n- Cristiano Ronaldo \n- Ruud Van Nistelrooy \n- Wayne Rooney \n- Gary Neville \n- Dennis Law \n- George Best \n- Peter Schmeichel",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 2.324066162109375,
            "normalized": 0.5211617606026786
        },
        "potential_entropy": {
            "strategy_component": 0.8899999999999999,
            "strategy_details": {
                "raw": 1.7799999999999998,
                "normalized": 1.7799999999999998,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.2999999999999998,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 953,
        "original_data": {
            "instruction": "What is the use of a vaccine?",
            "input": "A vaccine is a biological preparation that provides active acquired immunity to a particular infectious or malignant disease. The safety and effectiveness of vaccines has been widely studied and verified. A vaccine typically contains an agent that resembles a disease-causing microorganism and is often made from weakened or killed forms of the microbe, its toxins, or one of its surface proteins. The agent stimulates the body's immune system to recognize the agent as a threat, destroy it, and to further recognize and destroy any of the microorganisms associated with that agent that it may encounter in the future.",
            "output": "Vaccines can be used to provide immunity to a specific disease or infection. It works by stimulating the body's immune system to be able to identify and destroy threat agents.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.95,
            "s_inp_complex": 0.85,
            "s_out_cot": 0.8,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.050000000000000044,
            "s_inp_complex": 0.15000000000000002,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.5691637992858887,
            "normalized": 0.5911896569388253
        },
        "potential_entropy": {
            "strategy_component": 0.32500000000000007,
            "strategy_details": {
                "raw": 0.6500000000000001,
                "normalized": 0.6500000000000001,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.20000000000000007,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.050000000000000044,
                    "inp_complex": 0.15000000000000002,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 954,
        "original_data": {
            "instruction": "Classify each of the following as characters who appear in books by Kevin Kwan or Veronica Roth: Astrid Leong-Teo, Kitty Pong, Colin Khoo, Peik Lin Goh, Charlie Wu, Caleb Prior, Beatrice Prior, Tori, Jeanine Matthews, Tobias Eaton",
            "input": "",
            "output": "Kevin Kwan characters: Astrid Leong-Teo, Kitty Pong, Colin Khoo, Peik Lin Goh, Charlie Wu\nVeronica Roth characters: Caleb Prior, Beatrice Prior, Tori, Jeanine Matthews, Tobias Eaton",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 5.427893161773682,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 955,
        "original_data": {
            "instruction": "How many continents are in the world?",
            "input": "",
            "output": "There are seven continents in the world",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.95,
            "s_out_div": 0.1,
            "s_out_dens": 0.9,
            "s_out_bg": 0.2
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.9,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.8
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.8992838859558105,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2475,
            "strategy_details": {
                "raw": 2.495,
                "normalized": 2.495,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.555
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.9,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.8
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 956,
        "original_data": {
            "instruction": "Identify which animal species is alive or extinct: Cave Bear, Saola",
            "input": "",
            "output": "Saola is alive, Cave Bear is extinct.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.9,
            "s_out_div": 0.5,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.5,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 6.15394401550293,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.815,
            "strategy_details": {
                "raw": 1.63,
                "normalized": 1.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.6,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.5,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 957,
        "original_data": {
            "instruction": "Which is a species of fish? Tenpounder or Ninepounder",
            "input": "",
            "output": "Tenpounder",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 9.86819076538086,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 958,
        "original_data": {
            "instruction": "Based on the paragraph about the Korean War, what is the name of the new sovereign state created in the north?",
            "input": "In 1910, Imperial Japan annexed Korea, where it ruled for 35 years until its surrender at the end of World War II on 15 August 1945. The United States and the Soviet Union divided Korea along the 38th parallel into two zones of occupation. The Soviets administered the northern zone and the Americans administered the southern zone. In 1948, as a result of Cold War tensions, the occupation zones became two sovereign states. A socialist state, the Democratic People's Republic of Korea, was established in the north under the totalitarian communist leadership of Kim Il-sung, while a capitalist state, the Republic of Korea, was established in the south under the autocratic leadership of Syngman Rhee. Both governments of the two new Korean states claimed to be the sole legitimate government of all of Korea, and neither accepted the border as permanent.",
            "output": "Democratic People's Republic of Korea",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 3.300629138946533,
            "normalized": 0.8001797539847237
        },
        "potential_entropy": {
            "strategy_component": 0.325,
            "strategy_details": {
                "raw": 0.65,
                "normalized": 0.65,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 959,
        "original_data": {
            "instruction": "What are the pros and cons of high interest rates?",
            "input": "",
            "output": "Rising interest rates are an advantage for savers who deposit money. Banks pay savers a higher return on their savings accounts. High interest rates also dampen high inflation as they make borrowing more expensive. This reduces the demand and can lead to falling prices. \nOn the other hand, high interest rates make things like financing a house more difficult. Furthermore, the decreased demand can reduce economic growth.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.8,
            "s_out_div": 0.85,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.2537543773651123,
            "normalized": 0.7867869649614606
        },
        "potential_entropy": {
            "strategy_component": 0.48500000000000004,
            "strategy_details": {
                "raw": 0.9700000000000001,
                "normalized": 0.9700000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.55,
                    "output": 0.23999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 960,
        "original_data": {
            "instruction": "Who is Edgar Allen Poe?",
            "input": "",
            "output": "Edgar Allen Poe is an American born poet and writer.  He has written many short stories, and is a renown figure of Romanticism in the US and US Literature.  High schools will teach about Edgar Allen Poe when teaching about American Literature and poetry.  One of his best quotes is the following: \"\"Words have no power to impress the mind without the exquisite horror of their reality.\"",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 3.4603779315948486,
            "normalized": 0.8458222661699567
        },
        "potential_entropy": {
            "strategy_component": 1.315,
            "strategy_details": {
                "raw": 2.63,
                "normalized": 2.63,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 961,
        "original_data": {
            "instruction": "What is the standard loft of a 9-iron in golf?",
            "input": "",
            "output": "The standard loft in today's 9-irons ranges from 45 to 48 degrees.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.8,
            "s_out_div": 0.3,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.19999999999999996,
            "s_out_div": 0.7,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            2,
            1
        ],
        "entropy": {
            "raw": 4.0564961433410645,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.15,
            "strategy_details": {
                "raw": 2.3,
                "normalized": 2.3,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 1.7000000000000002,
                    "output": 0.48
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.19999999999999996,
                    "out_div": 0.7,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 962,
        "original_data": {
            "instruction": "What are some tricks to learning a new language?",
            "input": "",
            "output": "Learning a new language can be a great challenge. Here are some tips to help you learn a new language:\n1. Turn on Netflix subtitles or dubbing. You'll hear and read how people actually talk, including slang.\n2. Join an online chat group for other people learning your language or find someone in your community to practice speaking with.\n3. Use flash cards to common words and phrases. \n4. Listen to radio or podcasts in your new language.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 2.722506046295166,
            "normalized": 0.6350017275129046
        },
        "potential_entropy": {
            "strategy_component": 1.2175,
            "strategy_details": {
                "raw": 2.435,
                "normalized": 2.435,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 2.0,
                    "output": 0.255
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 963,
        "original_data": {
            "instruction": "What are the words of House Footly?",
            "input": "",
            "output": "\"Tread Lightly Here\"",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.7,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.30000000000000004,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 4.26817512512207,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.225,
            "strategy_details": {
                "raw": 2.45,
                "normalized": 2.45,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.51
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.30000000000000004,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 964,
        "original_data": {
            "instruction": "Is it true that Lewis Hamilton won the championship in his first F1 season?",
            "input": "Hamilton's first season in Formula One saw him partner two-time and defending World Champion Fernando Alonso. Hamilton is the first and, as of 2022, the only black driver to race in the series. After finishing on the podium in his debut, Hamilton went on to set several records as he finished runner-up in the 2007 World Drivers' Championship to Kimi Räikkönen by one point, including those for the most consecutive podium finishes from debut (9), the joint most wins in a debut season (4) and the most points in a debut season (109). Throughout the season, Hamilton and Alonso were involved in a number of incidents which resulted in tensions between both drivers and the team, culminating in Alonso and McLaren terminating their contract by mutual consent in November. Following a successful first season at McLaren, Hamilton signed a multi-million-pound contract to stay with the team until 2012.",
            "output": "Lewis Hamilton did not win the F1 world driver's championship in his rookie season in 2007. The title went to Kimi Räikkönen in the Ferrari team. Hamilton finished runner up, in second place, after a close battle with both Ferrari and his teammate Fernando Alonso.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.85,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.7235379219055176,
            "normalized": 0.6352965491158622
        },
        "potential_entropy": {
            "strategy_component": 0.31,
            "strategy_details": {
                "raw": 0.62,
                "normalized": 0.62,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 965,
        "original_data": {
            "instruction": "Identify the political office or offices Julius Steele Barnes held.",
            "input": "Julius Steele Barnes (23 February 1792 – 12 November 1870) was an American physician. Besides being a skillful practitioner, and devoted to his calling, he also labored heartily for the social good of the community. He served one term as Connecticut State Senator, and held for a time the office of Judge of Probate.",
            "output": "Julius Steele Barnes was a Connecticut State senator and a Judge of Probate.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.9,
            "s_out_div": 0.6,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.4,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 5.767380714416504,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3975000000000001,
            "strategy_details": {
                "raw": 0.7950000000000002,
                "normalized": 0.7950000000000002,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.45000000000000007,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.4,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 966,
        "original_data": {
            "instruction": "Tell me about the football career of Adao",
            "input": "Carlos Manuel Pereira Pinto (born 3 April 1960), commonly known as Adão, is a Portuguese former footballer who played as a central midfielder.\n\nClub career\nBorn in Chaves, Vila Real District, Adão started his career with hometown club G.D. Chaves, moving to the Primeira Liga in 1980 with Varzim SC. He appeared in only eight games in his first season, which ended in relegation, but helped with 27 and three goals to an immediate promotion.\n\nIn summer 1985, Adão joined Vitória de Guimarães. In his second year he played 30 matches as starter and scored twice – including once in 1–0 away win over S.C. Braga in the Minho derby – as his team finished in third position.\n\nAdão signed with C.F. Os Belenenses for 1988–89. On 12 October 1988, in that campaign's UEFA Cup, he scored the game's only goal at home against Bayer 04 Leverkusen, as the Lisbon side ousted the title holders 2–0 on aggregate. Later in the season he also appeared in the final of the Portuguese Cup, won after beating S.L. Benfica.\n\nAged 30, Adão moved to F.C. Penafiel, spending his last three years (of five overall) in the second division. Over the course of 11 seasons, he amassed top-tier totals of 279 games and 16 goals.",
            "output": "Carlos Manuel Pereira Pinto (born 3 April 1960), commonly known as Adao, is a Portuguese former footballer who played as a central midfielder.\n\nBorn in Chaves, Vila Real District, Adao started his career with hometown club G.D. Chaves, moving to the Primeira Liga in 1980 with Varzim SC. He appeared in only eight games in his first season, which ended in relegation, but helped with 27 and three goals to an immediate promotion.\n\nIn summer 1985, Adao joined Vitoria de Guimaraes. In his second year he played 30 matches as starter and scored twice – including once in 1–0 away win over S.C. Braga in the Minho derby – as his team finished in third position.\n\nAdao signed with C.F. Os Belenenses for 1988–89. On 12 October 1988, in that campaign's UEFA Cup, he scored the game's only goal at home against Bayer 04 Leverkusen, as the Lisbon side ousted the title holders 2–0 on aggregate. Later in the season he also appeared in the final of the Portuguese Cup, won after beating S.L. Benfica.\n\nAged 30, Adao moved to F.C. Penafiel, spending his last three years (of five overall) in the second division. Over the course of 11 seasons, he amassed top-tier totals of 279 games and 16 goals.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.88,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.12,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.8040242195129395,
            "normalized": 0.6582926341465541
        },
        "potential_entropy": {
            "strategy_component": 0.34299999999999997,
            "strategy_details": {
                "raw": 0.6859999999999999,
                "normalized": 0.6859999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21600000000000003
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.12,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 967,
        "original_data": {
            "instruction": "Please list down some details about Kate Elizabeth Pickett from given text",
            "input": "Kate Elizabeth Pickett OBE FFPH FAcSS FRSA (born 1965) is a British epidemiologist and political activist who is Professor of Epidemiology in the Department of Health Sciences at the University of York, and was a National Institute for Health and Care Research Career Scientist from 2007–2012. She co-authored (with Richard G. Wilkinson) The Spirit Level: Why More Equal Societies Almost Always Do Better and is a co-founder of The Equality Trust. Pickett was awarded a 2013 Silver Rose Award from Solidar for championing equality and the 2014 Charles Cully Memorial Medal by the Irish Cancer Society.",
            "output": "1. Kate Elizabeth Pickett, a British epidemiologist and political activist, was a Career Scientist at the National Institute for Health and Care Research from 2007 to 2012 and is currently a Professor of Epidemiology in the Department of Health Sciences at the University of York.\n2. She founded The Equality Trust and co-authored The Spirit Level: Why More Equal Societies Almost Always Do Better with Richard G. Wilkinson.\n3. For promoting equality, Pickett received the Solidar Silver Rose Award in 2013 and the Irish Cancer Society's Charles Cully Memorial Medal in 2014.",
            "category": "summarization"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.9,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.95,
            "s_out_div": 0.8,
            "s_out_dens": 0.85,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.09999999999999998,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            0
        ],
        "entropy": {
            "raw": 3.1950583457946777,
            "normalized": 0.7700166702270508
        },
        "potential_entropy": {
            "strategy_component": 0.34,
            "strategy_details": {
                "raw": 0.68,
                "normalized": 0.68,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.35,
                    "output": 0.15
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.09999999999999998,
                    "inp_complex": 0.25,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 968,
        "original_data": {
            "instruction": "Classify each of the following as a programming language or not: Lisp, Basic, Esperanto, Prolog, Sympathy, Pascal, Tessellation, Kumquats, Algol, Java, C++, Corkscrew, Envy, Brainfuck",
            "input": "",
            "output": "From the list, these are programming languages: Lisp, Basic, Prolog, Pascal, Algol, Java, C++, Envy, Brainfuck.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.637885332107544,
            "normalized": 0.8965386663164411
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 969,
        "original_data": {
            "instruction": "Approximately how much cashmere is produced each year?",
            "input": "China has become the largest producer of raw cashmere, estimated at 19,200 metric tons (in hair) per year (2016). Mongolia follows with 8,900 tons (in hair) as of 2016, while Afghanistan, Iran, Turkey, Kyrgyzstan and other Central Asian republics produce lesser amounts. The annual world raw production is estimated to be between 15,000 and 20,000 tons (13,605 and 18,140 tonnes) (in hair). Pure cashmere, resulting from removing animal grease, dirt and coarse hairs from the fleece, is estimated at 6,500 tons (5,895 tonnes). Ultra-fine Cashmere or Pashmina is still produced by communities in Kashmir but its rarity and high price, along with political instability in the region, make it very hard to source and to regulate quality. It is estimated that the average yearly production per goat is 150 grams (0.33 lb).\n\nPure cashmere can be dyed and spun into yarns and knitted into jumpers (sweaters), hats, gloves, socks and other clothing, or woven into fabrics then cut and assembled into garments such as outer coats, jackets, trousers (pants), pajamas, scarves, blankets, and other items. Fabric and garment producers in Scotland, Italy, and Japan have long been known as market leaders. Cashmere may also be blended with other fibers to bring the garment cost down, or to gain their properties, such as elasticity from wool, or sheen from silk.\n\nThe town of Uxbridge, Massachusetts, in the United States was an incubator for the cashmere wool industry. It had the first power looms for woolens and the first manufacture of \"satinets\". Capron Mill had the first power looms, in 1820. It burned on July 21, 2007, in the Bernat Mill fire.\n\nIn the United States, under the U.S. Wool Products Labeling Act of 1939, as amended, (15 U. S. Code Section 68b(a)(6)), a wool or textile product may be labelled as containing cashmere only if the following criteria are met:\n\nsuch wool product is the fine (dehaired) undercoat fibers produced by a cashmere goat (Capra hircus laniger);\nthe average diameter of the fiber of such wool product does not exceed 19 microns; and\nsuch wool product does not contain more than 3 percent (by weight) of cashmere fibers with average diameters that exceed 30 microns.\nthe average fiber diameter may be subject to a coefficient of variation around the mean that shall not exceed 24 percent.",
            "output": "It is estimated that about 6,500 tons of cashmere is produced each year.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.369868278503418,
            "normalized": 0.8199623652866909
        },
        "potential_entropy": {
            "strategy_component": 0.385,
            "strategy_details": {
                "raw": 0.77,
                "normalized": 0.77,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 970,
        "original_data": {
            "instruction": "What is Economics?",
            "input": "Economics is the social science that studies the production, distribution, and consumption of goods and services. Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics analyzes what's viewed as basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the economy as a system where production, consumption, saving, and investment interact, and factors affecting it: employment of the resources of labour, capital, and land, currency inflation, economic growth, and public policies that have impact on these elements.",
            "output": "Economics allows us to understand resources, production of goods and services, growth over time and other issues that impact society.  Microeconomics is the study of individual agents such as buyers, sellers and households.  Macroeconomics studies the economy as a whole system and the factors that affect it.",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.75,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.25,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.397974967956543,
            "normalized": 0.542278562273298
        },
        "potential_entropy": {
            "strategy_component": 0.43,
            "strategy_details": {
                "raw": 0.86,
                "normalized": 0.86,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.25,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 971,
        "original_data": {
            "instruction": "Which is a species of fish? Escolar or Escobar",
            "input": "",
            "output": "Escolar",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.7
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.30000000000000004
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 6.7400336265563965,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.195,
            "strategy_details": {
                "raw": 2.39,
                "normalized": 2.39,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.44999999999999996
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.30000000000000004
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 972,
        "original_data": {
            "instruction": "Based on this passage about Ted Kennedy, at the time of his death, who were the three longest-serving senators of all time?",
            "input": "When Kennedy died in August 2009, he was the second-most senior member of the Senate (after President pro tempore Robert Byrd of West Virginia) and the third longest-serving senator of all time, behind Byrd and Strom Thurmond of South Carolina. Later that same year, he was passed by Daniel Inouye of Hawaii. Kennedy therefore held the record as the longest-serving Democratic member of Congress to solely serve as a senator until October 2021, when he was surpassed by fellow Democrat Patrick Leahy of Vermont.",
            "output": "Based on this passage, Robert Byrd, Strom Thurmond and Kennedy were the three longest-serving senators of all time.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.75,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.25,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 4.069481372833252,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.4,
            "strategy_details": {
                "raw": 0.8,
                "normalized": 0.8,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.39
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.25,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 973,
        "original_data": {
            "instruction": "Write a letter from the perspective of a potential home buyer to a home owner about wanting to buy the home.",
            "input": "",
            "output": "Dear home owner, \n\nYou have a beautiful home and as a home buyer, I am very interested in purchasing this home from you. I have been on the market to buy a home for a long time, but until now, I have not found a home that fits my criteria. Your home meets everything I am looking for and I hope you will accept my offer.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.1215765476226807,
            "normalized": 0.4633075850350516
        },
        "potential_entropy": {
            "strategy_component": 0.5575,
            "strategy_details": {
                "raw": 1.115,
                "normalized": 1.115,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 974,
        "original_data": {
            "instruction": "What are places that should be on everyone's bucket list?",
            "input": "",
            "output": "Everyone's bucket list should include Acadia National Park in Maine, Arches National Park in Utah, Venice (Italy), Petra (Jordan), Santorini (Greece), Rothenburn ob der Tauber (Germany), New York City, Sydney (Australia), Hong Kong, Califoria's Big Sur coastline, London, Paris, Rome, Florence and the Amalfi Coast.",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.85,
            "s_out_dens": 0.75,
            "s_out_bg": 0.6
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.25,
            "s_out_bg": 0.4
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.7529633045196533,
            "normalized": 0.9294180870056152
        },
        "potential_entropy": {
            "strategy_component": 1.135,
            "strategy_details": {
                "raw": 2.27,
                "normalized": 2.27,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.33
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.25,
                    "out_bg": 0.4
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 975,
        "original_data": {
            "instruction": "Classify the following hikes in terms of difficulty: Half dome in Yosemite, Mt Everest in Nepal, Mission Peak in California",
            "input": "",
            "output": "Mt Everest is the toughest among the list based on scaling height of 28k feet and requires multiple days of hike, acclimatization at different altitudes, requires carrying oxygen tanks and heavy gear.\n\nHalf-dome is also considered as a difficult hike but less tougher than Mt Everest. Half-dome can be done as a day-hike and is at 4k elevation gain from the Yosemite valley and with moderate fitness & stamina its possible to do.\n\nMission Peak is considered a medium difficulty hike. It takes between 2-3 hours to scale Mission peak, it can be strenuous for people with health difficulties and less stamina.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.75,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.19999999999999996,
            "s_inp_complex": 0.25,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 4.041786193847656,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.3225,
            "strategy_details": {
                "raw": 0.645,
                "normalized": 0.645,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.44999999999999996,
                    "output": 0.13499999999999998
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.19999999999999996,
                    "inp_complex": 0.25,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 976,
        "original_data": {
            "instruction": "What is a narcissist?",
            "input": "",
            "output": "People who see themselves as the center of the universe. Narcissists gain confidence by putting others down. They feel attacked at the slightest thing, even when a comment has nothing to do with them. They're small human beings. They lack grace, wisdom, humility, integrity, and perhaps even a conscience. Narcissists make the worst mothers — traumatizing their children, feeling resentful and jealous of their children and in the worst cases, seeing their daughters as their competition. Narcissists make the worst kind of boss. Unfortunately, many narcissists are in positions of power.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.571007251739502,
            "normalized": 0.8774306433541434
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 977,
        "original_data": {
            "instruction": "What are toe beans?",
            "input": "",
            "output": "Toe beans is a colloquial term originating on the Internet and is used to refer to the leathers of a cat (its toes and paw pad). It can also refer to a dog's paws but the term was popularized in the context of cats.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.4,
            "s_out_dens": 0.6,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.6,
            "s_out_dens": 0.4,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 3.0019571781158447,
            "normalized": 0.7148449080330985
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.6,
                    "out_dens": 0.4,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 978,
        "original_data": {
            "instruction": "Why practicing sport is important ?",
            "input": "",
            "output": "Practicing sport is important for your physical health and mental health. It teaches you to set goals and and how to reach them. You will learn discipline, consistency and regularity. But it's also very important for you physical health because of the endorphins and dopamina it secretes. It regulates your blood pressure level. It will reduce your stress, improve your sleep and you will feel happier.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.1441304683685303,
            "normalized": 0.7554658481052944
        },
        "potential_entropy": {
            "strategy_component": 0.6124999999999999,
            "strategy_details": {
                "raw": 1.2249999999999999,
                "normalized": 1.2249999999999999,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.285
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 979,
        "original_data": {
            "instruction": "Which of these counties has only one neighbour, and which have more than one neighbour: Haiti, Russia, South Korea, Germany, Portugal, Ireland, Canada, Brazil, Denmark, Austria, Qatar and France.",
            "input": "",
            "output": "The following countries have only one neighbouring country: South Korea, Qatar, Portugal, Ireland, Haiti, Canada, and Denmark. These countries have multiple neighbours:Russia, Brazil, Germany, Austria, and France.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 3.7242391109466553,
            "normalized": 0.9212111745561872
        },
        "potential_entropy": {
            "strategy_component": 0.46,
            "strategy_details": {
                "raw": 0.92,
                "normalized": 0.92,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.65,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 980,
        "original_data": {
            "instruction": "Based on the text provided, what does a minimum spanning tree optimize?",
            "input": "A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. That is, it is a spanning tree whose sum of edge weights is as small as possible. More generally, any edge-weighted undirected graph (not necessarily connected) has a minimum spanning forest, which is a union of the minimum spanning trees for its connected components.\n\nThere are many use cases for minimum spanning trees. One example is a telecommunications company trying to lay cable in a new neighborhood. If it is constrained to bury the cable only along certain paths (e.g. roads), then there would be a graph containing the points (e.g. houses) connected by those paths. Some of the paths might be more expensive, because they are longer, or require the cable to be buried deeper; these paths would be represented by edges with larger weights. Currency is an acceptable unit for edge weight – there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality. A spanning tree for that graph would be a subset of those paths that has no cycles but still connects every house; there might be several spanning trees possible. A minimum spanning tree would be one with the lowest total cost, representing the least expensive path for laying the cable.",
            "output": "A minimum spanning tree optimizes the a graph in a way that minimizes the sum of all the weights of the edges",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.7,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            1
        ],
        "entropy": {
            "raw": 2.4806342124938965,
            "normalized": 0.5658954892839704
        },
        "potential_entropy": {
            "strategy_component": 0.33999999999999997,
            "strategy_details": {
                "raw": 0.6799999999999999,
                "normalized": 0.6799999999999999,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.21000000000000002
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 981,
        "original_data": {
            "instruction": "What is Mr. Burn's assistant name on the Simpsons?",
            "input": "",
            "output": "Smithers",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.95,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.050000000000000044,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 8.877175331115723,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2625000000000002,
            "strategy_details": {
                "raw": 2.5250000000000004,
                "normalized": 2.5250000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.5850000000000001
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.050000000000000044,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 982,
        "original_data": {
            "instruction": "what is considered the birthplace of mountain biking?",
            "input": "",
            "output": "Fairfax California is considered the birthplace of mountain biking.  Local high school students who grew up in the area would take old single-speed bikes with coaster brakes up local fire roads and then ride them down the hills as fast as they could.  Eventually they started adding gears to the bikes so that they could pedal them up the hill as well.  The most famous trail was named Repack, because often after the races riders would have to re-pack the coaster brakes with grease because the grease would vaporize from the heat from the hard braking as they raced down the hill.  Modern bike brands like Fisher, Ritchey, Klein, Breezer, and Specialized all had their start in the Marin County foothills north of San Francisco, in the small town of Fairfax.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            0,
            1,
            2
        ],
        "entropy": {
            "raw": 3.7793335914611816,
            "normalized": 0.9369524547031948
        },
        "potential_entropy": {
            "strategy_component": 1.165,
            "strategy_details": {
                "raw": 2.33,
                "normalized": 2.33,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 2.0,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 983,
        "original_data": {
            "instruction": "What is Gettysburg College?",
            "input": "",
            "output": "Gettysburg College is a private liberal arts college located in Gettysburg, Pennsylvania. It was founded in 1832 and is noted as being a key fixture in the Battle of Gettysburg fought during the Civil War.Gettysburg College has about 2,600 students and is 90 miles north of Washington D.C.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.95
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.050000000000000044
        },
        "final_strategy_mark": [
            1,
            1,
            1
        ],
        "entropy": {
            "raw": 2.1680688858032227,
            "normalized": 0.4765911102294922
        },
        "potential_entropy": {
            "strategy_component": 1.21,
            "strategy_details": {
                "raw": 2.42,
                "normalized": 2.42,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.18000000000000002
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.050000000000000044
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 984,
        "original_data": {
            "instruction": "What is the key to longevity?",
            "input": "",
            "output": "Researchers and doctors have been studying this for many years and a number of patterns has consistently shown to be true: It's a combination of nutrition, exercise and sleep. From a nutrition perspective, a predominantly plant-based diet is the way to go. The Mediterranean diet continues to outperform all other diets year after year. A plant-based diet can not only prevent disease but it can reverse cardiovascular illness and many others. We now also have enough evidence on the importance of movement, exercise and specifically a combination of resistance and endurance training. Muscle growth is not only positive for the muscles themselves but it also positively affects bone density (which we tend to lose as we age), as well as effects on your immune system, gut microbiome and even mental health. The positives are endless. Lastly, if we do not get proper sleep (typically 7-9hours/night), the other two factors, diet and exercise, will not be optimal. Consistency in going to sleep and waking up at the same time is crucial to overall wellbeing and recovery. \nResearchers of the Blue Zones argue that another crucial part to living healthily into old age is the social circle or community we are part of. \nIf you want to live a long and healthy life, eat more plants, exercise, get some sleep and spend time with your friends and family.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.75,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.85,
            "s_out_div": 0.8,
            "s_out_dens": 0.9,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.25,
            "s_inp_complex": 0.30000000000000004,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.09999999999999998,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.015962839126587,
            "normalized": 0.7188465254647392
        },
        "potential_entropy": {
            "strategy_component": 0.5,
            "strategy_details": {
                "raw": 1.0,
                "normalized": 1.0,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.55,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.25,
                    "inp_complex": 0.30000000000000004,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.09999999999999998,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 985,
        "original_data": {
            "instruction": "Suggest some TV shows I should watch in a few different genres.",
            "input": "",
            "output": "Mystery:\nWhite Collar\nThe Flight Attendant\n\nSci-Fi:\nFor All Mankind\nFirefly\n\nComedy:\nTed Lasso\nSchitt's Creek\n\nSuperhero:\nTitans\nLoki",
            "category": "brainstorming"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.65,
            "s_out_cot": 0.9,
            "s_out_div": 0.95,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.35,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.050000000000000044,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.9397130012512207,
            "normalized": 0.9827751432146344
        },
        "potential_entropy": {
            "strategy_component": 0.505,
            "strategy_details": {
                "raw": 1.01,
                "normalized": 1.01,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.65,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.35,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.050000000000000044,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 986,
        "original_data": {
            "instruction": "What kind of publisher is the American Fantasy Press",
            "input": "American Fantasy Press is a science fiction/fantasy/horror specialty press owned and operated by Robert T. Garcia and Nancy Garcia. Located in Woodstock, Illinois, the press has published: The first U.S. hardcover edition of Dennis Etchison's Darkside (A 10th anniversary edition); the chapbook edition of The Man on the Ceiling by Steve Rasnic and Melanie Tem, the chapbook of A Walking Tour of the Shambles by Neil Gaiman and Gene Wolfe, the first U.S. hardcover edition of Michael Moorcock's The Dreamthief's Daughter (a signed, limited edition), the chapbook The Broecker Sampler, featuring the artwork of Randy Broecker, a broadsheet of Jo Fletcher's poem Midnight Monster illustrated by Gahan Wilson, and the first hardcover edition of the Mary Frances Zambreno's short story collection Invisible Pleasures. It published the fantasy short story collection by Mike Resnick, Stalking the Zombie which features his hard-boiled detective John J. Mallory in an alternate Manhattan. In 2015, the press published The Collectors' Book of Virgil Finlay, by Robert Weinberg, Douglas Ellis and AF publisher Robert Garcia.",
            "output": "American Fantasy Press is a science fiction/fantasy/horror specialty press owned and operated by Robert T. Garcia and Nancy Garcia. \n\nLocated in Woodstock, Illinois, the press has published: The first U.S. hardcover edition of Dennis Etchison's Darkside (A 10th anniversary edition); the chapbook edition of The Man on the Ceiling by Steve Rasnic and Melanie Tem, the chapbook of A Walking Tour of the Shambles by Neil Gaiman and Gene Wolfe, the first U.S. hardcover edition of Michael Moorcock's The Dreamthief's Daughter (a signed, limited edition), the chapbook The Broecker Sampler, featuring the artwork of Randy Broecker, a broadsheet of Jo Fletcher's poem Midnight Monster illustrated by Gahan Wilson, and the first hardcover edition of the Mary Frances Zambreno's short story collection Invisible Pleasures. \n\nIt published the fantasy short story collection by Mike Resnick, Stalking the Zombie which features his hard-boiled detective John J. Mallory in an alternate Manhattan. In 2015, the press published The Collectors' Book of Virgil Finlay, by Robert Weinberg, Douglas Ellis and AF publisher Robert Garcia.",
            "category": "closed_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.9,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.88,
            "s_out_bg": 0.92
        },
        "metric_gaps": {
            "s_ins_tone": 0.09999999999999998,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.12,
            "s_out_bg": 0.07999999999999996
        },
        "final_strategy_mark": [
            0,
            0,
            0
        ],
        "entropy": {
            "raw": 3.9573864936828613,
            "normalized": 0.9878247124808175
        },
        "potential_entropy": {
            "strategy_component": 0.2875,
            "strategy_details": {
                "raw": 0.575,
                "normalized": 0.575,
                "components": {
                    "instruction": 0.11999999999999997,
                    "input": 0.35,
                    "output": 0.105
                },
                "gaps": {
                    "ins_tone": 0.09999999999999998,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.12,
                    "out_bg": 0.07999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 987,
        "original_data": {
            "instruction": "From the passage provided, extract the list of software products sold by Adobe in the order they appear in the passage.",
            "input": "Adobe Inc., originally called Adobe Systems Incorporated, is an American multinational computer software company incorporated in Delaware and headquartered in San Jose, California. It has historically specialized in software for the creation and publication of a wide range of content, including graphics, photography, illustration, animation, multimedia/video, motion pictures, and print. Its flagship products include Adobe Photoshop image editing software; Adobe Illustrator vector-based illustration software; Adobe Acrobat Reader and the Portable Document Format (PDF); and a host of tools primarily for audio-visual content creation, editing and publishing. Adobe offered a bundled solution of its products named Adobe Creative Suite, which evolved into a subscription software as a service (SaaS) offering named Adobe Creative Cloud. The company also expanded into digital marketing software and in 2021 was considered one of the top global leaders in Customer Experience Management (CXM).\n\nAdobe was founded in December 1982 by John Warnock and Charles Geschke, who established the company after leaving Xerox PARC to develop and sell the PostScript page description language. In 1985, Apple Computer licensed PostScript for use in its LaserWriter printers, which helped spark the desktop publishing revolution. Adobe later developed animation and multimedia through its acquisition of Macromedia, from which it acquired Adobe Flash; video editing and compositing software with Adobe Premiere, later known as Adobe Premiere Pro; low-code web development with Adobe Muse; and a suite of software for digital marketing management.\n\nAs of 2022, Adobe has more than 26,000 employees worldwide. Adobe also has major development operations in the United States in Newton, New York City, Arden Hills, Lehi, Seattle, Austin and San Francisco. It also has major development operations in Noida and Bangalore in India.",
            "output": "- Adobe Photoshop\n- Adobe Illustrator\n- Adobe Acrobat Reader\n- Adobe Creative Suite\n- Adobe Flash\n- Adobe Premiere Pro\n- Adobe Muse",
            "category": "information_extraction"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.95,
            "s_inp_depth": 0.85,
            "s_inp_complex": 0.8,
            "s_out_cot": 0.9,
            "s_out_div": 0.85,
            "s_out_dens": 0.88,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.050000000000000044,
            "s_inp_depth": 0.15000000000000002,
            "s_inp_complex": 0.19999999999999996,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.15000000000000002,
            "s_out_dens": 0.12,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            0,
            0,
            3
        ],
        "entropy": {
            "raw": 2.0054898262023926,
            "normalized": 0.43013995034354074
        },
        "potential_entropy": {
            "strategy_component": 0.29800000000000004,
            "strategy_details": {
                "raw": 0.5960000000000001,
                "normalized": 0.5960000000000001,
                "components": {
                    "instruction": 0.06000000000000005,
                    "input": 0.35,
                    "output": 0.186
                },
                "gaps": {
                    "ins_tone": 0.050000000000000044,
                    "inp_depth": 0.15000000000000002,
                    "inp_complex": 0.19999999999999996,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.15000000000000002,
                    "out_dens": 0.12,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 988,
        "original_data": {
            "instruction": "Identify which animal species is alive or extinct: Sarcosuchus, Plains Bison",
            "input": "",
            "output": "Sarcosuchus is extinct, Plains Bison is alive.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.5,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.5,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 5.077908992767334,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 0.5975,
            "strategy_details": {
                "raw": 1.195,
                "normalized": 1.195,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.31499999999999995
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.5,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 989,
        "original_data": {
            "instruction": "Classify these as either web browsers or programming languages: Firefox, Netscape, C++, Python, Internet Explorer, Edge, Ruby, C#, Safari, AOL Explorer",
            "input": "",
            "output": "Web browsers: Firefox, Netscape, Internet Explorer, Edge, Safari, AOL Explorer\nProgramming languages: C++, Python, Ruby, C#",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.95,
            "s_out_div": 0.9,
            "s_out_dens": 0.8,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.050000000000000044,
            "s_out_div": 0.09999999999999998,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 3.367821455001831,
            "normalized": 0.8193775585719517
        },
        "potential_entropy": {
            "strategy_component": 0.53,
            "strategy_details": {
                "raw": 1.06,
                "normalized": 1.06,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.18
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.050000000000000044,
                    "out_div": 0.09999999999999998,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 990,
        "original_data": {
            "instruction": "Why do Jews eat matza on Passover?",
            "input": "",
            "output": "The book of Exodus commands the Jews to eat matza. The bible says that at the time of the exodus, the Jews left Egypt in such a hurry that their dough did not have time to rise. To commemorate this, they eat unleavened bread, or matza, every Passover.\n\nMany Jews eat matza today, even if they do not believe that they obeying a divine command. They may eat matza as a means of connecting with their Jewish heritage, or because they are participating in a seder, the traditional Passover meal, or just because they like the taste.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.6866202354431152,
            "normalized": 0.6247486386980329
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 991,
        "original_data": {
            "instruction": "Which is a species of fish? Wahoo or Yahoo",
            "input": "",
            "output": "Wahoo",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.9,
            "s_out_div": 0.1,
            "s_out_dens": 0.8,
            "s_out_bg": 0.1
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.9,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.9
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 15.724565505981445,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.2850000000000001,
            "strategy_details": {
                "raw": 2.5700000000000003,
                "normalized": 2.5700000000000003,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.63
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.9,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.9
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 992,
        "original_data": {
            "instruction": "How do I make a traditional Bolognese sauce?",
            "input": "",
            "output": "Bolognese sauce, sometimes called Bolognese Ragu has many variations, so I will provide the most common method. Dice one brown onion and then sauté in 25g of butter and 4 tablespoons of olive oil. Once the onions start to colour add 35 grams of diced pancetta and let the fat render. Add a mix of 1 chopped carrot, 1 chopped celery stick and 1 diced clove of garlic, stirring for 3-4 minutes. Add 500g of ground beef, seasoning the beef with salt and pepper. Next evaporate 1 cup of red wine, making sure the mixture reduces. Then evaporate 3/4 of a cup of fresh milk, also reducing the mixture. Add 4 chopped and peeled fresh tomatoes, 1 bay leaf and 1/4 tsp of chopped fresh thyme leaves and then simmer the sauce on low to medium heat for 1 and a half to two hours. Season again before serving with pasta.",
            "category": "open_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.9,
            "s_out_div": 0.7,
            "s_out_dens": 0.85,
            "s_out_bg": 0.75
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.5,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.15000000000000002,
            "s_out_bg": 0.25
        },
        "final_strategy_mark": [
            1,
            2,
            1
        ],
        "entropy": {
            "raw": 2.729776382446289,
            "normalized": 0.6370789664132255
        },
        "potential_entropy": {
            "strategy_component": 0.69,
            "strategy_details": {
                "raw": 1.38,
                "normalized": 1.38,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.9,
                    "output": 0.24
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.4,
                    "inp_complex": 0.5,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.15000000000000002,
                    "out_bg": 0.25
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 993,
        "original_data": {
            "instruction": "Classify each of the following as either computer hardware or computer software: hard drive, text editor, keyboard, monitor, mouse, operating system, and Internet browser.",
            "input": "",
            "output": "hard drive - computer hardware, text editor - computer software, keyboard - computer hardware, monitor - computer hardware, mouse - computer hardware, operating system - computer software, and Internet browser - computer software.",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.65
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.35
        },
        "final_strategy_mark": [
            1,
            0,
            3
        ],
        "entropy": {
            "raw": 2.3705594539642334,
            "normalized": 0.5344455582754952
        },
        "potential_entropy": {
            "strategy_component": 0.5750000000000001,
            "strategy_details": {
                "raw": 1.1500000000000001,
                "normalized": 1.1500000000000001,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.26999999999999996
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.35
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 994,
        "original_data": {
            "instruction": "What is an audible?",
            "input": "",
            "output": "An audible can be used to describe how one can be prepared for changing circumstances.  When asked an unexpected question, one is able to pivot their prepared response for another.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.0,
            "s_inp_complex": 0.0,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.65,
            "s_out_bg": 0.5
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 1.0,
            "s_inp_complex": 1.0,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.35,
            "s_out_bg": 0.5
        },
        "final_strategy_mark": [
            1,
            1,
            3
        ],
        "entropy": {
            "raw": 4.201549053192139,
            "normalized": 1.0
        },
        "potential_entropy": {
            "strategy_component": 1.3524999999999998,
            "strategy_details": {
                "raw": 2.7049999999999996,
                "normalized": 2.7049999999999996,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 2.0,
                    "output": 0.46499999999999997
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 1.0,
                    "inp_complex": 1.0,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.35,
                    "out_bg": 0.5
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 995,
        "original_data": {
            "instruction": "why does daylight savings still exist in the United States?",
            "input": "",
            "output": "In the United States, the economy used to be heavily based on agriculture.  Therefore, daylight savings was originally instituted to facilitate a farmer's workday so that much of the working hours could coincide with the hours where there was more daylight (daylight begins is later in the spring). Since then; however, the U.S. economy has become less agriculturally based and also with technological advancements that have drastically impacted American work patterns, the need to follow daylight windows has reduced dramatically. While daylight savings still allows many Americans to spend their working hours during the daylight, there is the popular opinion that daylight savings should no longer be observed in the United States, especially with the confusion that comes with changing hours during the spring and fall.  There are at least a couple states that do not change times for daylight savings which further adds to the confusion.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.7,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.6,
            "s_out_div": 0.5,
            "s_out_dens": 0.55,
            "s_out_bg": 0.45
        },
        "metric_gaps": {
            "s_ins_tone": 0.30000000000000004,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.4,
            "s_out_div": 0.5,
            "s_out_dens": 0.44999999999999996,
            "s_out_bg": 0.55
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.357515811920166,
            "normalized": 0.8164330891200474
        },
        "potential_entropy": {
            "strategy_component": 1.115,
            "strategy_details": {
                "raw": 2.23,
                "normalized": 2.23,
                "components": {
                    "instruction": 0.36000000000000004,
                    "input": 1.2999999999999998,
                    "output": 0.5700000000000001
                },
                "gaps": {
                    "ins_tone": 0.30000000000000004,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.4,
                    "out_div": 0.5,
                    "out_dens": 0.44999999999999996,
                    "out_bg": 0.55
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 996,
        "original_data": {
            "instruction": "How do you train for a marathon?",
            "input": "",
            "output": "A lot of people who run marathons will start training about six months before the marathon. You would typically start by running short lengths, a couple times a week, such as 2 to 3 miles a day, 2 to 3 days per week after a couple weeks of that, you would build up to six or 7 miles per day Often marathon runners will run one day a week longer, such as 10 miles on a Saturday or Sunday. About one month out is when you shored train the longest, such as a 15 mile run on the weekend. Within about two weeks of the race you want to start cooling down and resting.  usually a day or two before before the race you will start carb loading, which is when you eat a lot of carbohydrates. The day of the race is usually a celebratory event, as running a marathon is a big milestone!",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.2,
            "s_inp_complex": 0.1,
            "s_out_cot": 0.7,
            "s_out_div": 0.6,
            "s_out_dens": 0.5,
            "s_out_bg": 0.4
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.8,
            "s_inp_complex": 0.9,
            "s_out_cot": 0.30000000000000004,
            "s_out_div": 0.4,
            "s_out_dens": 0.5,
            "s_out_bg": 0.6
        },
        "final_strategy_mark": [
            1,
            2,
            3
        ],
        "entropy": {
            "raw": 3.1180782318115234,
            "normalized": 0.7480223519461495
        },
        "potential_entropy": {
            "strategy_component": 1.2400000000000002,
            "strategy_details": {
                "raw": 2.4800000000000004,
                "normalized": 2.4800000000000004,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 1.7000000000000002,
                    "output": 0.54
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.8,
                    "inp_complex": 0.9,
                    "out_cot": 0.30000000000000004,
                    "out_div": 0.4,
                    "out_dens": 0.5,
                    "out_bg": 0.6
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 997,
        "original_data": {
            "instruction": "What is a good anime to watch for someone who has never watched anime before?",
            "input": "",
            "output": "This very much depends on on your personal preferences, but I’ll give you a couple of options that I think would be good starter animes. One of the most famous and highly acclaimed anime is Cowboy Bepop, which is a genre-crossing single-season show about a crew of bounty hunters aboard a spaceship called the Bepop. Another popular entry into anime is One Punch Man, which is an action/comedy about a superhero who can defeat anyone with one punch, leading him to grow bored and search for meaning and challenge.",
            "category": "general_qa"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.75,
            "s_inp_depth": 0.4,
            "s_inp_complex": 0.3,
            "s_out_cot": 0.85,
            "s_out_div": 0.7,
            "s_out_dens": 0.65,
            "s_out_bg": 0.8
        },
        "metric_gaps": {
            "s_ins_tone": 0.25,
            "s_inp_depth": 0.6,
            "s_inp_complex": 0.7,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.30000000000000004,
            "s_out_dens": 0.35,
            "s_out_bg": 0.19999999999999996
        },
        "final_strategy_mark": [
            1,
            2,
            2
        ],
        "entropy": {
            "raw": 3.4648075103759766,
            "normalized": 0.8470878601074219
        },
        "potential_entropy": {
            "strategy_component": 0.95,
            "strategy_details": {
                "raw": 1.9,
                "normalized": 1.9,
                "components": {
                    "instruction": 0.3,
                    "input": 1.2999999999999998,
                    "output": 0.3
                },
                "gaps": {
                    "ins_tone": 0.25,
                    "inp_depth": 0.6,
                    "inp_complex": 0.7,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.30000000000000004,
                    "out_dens": 0.35,
                    "out_bg": 0.19999999999999996
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 998,
        "original_data": {
            "instruction": "Classify which cat breeds into long-haired and short-haired breeds:  Siamese, Maine Coon, Norwegian Forest Cat, Russian Blue, British Shorthair, Persian, Ragdoll, Himalayan, Burman",
            "input": "",
            "output": "Long-haired: Maine Coon, Norwegian Forest Cat, Persian, Ragdoll, Himalayan \nShort-haired: Siamese, Burman, Russian Blue, British Shorthair",
            "category": "classification"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.85,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.9,
            "s_out_div": 0.8,
            "s_out_dens": 0.75,
            "s_out_bg": 0.85
        },
        "metric_gaps": {
            "s_ins_tone": 0.15000000000000002,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.09999999999999998,
            "s_out_div": 0.19999999999999996,
            "s_out_dens": 0.25,
            "s_out_bg": 0.15000000000000002
        },
        "final_strategy_mark": [
            1,
            0,
            2
        ],
        "entropy": {
            "raw": 2.9509365558624268,
            "normalized": 0.7002675873892648
        },
        "potential_entropy": {
            "strategy_component": 0.545,
            "strategy_details": {
                "raw": 1.09,
                "normalized": 1.09,
                "components": {
                    "instruction": 0.18000000000000002,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.15000000000000002,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.09999999999999998,
                    "out_div": 0.19999999999999996,
                    "out_dens": 0.25,
                    "out_bg": 0.15000000000000002
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    },
    {
        "sample_id": 999,
        "original_data": {
            "instruction": "How do you cook the perfect pizza?",
            "input": "",
            "output": "The first thing you must do is preheat your oven up as high as it will go. It takes longer than you think to heat your oven up all the way! You want to give your oven at least 30 minutes to preheat. If you have a pizza stone, make sure it’s inside the preheating oven.\nFor highend ovens, this can be up to 550 degrees F. For regular ovens, they are not going above 475 F. Pizza is traditionally cooked in 900 or 1,000 degree ovens. We can’t get our ovens at home quite that hot, but we want to get as close as we can. Figure out what your oven can do and crank up the heat.\nStart out with about 12-16 ounces of dough. Any pizza dough, homemade or store bought, will work for this recipe.\nPlace your ball of dough in the center of your work surface and use your hands to press it down, starting from the center. Keep gently patting out the dough, moving the dough outward from the center. I like to leave a pretty healthy crust on the edges of my dough.\nContinue patting and stretching the dough gently with your hands until it is about 12 inches across. Of course you can use a rolling pin if you want! (Use your hands to lift up the edges of the pizza when you are done to get more a lip for the crust.)\nAt this point, transfer the dough to a square of parchment paper. I highly recommend using parchment paper because it’s SO much easier transferring your pizza into the oven.\nStretch and arrange the dough on the parchment paper. The dough shrinks a bit when you pick it up to transfer, just gently stretch it out again.\nDrizzle a bit of olive oil over the top of your dough. Use your hands or a pastry brush to rub the oil all over the top of the dough, especially the edges of the crust. This layer of oil helps the toppings stay separate from the dough, helping it cook more evenly. It also tastes delicious, especially on your crust. \nOnce the oven is up to temperature, we are going to do a 1 to 2 minute par bake.\nSome of my favorite pizza toppings:\nJust Cheese: Any will work: Mozzarella, Fontina, Parmesan, and Gorgonzola would be amazing.\nPepperoni\nMeat Lovers: Pepperoni, salami, Italian sausage, ham, bacon\nHawaiian: Canadian bacon, pineapple\nMargherita: fresh mozzarella, fresh basil, tomatoes\nBBQ: Use barbecue sauce instead of tomato sauce, then rotisserie chicken, red onions, bacon\nVeggie pizza: spinach, tomato, corn, peppers, red onion, mushrooms\nBuffalo: Use Frank’s sauce instead of tomato sauce, then add rotisserie chicken, garlic, blue cheese, red onions\nOnce your pizza is in the oven, you need to cook it for about 8-12 minutes. This is of course going to depend on how hot your oven is, and how thick your pizza is. The crust should be golden brown, and the cheese should be bubbly and also starting to brown. If you don’t have a pizza stone, use a spatula to lift the edge of your pizza to make sure that it is browning all across the center on bottom. If the bottom is still white, you are looking at a doughy pizza. Leave it in longer.",
            "category": "creative_writing"
        },
        "evaluation_scores": {
            "s_ins_tone": 0.8,
            "s_inp_depth": 0.7,
            "s_inp_complex": 0.6,
            "s_out_cot": 0.85,
            "s_out_div": 0.75,
            "s_out_dens": 0.8,
            "s_out_bg": 0.9
        },
        "metric_gaps": {
            "s_ins_tone": 0.19999999999999996,
            "s_inp_depth": 0.30000000000000004,
            "s_inp_complex": 0.4,
            "s_out_cot": 0.15000000000000002,
            "s_out_div": 0.25,
            "s_out_dens": 0.19999999999999996,
            "s_out_bg": 0.09999999999999998
        },
        "final_strategy_mark": [
            1,
            0,
            1
        ],
        "entropy": {
            "raw": 2.6327502727508545,
            "normalized": 0.6093572207859584
        },
        "potential_entropy": {
            "strategy_component": 0.575,
            "strategy_details": {
                "raw": 1.15,
                "normalized": 1.15,
                "components": {
                    "instruction": 0.23999999999999994,
                    "input": 0.7000000000000001,
                    "output": 0.21
                },
                "gaps": {
                    "ins_tone": 0.19999999999999996,
                    "inp_depth": 0.30000000000000004,
                    "inp_complex": 0.4,
                    "out_cot": 0.15000000000000002,
                    "out_div": 0.25,
                    "out_dens": 0.19999999999999996,
                    "out_bg": 0.09999999999999998
                }
            },
            "weights_used": {
                "local_entropy": 0.5,
                "strategy_score": 0.5,
                "ins_weight": 1.2,
                "inp_weight": 1.0,
                "out_weight": 0.3
            }
        }
    }
]